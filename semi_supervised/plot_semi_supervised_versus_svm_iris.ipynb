{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afceeda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset\n",
    "===============================================================================\n",
    "\n",
    "A comparison for the decision boundaries generated on the iris dataset\n",
    "by Label Spreading, Self-training and SVM.\n",
    "\n",
    "This example demonstrates that Label Spreading and Self-training can learn\n",
    "good boundaries even when small amounts of labeled data are available.\n",
    "\n",
    "Note that Self-training with 100% of the data is omitted as it is functionally\n",
    "identical to training the SVC on 100% of the data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.02\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "y_rand = rng.rand(y.shape[0])\n",
    "y_30 = np.copy(y)\n",
    "y_30[y_rand < 0.3] = -1  # set random samples to be unlabeled\n",
    "y_50 = np.copy(y)\n",
    "y_50[y_rand < 0.5] = -1\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "ls30 = (LabelSpreading().fit(X, y_30), y_30, \"Label Spreading 30% data\")\n",
    "ls50 = (LabelSpreading().fit(X, y_50), y_50, \"Label Spreading 50% data\")\n",
    "ls100 = (LabelSpreading().fit(X, y), y, \"Label Spreading 100% data\")\n",
    "\n",
    "# the base classifier for self-training is identical to the SVC\n",
    "base_classifier = SVC(kernel=\"rbf\", gamma=0.5, probability=True)\n",
    "st30 = (\n",
    "    SelfTrainingClassifier(base_classifier).fit(X, y_30),\n",
    "    y_30,\n",
    "    \"Self-training 30% data\",\n",
    ")\n",
    "st50 = (\n",
    "    SelfTrainingClassifier(base_classifier).fit(X, y_50),\n",
    "    y_50,\n",
    "    \"Self-training 50% data\",\n",
    ")\n",
    "\n",
    "rbf_svc = (SVC(kernel=\"rbf\", gamma=0.5).fit(X, y), y, \"SVC with rbf kernel\")\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "color_map = {-1: (1, 1, 1), 0: (0, 0, 0.9), 1: (1, 0, 0), 2: (0.8, 0.6, 0)}\n",
    "\n",
    "classifiers = (ls30, st30, ls50, st50, ls100, rbf_svc)\n",
    "for i, (clf, y_train, title) in enumerate(classifiers):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(3, 2, i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot also the training points\n",
    "    colors = [color_map[y] for y in y_train]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors=\"black\")\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "plt.suptitle(\"Unlabeled points are colored white\", y=0.1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
