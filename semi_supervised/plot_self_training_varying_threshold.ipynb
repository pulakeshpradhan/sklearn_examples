{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================\n",
    "Effect of varying threshold for self-training\n",
    "=============================================\n",
    "\n",
    "This example illustrates the effect of a varying threshold on self-training.\n",
    "The `breast_cancer` dataset is loaded, and labels are deleted such that only 50\n",
    "out of 569 samples have labels. A `SelfTrainingClassifier` is fitted on this\n",
    "dataset, with varying thresholds.\n",
    "\n",
    "The upper graph shows the amount of labeled samples that the classifier has\n",
    "available by the end of fit, and the accuracy of the classifier. The lower\n",
    "graph shows the last iteration in which a sample was labeled. All values are\n",
    "cross validated with 3 folds.\n",
    "\n",
    "At low thresholds (in [0.4, 0.5]), the classifier learns from samples that were\n",
    "labeled with a low confidence. These low-confidence samples are likely have\n",
    "incorrect predicted labels, and as a result, fitting on these incorrect labels\n",
    "produces a poor accuracy. Note that the classifier labels almost all of the\n",
    "samples, and only takes one iteration.\n",
    "\n",
    "For very high thresholds (in [0.9, 1)) we observe that the classifier does not\n",
    "augment its dataset (the amount of self-labeled samples is 0). As a result, the\n",
    "accuracy achieved with a threshold of 0.9999 is the same as a normal supervised\n",
    "classifier would achieve.\n",
    "\n",
    "The optimal accuracy lies in between both of these extremes at a threshold of\n",
    "around 0.7.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_splits = 3\n",
    "\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "y_true = y.copy()\n",
    "y[50:] = -1\n",
    "total_samples = y.shape[0]\n",
    "\n",
    "base_classifier = SVC(probability=True, gamma=0.001, random_state=42)\n",
    "\n",
    "x_values = np.arange(0.4, 1.05, 0.05)\n",
    "x_values = np.append(x_values, 0.99999)\n",
    "scores = np.empty((x_values.shape[0], n_splits))\n",
    "amount_labeled = np.empty((x_values.shape[0], n_splits))\n",
    "amount_iterations = np.empty((x_values.shape[0], n_splits))\n",
    "\n",
    "for i, threshold in enumerate(x_values):\n",
    "    self_training_clf = SelfTrainingClassifier(base_classifier, threshold=threshold)\n",
    "\n",
    "    # We need manual cross validation so that we don't treat -1 as a separate\n",
    "    # class when computing accuracy\n",
    "    skfolds = StratifiedKFold(n_splits=n_splits)\n",
    "    for fold, (train_index, test_index) in enumerate(skfolds.split(X, y)):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        y_test_true = y_true[test_index]\n",
    "\n",
    "        self_training_clf.fit(X_train, y_train)\n",
    "\n",
    "        # The amount of labeled samples that at the end of fitting\n",
    "        amount_labeled[i, fold] = (\n",
    "            total_samples\n",
    "            - np.unique(self_training_clf.labeled_iter_, return_counts=True)[1][0]\n",
    "        )\n",
    "        # The last iteration the classifier labeled a sample in\n",
    "        amount_iterations[i, fold] = np.max(self_training_clf.labeled_iter_)\n",
    "\n",
    "        y_pred = self_training_clf.predict(X_test)\n",
    "        scores[i, fold] = accuracy_score(y_test_true, y_pred)\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(211)\n",
    "ax1.errorbar(\n",
    "    x_values, scores.mean(axis=1), yerr=scores.std(axis=1), capsize=2, color=\"b\"\n",
    ")\n",
    "ax1.set_ylabel(\"Accuracy\", color=\"b\")\n",
    "ax1.tick_params(\"y\", colors=\"b\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(\n",
    "    x_values,\n",
    "    amount_labeled.mean(axis=1),\n",
    "    yerr=amount_labeled.std(axis=1),\n",
    "    capsize=2,\n",
    "    color=\"g\",\n",
    ")\n",
    "ax2.set_ylim(bottom=0)\n",
    "ax2.set_ylabel(\"Amount of labeled samples\", color=\"g\")\n",
    "ax2.tick_params(\"y\", colors=\"g\")\n",
    "\n",
    "ax3 = plt.subplot(212, sharex=ax1)\n",
    "ax3.errorbar(\n",
    "    x_values,\n",
    "    amount_iterations.mean(axis=1),\n",
    "    yerr=amount_iterations.std(axis=1),\n",
    "    capsize=2,\n",
    "    color=\"b\",\n",
    ")\n",
    "ax3.set_ylim(bottom=0)\n",
    "ax3.set_ylabel(\"Amount of iterations\")\n",
    "ax3.set_xlabel(\"Threshold\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
