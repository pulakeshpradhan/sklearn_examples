{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6112264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Lasso model selection via information criteria\n",
    "==============================================\n",
    "\n",
    "This example reproduces the example of Fig. 2 of [ZHT2007]_. A\n",
    ":class:`~sklearn.linear_model.LassoLarsIC` estimator is fit on a\n",
    "diabetes dataset and the AIC and the BIC criteria are used to select\n",
    "the best model.\n",
    "\n",
    ".. note::\n",
    "    It is important to note that the optimization to find `alpha` with\n",
    "    :class:`~sklearn.linear_model.LassoLarsIC` relies on the AIC or BIC\n",
    "    criteria that are computed in-sample, thus on the training set directly.\n",
    "    This approach differs from the cross-validation procedure. For a comparison\n",
    "    of the two approaches, you can refer to the following example:\n",
    "    :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`.\n",
    "\n",
    ".. rubric:: References\n",
    "\n",
    ".. [ZHT2007] :arxiv:`Zou, Hui, Trevor Hastie, and Robert Tibshirani.\n",
    "    \"On the degrees of freedom of the lasso.\"\n",
    "    The Annals of Statistics 35.5 (2007): 2173-2192.\n",
    "    <0712.0881>`\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# We will use the diabetes dataset.\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "n_samples = X.shape[0]\n",
    "X.head()\n",
    "\n",
    "# %%\n",
    "# Scikit-learn provides an estimator called\n",
    "# :class:`~sklearn.linear_model.LassoLarsIC` that uses either Akaike's\n",
    "# information criterion (AIC) or the Bayesian information criterion (BIC) to\n",
    "# select the best model. Before fitting\n",
    "# this model, we will scale the dataset.\n",
    "#\n",
    "# In the following, we are going to fit two models to compare the values\n",
    "# reported by AIC and BIC.\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "lasso_lars_ic = make_pipeline(StandardScaler(), LassoLarsIC(criterion=\"aic\")).fit(X, y)\n",
    "\n",
    "\n",
    "# %%\n",
    "# To be in line with the definition in [ZHT2007]_, we need to rescale the\n",
    "# AIC and the BIC. Indeed, Zou et al. are ignoring some constant terms\n",
    "# compared to the original definition of AIC derived from the maximum\n",
    "# log-likelihood of a linear model. You can refer to\n",
    "# :ref:`mathematical detail section for the User Guide <lasso_lars_ic>`.\n",
    "def zou_et_al_criterion_rescaling(criterion, n_samples, noise_variance):\n",
    "    \"\"\"Rescale the information criterion to follow the definition of Zou et al.\"\"\"\n",
    "    return criterion - n_samples * np.log(2 * np.pi * noise_variance) - n_samples\n",
    "\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "\n",
    "aic_criterion = zou_et_al_criterion_rescaling(\n",
    "    lasso_lars_ic[-1].criterion_,\n",
    "    n_samples,\n",
    "    lasso_lars_ic[-1].noise_variance_,\n",
    ")\n",
    "\n",
    "index_alpha_path_aic = np.flatnonzero(\n",
    "    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n",
    ")[0]\n",
    "\n",
    "# %%\n",
    "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\n",
    "\n",
    "bic_criterion = zou_et_al_criterion_rescaling(\n",
    "    lasso_lars_ic[-1].criterion_,\n",
    "    n_samples,\n",
    "    lasso_lars_ic[-1].noise_variance_,\n",
    ")\n",
    "\n",
    "index_alpha_path_bic = np.flatnonzero(\n",
    "    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n",
    ")[0]\n",
    "\n",
    "# %%\n",
    "# Now that we collected the AIC and BIC, we can as well check that the minima\n",
    "# of both criteria happen at the same alpha. Then, we can simplify the\n",
    "# following plot.\n",
    "index_alpha_path_aic == index_alpha_path_bic\n",
    "\n",
    "# %%\n",
    "# Finally, we can plot the AIC and BIC criterion and the subsequent selected\n",
    "# regularization parameter.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(aic_criterion, color=\"tab:blue\", marker=\"o\", label=\"AIC criterion\")\n",
    "plt.plot(bic_criterion, color=\"tab:orange\", marker=\"o\", label=\"BIC criterion\")\n",
    "plt.vlines(\n",
    "    index_alpha_path_bic,\n",
    "    aic_criterion.min(),\n",
    "    aic_criterion.max(),\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Selected alpha\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.ylabel(\"Information criterion\")\n",
    "plt.xlabel(\"Lasso model sequence\")\n",
    "_ = plt.title(\"Lasso model selection via AIC and BIC\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
