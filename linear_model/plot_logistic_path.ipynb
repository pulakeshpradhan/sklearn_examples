{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "Regularization path of L1- Logistic Regression\n",
    "==============================================\n",
    "\n",
    "\n",
    "Train l1-penalized logistic regression models on a binary classification\n",
    "problem derived from the Iris dataset.\n",
    "\n",
    "The models are ordered from strongest regularized to least regularized. The 4\n",
    "coefficients of the models are collected and plotted as a \"regularization\n",
    "path\": on the left-hand side of the figure (strong regularizers), all the\n",
    "coefficients are exactly 0. When regularization gets progressively looser,\n",
    "coefficients can get non-zero values one after the other.\n",
    "\n",
    "Here we choose the liblinear solver because it can efficiently optimize for the\n",
    "Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.\n",
    "\n",
    "Also note that we set a low value for the tolerance to make sure that the model\n",
    "has converged before collecting the coefficients.\n",
    "\n",
    "We also use warm_start=True which means that the coefficients of the models are\n",
    "reused to initialize the next model fit to speed-up the computation of the\n",
    "full-path.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Load data\n",
    "# ---------\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "X /= X.max()  # Normalize X to speed-up convergence\n",
    "\n",
    "# %%\n",
    "# Compute regularization path\n",
    "# ---------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import l1_min_c\n",
    "\n",
    "cs = l1_min_c(X, y, loss=\"log\") * np.logspace(0, 10, 16)\n",
    "\n",
    "clf = linear_model.LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    tol=1e-6,\n",
    "    max_iter=int(1e6),\n",
    "    warm_start=True,\n",
    "    intercept_scaling=10000.0,\n",
    ")\n",
    "coefs_ = []\n",
    "for c in cs:\n",
    "    clf.set_params(C=c)\n",
    "    clf.fit(X, y)\n",
    "    coefs_.append(clf.coef_.ravel().copy())\n",
    "\n",
    "coefs_ = np.array(coefs_)\n",
    "\n",
    "# %%\n",
    "# Plot regularization path\n",
    "# ------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.log10(cs), coefs_, marker=\"o\")\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel(\"log(C)\")\n",
    "plt.ylabel(\"Coefficients\")\n",
    "plt.title(\"Logistic Regression Path\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
