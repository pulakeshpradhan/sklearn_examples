{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================\n",
    "L1 Penalty and Sparsity in Logistic Regression\n",
    "==============================================\n",
    "\n",
    "Comparison of the sparsity (percentage of zero coefficients) of solutions when\n",
    "L1, L2 and Elastic-Net penalty are used for different values of C. We can see\n",
    "that large values of C give more freedom to the model.  Conversely, smaller\n",
    "values of C constrain the model more. In the L1 penalty case, this leads to\n",
    "sparser solutions. As expected, the Elastic-Net penalty sparsity is between\n",
    "that of L1 and L2.\n",
    "\n",
    "We classify 8x8 images of digits into two classes: 0-4 against 5-9.\n",
    "The visualization shows coefficients of the models for varying C.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# classify small against large digits\n",
    "y = (y > 4).astype(int)\n",
    "\n",
    "l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "# Set regularization parameter\n",
    "for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n",
    "    # Increase tolerance for short training time\n",
    "    clf_l1_LR = LogisticRegression(C=C, penalty=\"l1\", tol=0.01, solver=\"saga\")\n",
    "    clf_l2_LR = LogisticRegression(C=C, penalty=\"l2\", tol=0.01, solver=\"saga\")\n",
    "    clf_en_LR = LogisticRegression(\n",
    "        C=C, penalty=\"elasticnet\", solver=\"saga\", l1_ratio=l1_ratio, tol=0.01\n",
    "    )\n",
    "    clf_l1_LR.fit(X, y)\n",
    "    clf_l2_LR.fit(X, y)\n",
    "    clf_en_LR.fit(X, y)\n",
    "\n",
    "    coef_l1_LR = clf_l1_LR.coef_.ravel()\n",
    "    coef_l2_LR = clf_l2_LR.coef_.ravel()\n",
    "    coef_en_LR = clf_en_LR.coef_.ravel()\n",
    "\n",
    "    # coef_l1_LR contains zeros due to the\n",
    "    # L1 sparsity inducing norm\n",
    "\n",
    "    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n",
    "    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n",
    "    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n",
    "\n",
    "    print(f\"C={C:.2f}\")\n",
    "    print(f\"{'Sparsity with L1 penalty:':<40} {sparsity_l1_LR:.2f}%\")\n",
    "    print(f\"{'Sparsity with Elastic-Net penalty:':<40} {sparsity_en_LR:.2f}%\")\n",
    "    print(f\"{'Sparsity with L2 penalty:':<40} {sparsity_l2_LR:.2f}%\")\n",
    "    print(f\"{'Score with L1 penalty:':<40} {clf_l1_LR.score(X, y):.2f}\")\n",
    "    print(f\"{'Score with Elastic-Net penalty:':<40} {clf_en_LR.score(X, y):.2f}\")\n",
    "    print(f\"{'Score with L2 penalty:':<40} {clf_l2_LR.score(X, y):.2f}\")\n",
    "\n",
    "    if i == 0:\n",
    "        axes_row[0].set_title(\"L1 penalty\")\n",
    "        axes_row[1].set_title(\"Elastic-Net\\nl1_ratio = %s\" % l1_ratio)\n",
    "        axes_row[2].set_title(\"L2 penalty\")\n",
    "\n",
    "    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n",
    "        ax.imshow(\n",
    "            np.abs(coefs.reshape(8, 8)),\n",
    "            interpolation=\"nearest\",\n",
    "            cmap=\"binary\",\n",
    "            vmax=1,\n",
    "            vmin=0,\n",
    "        )\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "\n",
    "    axes_row[0].set_ylabel(f\"C = {C}\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
