{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3676b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================\n",
    "Polynomial and Spline interpolation\n",
    "===================================\n",
    "\n",
    "This example demonstrates how to approximate a function with polynomials up to\n",
    "degree ``degree`` by using ridge regression. We show two different ways given\n",
    "``n_samples`` of 1d points ``x_i``:\n",
    "\n",
    "- :class:`~sklearn.preprocessing.PolynomialFeatures` generates all monomials\n",
    "  up to ``degree``. This gives us the so called Vandermonde matrix with\n",
    "  ``n_samples`` rows and ``degree + 1`` columns::\n",
    "\n",
    "    [[1, x_0, x_0 ** 2, x_0 ** 3, ..., x_0 ** degree],\n",
    "     [1, x_1, x_1 ** 2, x_1 ** 3, ..., x_1 ** degree],\n",
    "     ...]\n",
    "\n",
    "  Intuitively, this matrix can be interpreted as a matrix of pseudo features\n",
    "  (the points raised to some power). The matrix is akin to (but different from)\n",
    "  the matrix induced by a polynomial kernel.\n",
    "\n",
    "- :class:`~sklearn.preprocessing.SplineTransformer` generates B-spline basis\n",
    "  functions. A basis function of a B-spline is a piece-wise polynomial function\n",
    "  of degree ``degree`` that is non-zero only between ``degree+1`` consecutive\n",
    "  knots. Given ``n_knots`` number of knots, this results in matrix of\n",
    "  ``n_samples`` rows and ``n_knots + degree - 1`` columns::\n",
    "\n",
    "    [[basis_1(x_0), basis_2(x_0), ...],\n",
    "     [basis_1(x_1), basis_2(x_1), ...],\n",
    "     ...]\n",
    "\n",
    "This example shows that these two transformers are well suited to model\n",
    "non-linear effects with a linear model, using a pipeline to add non-linear\n",
    "features. Kernel methods extend this idea and can induce very high (even\n",
    "infinite) dimensional feature spaces.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
    "\n",
    "# %%\n",
    "# We start by defining a function that we intend to approximate and prepare\n",
    "# plotting it.\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function to be approximated by polynomial interpolation.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "# whole range we want to plot\n",
    "x_plot = np.linspace(-1, 11, 100)\n",
    "\n",
    "# %%\n",
    "# To make it interesting, we only give a small subset of points to train on.\n",
    "\n",
    "x_train = np.linspace(0, 10, 100)\n",
    "rng = np.random.RandomState(0)\n",
    "x_train = np.sort(rng.choice(x_train, size=20, replace=False))\n",
    "y_train = f(x_train)\n",
    "\n",
    "# create 2D-array versions of these arrays to feed to transformers\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "# %%\n",
    "# Now we are ready to create polynomial features and splines, fit on the\n",
    "# training points and show how well they interpolate.\n",
    "\n",
    "# plot function\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(\n",
    "    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n",
    ")\n",
    "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
    "\n",
    "# plot training points\n",
    "ax.scatter(x_train, y_train, label=\"training points\")\n",
    "\n",
    "# polynomial features\n",
    "for degree in [3, 4, 5]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
    "\n",
    "# B-spline with 4 + 3 - 1 = 6 basis functions\n",
    "model = make_pipeline(SplineTransformer(n_knots=4, degree=3), Ridge(alpha=1e-3))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_plot = model.predict(X_plot)\n",
    "ax.plot(x_plot, y_plot, label=\"B-spline\")\n",
    "ax.legend(loc=\"lower center\")\n",
    "ax.set_ylim(-20, 10)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# This shows nicely that higher degree polynomials can fit the data better. But\n",
    "# at the same time, too high powers can show unwanted oscillatory behaviour\n",
    "# and are particularly dangerous for extrapolation beyond the range of fitted\n",
    "# data. This is an advantage of B-splines. They usually fit the data as well as\n",
    "# polynomials and show very nice and smooth behaviour. They have also good\n",
    "# options to control the extrapolation, which defaults to continue with a\n",
    "# constant. Note that most often, you would rather increase the number of knots\n",
    "# but keep ``degree=3``.\n",
    "#\n",
    "# In order to give more insights into the generated feature bases, we plot all\n",
    "# columns of both transformers separately.\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(16, 5))\n",
    "pft = PolynomialFeatures(degree=3).fit(X_train)\n",
    "axes[0].plot(x_plot, pft.transform(X_plot))\n",
    "axes[0].legend(axes[0].lines, [f\"degree {n}\" for n in range(4)])\n",
    "axes[0].set_title(\"PolynomialFeatures\")\n",
    "\n",
    "splt = SplineTransformer(n_knots=4, degree=3).fit(X_train)\n",
    "axes[1].plot(x_plot, splt.transform(X_plot))\n",
    "axes[1].legend(axes[1].lines, [f\"spline {n}\" for n in range(6)])\n",
    "axes[1].set_title(\"SplineTransformer\")\n",
    "\n",
    "# plot knots of spline\n",
    "knots = splt.bsplines_[0].t\n",
    "axes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles=\"dashed\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# In the left plot, we recognize the lines corresponding to simple monomials\n",
    "# from ``x**0`` to ``x**3``. In the right figure, we see the six B-spline\n",
    "# basis functions of ``degree=3`` and also the four knot positions that were\n",
    "# chosen during ``fit``. Note that there are ``degree`` number of additional\n",
    "# knots each to the left and to the right of the fitted interval. These are\n",
    "# there for technical reasons, so we refrain from showing them. Every basis\n",
    "# function has local support and is continued as a constant beyond the fitted\n",
    "# range. This extrapolating behaviour could be changed by the argument\n",
    "# ``extrapolation``.\n",
    "\n",
    "# %%\n",
    "# Periodic Splines\n",
    "# ----------------\n",
    "# In the previous example we saw the limitations of polynomials and splines for\n",
    "# extrapolation beyond the range of the training observations. In some\n",
    "# settings, e.g. with seasonal effects, we expect a periodic continuation of\n",
    "# the underlying signal. Such effects can be modelled using periodic splines,\n",
    "# which have equal function value and equal derivatives at the first and last\n",
    "# knot. In the following case we show how periodic splines provide a better fit\n",
    "# both within and outside of the range of training data given the additional\n",
    "# information of periodicity. The splines period is the distance between\n",
    "# the first and last knot, which we specify manually.\n",
    "#\n",
    "# Periodic splines can also be useful for naturally periodic features (such as\n",
    "# day of the year), as the smoothness at the boundary knots prevents a jump in\n",
    "# the transformed values (e.g. from Dec 31st to Jan 1st). For such naturally\n",
    "# periodic features or more generally features where the period is known, it is\n",
    "# advised to explicitly pass this information to the `SplineTransformer` by\n",
    "# setting the knots manually.\n",
    "\n",
    "\n",
    "# %%\n",
    "def g(x):\n",
    "    \"\"\"Function to be approximated by periodic spline interpolation.\"\"\"\n",
    "    return np.sin(x) - 0.7 * np.cos(x * 3)\n",
    "\n",
    "\n",
    "y_train = g(x_train)\n",
    "\n",
    "# Extend the test data into the future:\n",
    "x_plot_ext = np.linspace(-1, 21, 200)\n",
    "X_plot_ext = x_plot_ext[:, np.newaxis]\n",
    "\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(color=[\"black\", \"tomato\", \"teal\"])\n",
    "ax.plot(x_plot_ext, g(x_plot_ext), linewidth=lw, label=\"ground truth\")\n",
    "ax.scatter(x_train, y_train, label=\"training points\")\n",
    "\n",
    "for transformer, label in [\n",
    "    (SplineTransformer(degree=3, n_knots=10), \"spline\"),\n",
    "    (\n",
    "        SplineTransformer(\n",
    "            degree=3,\n",
    "            knots=np.linspace(0, 2 * np.pi, 10)[:, None],\n",
    "            extrapolation=\"periodic\",\n",
    "        ),\n",
    "        \"periodic spline\",\n",
    "    ),\n",
    "]:\n",
    "    model = make_pipeline(transformer, Ridge(alpha=1e-3))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_plot_ext = model.predict(X_plot_ext)\n",
    "    ax.plot(x_plot_ext, y_plot_ext, label=label)\n",
    "\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "# %% We again plot the underlying splines.\n",
    "fig, ax = plt.subplots()\n",
    "knots = np.linspace(0, 2 * np.pi, 4)\n",
    "splt = SplineTransformer(knots=knots[:, None], degree=3, extrapolation=\"periodic\").fit(\n",
    "    X_train\n",
    ")\n",
    "ax.plot(x_plot_ext, splt.transform(X_plot_ext))\n",
    "ax.legend(ax.lines, [f\"spline {n}\" for n in range(3)])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
