{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910492fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================\n",
    "Quantile regression\n",
    "===================\n",
    "\n",
    "This example illustrates how quantile regression can predict non-trivial\n",
    "conditional quantiles.\n",
    "\n",
    "The left figure shows the case when the error distribution is normal,\n",
    "but has non-constant variance, i.e. with heteroscedasticity.\n",
    "\n",
    "The right figure shows an example of an asymmetric error distribution,\n",
    "namely the Pareto distribution.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Dataset generation\n",
    "# ------------------\n",
    "#\n",
    "# To illustrate the behaviour of quantile regression, we will generate two\n",
    "# synthetic datasets. The true generative random processes for both datasets\n",
    "# will be composed by the same expected value with a linear relationship with a\n",
    "# single feature `x`.\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = np.linspace(start=0, stop=10, num=100)\n",
    "X = x[:, np.newaxis]\n",
    "y_true_mean = 10 + 0.5 * x\n",
    "\n",
    "# %%\n",
    "# We will create two subsequent problems by changing the distribution of the\n",
    "# target `y` while keeping the same expected value:\n",
    "#\n",
    "# - in the first case, a heteroscedastic Normal noise is added;\n",
    "# - in the second case, an asymmetric Pareto noise is added.\n",
    "y_normal = y_true_mean + rng.normal(loc=0, scale=0.5 + 0.5 * x, size=x.shape[0])\n",
    "a = 5\n",
    "y_pareto = y_true_mean + 10 * (rng.pareto(a, size=x.shape[0]) - 1 / (a - 1))\n",
    "\n",
    "# %%\n",
    "# Let's first visualize the datasets as well as the distribution of the\n",
    "# residuals `y - mean(y)`.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 11), sharex=\"row\", sharey=\"row\")\n",
    "\n",
    "axs[0, 0].plot(x, y_true_mean, label=\"True mean\")\n",
    "axs[0, 0].scatter(x, y_normal, color=\"black\", alpha=0.5, label=\"Observations\")\n",
    "axs[1, 0].hist(y_true_mean - y_normal, edgecolor=\"black\")\n",
    "\n",
    "\n",
    "axs[0, 1].plot(x, y_true_mean, label=\"True mean\")\n",
    "axs[0, 1].scatter(x, y_pareto, color=\"black\", alpha=0.5, label=\"Observations\")\n",
    "axs[1, 1].hist(y_true_mean - y_pareto, edgecolor=\"black\")\n",
    "\n",
    "axs[0, 0].set_title(\"Dataset with heteroscedastic Normal distributed targets\")\n",
    "axs[0, 1].set_title(\"Dataset with asymmetric Pareto distributed target\")\n",
    "axs[1, 0].set_title(\n",
    "    \"Residuals distribution for heteroscedastic Normal distributed targets\"\n",
    ")\n",
    "axs[1, 1].set_title(\"Residuals distribution for asymmetric Pareto distributed target\")\n",
    "axs[0, 0].legend()\n",
    "axs[0, 1].legend()\n",
    "axs[0, 0].set_ylabel(\"y\")\n",
    "axs[1, 0].set_ylabel(\"Counts\")\n",
    "axs[0, 1].set_xlabel(\"x\")\n",
    "axs[0, 0].set_xlabel(\"x\")\n",
    "axs[1, 0].set_xlabel(\"Residuals\")\n",
    "_ = axs[1, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "# %%\n",
    "# With the heteroscedastic Normal distributed target, we observe that the\n",
    "# variance of the noise is increasing when the value of the feature `x` is\n",
    "# increasing.\n",
    "#\n",
    "# With the asymmetric Pareto distributed target, we observe that the positive\n",
    "# residuals are bounded.\n",
    "#\n",
    "# These types of noisy targets make the estimation via\n",
    "# :class:`~sklearn.linear_model.LinearRegression` less efficient, i.e. we need\n",
    "# more data to get stable results and, in addition, large outliers can have a\n",
    "# huge impact on the fitted coefficients. (Stated otherwise: in a setting with\n",
    "# constant variance, ordinary least squares estimators converge much faster to\n",
    "# the *true* coefficients with increasing sample size.)\n",
    "#\n",
    "# In this asymmetric setting, the median or different quantiles give additional\n",
    "# insights. On top of that, median estimation is much more robust to outliers\n",
    "# and heavy tailed distributions. But note that extreme quantiles are estimated\n",
    "# by very few data points. 95% quantile are more or less estimated by the 5%\n",
    "# largest values and thus also a bit sensitive outliers.\n",
    "#\n",
    "# In the remainder of this tutorial, we will show how\n",
    "# :class:`~sklearn.linear_model.QuantileRegressor` can be used in practice and\n",
    "# give the intuition into the properties of the fitted models. Finally,\n",
    "# we will compare the both :class:`~sklearn.linear_model.QuantileRegressor`\n",
    "# and :class:`~sklearn.linear_model.LinearRegression`.\n",
    "#\n",
    "# Fitting a `QuantileRegressor`\n",
    "# -----------------------------\n",
    "#\n",
    "# In this section, we want to estimate the conditional median as well as\n",
    "# a low and high quantile fixed at 5% and 95%, respectively. Thus, we will get\n",
    "# three linear models, one for each quantile.\n",
    "#\n",
    "# We will use the quantiles at 5% and 95% to find the outliers in the training\n",
    "# sample beyond the central 90% interval.\n",
    "\n",
    "# %%\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "predictions = {}\n",
    "out_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\n",
    "for quantile in quantiles:\n",
    "    qr = QuantileRegressor(quantile=quantile, alpha=0)\n",
    "    y_pred = qr.fit(X, y_normal).predict(X)\n",
    "    predictions[quantile] = y_pred\n",
    "\n",
    "    if quantile == min(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred >= y_normal\n",
    "        )\n",
    "    elif quantile == max(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred <= y_normal\n",
    "        )\n",
    "\n",
    "# %%\n",
    "# Now, we can plot the three linear models and the distinguished samples that\n",
    "# are within the central 90% interval from samples that are outside this\n",
    "# interval.\n",
    "plt.plot(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\n",
    "\n",
    "for quantile, y_pred in predictions.items():\n",
    "    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")\n",
    "\n",
    "plt.scatter(\n",
    "    x[out_bounds_predictions],\n",
    "    y_normal[out_bounds_predictions],\n",
    "    color=\"black\",\n",
    "    marker=\"+\",\n",
    "    alpha=0.5,\n",
    "    label=\"Outside interval\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x[~out_bounds_predictions],\n",
    "    y_normal[~out_bounds_predictions],\n",
    "    color=\"black\",\n",
    "    alpha=0.5,\n",
    "    label=\"Inside interval\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\"Quantiles of heteroscedastic Normal distributed target\")\n",
    "\n",
    "# %%\n",
    "# Since the noise is still Normally distributed, in particular is symmetric,\n",
    "# the true conditional mean and the true conditional median coincide. Indeed,\n",
    "# we see that the estimated median almost hits the true mean. We observe the\n",
    "# effect of having an increasing noise variance on the 5% and 95% quantiles:\n",
    "# the slopes of those quantiles are very different and the interval between\n",
    "# them becomes wider with increasing `x`.\n",
    "#\n",
    "# To get an additional intuition regarding the meaning of the 5% and 95%\n",
    "# quantiles estimators, one can count the number of samples above and below the\n",
    "# predicted quantiles (represented by a cross on the above plot), considering\n",
    "# that we have a total of 100 samples.\n",
    "#\n",
    "# We can repeat the same experiment using the asymmetric Pareto distributed\n",
    "# target.\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "predictions = {}\n",
    "out_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\n",
    "for quantile in quantiles:\n",
    "    qr = QuantileRegressor(quantile=quantile, alpha=0)\n",
    "    y_pred = qr.fit(X, y_pareto).predict(X)\n",
    "    predictions[quantile] = y_pred\n",
    "\n",
    "    if quantile == min(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred >= y_pareto\n",
    "        )\n",
    "    elif quantile == max(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred <= y_pareto\n",
    "        )\n",
    "\n",
    "# %%\n",
    "plt.plot(X, y_true_mean, color=\"black\", linestyle=\"dashed\", label=\"True mean\")\n",
    "\n",
    "for quantile, y_pred in predictions.items():\n",
    "    plt.plot(X, y_pred, label=f\"Quantile: {quantile}\")\n",
    "\n",
    "plt.scatter(\n",
    "    x[out_bounds_predictions],\n",
    "    y_pareto[out_bounds_predictions],\n",
    "    color=\"black\",\n",
    "    marker=\"+\",\n",
    "    alpha=0.5,\n",
    "    label=\"Outside interval\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x[~out_bounds_predictions],\n",
    "    y_pareto[~out_bounds_predictions],\n",
    "    color=\"black\",\n",
    "    alpha=0.5,\n",
    "    label=\"Inside interval\",\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\"Quantiles of asymmetric Pareto distributed target\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Due to the asymmetry of the distribution of the noise, we observe that the\n",
    "# true mean and estimated conditional median are different. We also observe\n",
    "# that each quantile model has different parameters to better fit the desired\n",
    "# quantile. Note that ideally, all quantiles would be parallel in this case,\n",
    "# which would become more visible with more data points or less extreme\n",
    "# quantiles, e.g. 10% and 90%.\n",
    "#\n",
    "# Comparing `QuantileRegressor` and `LinearRegression`\n",
    "# ----------------------------------------------------\n",
    "#\n",
    "# In this section, we will linger on the difference regarding the error that\n",
    "# :class:`~sklearn.linear_model.QuantileRegressor` and\n",
    "# :class:`~sklearn.linear_model.LinearRegression` are minimizing.\n",
    "#\n",
    "# Indeed, :class:`~sklearn.linear_model.LinearRegression` is a least squares\n",
    "# approach minimizing the mean squared error (MSE) between the training and\n",
    "# predicted targets. In contrast,\n",
    "# :class:`~sklearn.linear_model.QuantileRegressor` with `quantile=0.5`\n",
    "# minimizes the mean absolute error (MAE) instead.\n",
    "#\n",
    "# Let's first compute the training errors of such models in terms of mean\n",
    "# squared error and mean absolute error. We will use the asymmetric Pareto\n",
    "# distributed target to make it more interesting as mean and median are not\n",
    "# equal.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "quantile_regression = QuantileRegressor(quantile=0.5, alpha=0)\n",
    "\n",
    "y_pred_lr = linear_regression.fit(X, y_pareto).predict(X)\n",
    "y_pred_qr = quantile_regression.fit(X, y_pareto).predict(X)\n",
    "\n",
    "print(\n",
    "    f\"\"\"Training error (in-sample performance)\n",
    "    {linear_regression.__class__.__name__}:\n",
    "    MAE = {mean_absolute_error(y_pareto, y_pred_lr):.3f}\n",
    "    MSE = {mean_squared_error(y_pareto, y_pred_lr):.3f}\n",
    "    {quantile_regression.__class__.__name__}:\n",
    "    MAE = {mean_absolute_error(y_pareto, y_pred_qr):.3f}\n",
    "    MSE = {mean_squared_error(y_pareto, y_pred_qr):.3f}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# On the training set, we see that MAE is lower for\n",
    "# :class:`~sklearn.linear_model.QuantileRegressor` than\n",
    "# :class:`~sklearn.linear_model.LinearRegression`. In contrast to that, MSE is\n",
    "# lower for :class:`~sklearn.linear_model.LinearRegression` than\n",
    "# :class:`~sklearn.linear_model.QuantileRegressor`. These results confirms that\n",
    "# MAE is the loss minimized by :class:`~sklearn.linear_model.QuantileRegressor`\n",
    "# while MSE is the loss minimized\n",
    "# :class:`~sklearn.linear_model.LinearRegression`.\n",
    "#\n",
    "# We can make a similar evaluation by looking at the test error obtained by\n",
    "# cross-validation.\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results_lr = cross_validate(\n",
    "    linear_regression,\n",
    "    X,\n",
    "    y_pareto,\n",
    "    cv=3,\n",
    "    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n",
    ")\n",
    "cv_results_qr = cross_validate(\n",
    "    quantile_regression,\n",
    "    X,\n",
    "    y_pareto,\n",
    "    cv=3,\n",
    "    scoring=[\"neg_mean_absolute_error\", \"neg_mean_squared_error\"],\n",
    ")\n",
    "print(\n",
    "    f\"\"\"Test error (cross-validated performance)\n",
    "    {linear_regression.__class__.__name__}:\n",
    "    MAE = {-cv_results_lr[\"test_neg_mean_absolute_error\"].mean():.3f}\n",
    "    MSE = {-cv_results_lr[\"test_neg_mean_squared_error\"].mean():.3f}\n",
    "    {quantile_regression.__class__.__name__}:\n",
    "    MAE = {-cv_results_qr[\"test_neg_mean_absolute_error\"].mean():.3f}\n",
    "    MSE = {-cv_results_qr[\"test_neg_mean_squared_error\"].mean():.3f}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# We reach similar conclusions on the out-of-sample evaluation.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
