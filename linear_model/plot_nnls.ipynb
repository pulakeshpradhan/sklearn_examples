{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================\n",
    "Non-negative least squares\n",
    "==========================\n",
    "\n",
    "In this example, we fit a linear model with positive constraints on the\n",
    "regression coefficients and compare the estimated coefficients to a classic\n",
    "linear regression.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# %%\n",
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples, n_features = 200, 50\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "true_coef = 3 * np.random.randn(n_features)\n",
    "# Threshold coefficients to render them non-negative\n",
    "true_coef[true_coef < 0] = 0\n",
    "y = np.dot(X, true_coef)\n",
    "\n",
    "# Add some noise\n",
    "y += 5 * np.random.normal(size=(n_samples,))\n",
    "\n",
    "# %%\n",
    "# Split the data in train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# %%\n",
    "# Fit the Non-Negative least squares.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_nnls = LinearRegression(positive=True)\n",
    "y_pred_nnls = reg_nnls.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_nnls = r2_score(y_test, y_pred_nnls)\n",
    "print(\"NNLS R2 score\", r2_score_nnls)\n",
    "\n",
    "# %%\n",
    "# Fit an OLS.\n",
    "reg_ols = LinearRegression()\n",
    "y_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_ols = r2_score(y_test, y_pred_ols)\n",
    "print(\"OLS R2 score\", r2_score_ols)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Comparing the regression coefficients between OLS and NNLS, we can observe\n",
    "# they are highly correlated (the dashed line is the identity relation),\n",
    "# but the non-negative constraint shrinks some to 0.\n",
    "# The Non-Negative Least squares inherently yield sparse results.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(reg_ols.coef_, reg_nnls.coef_, linewidth=0, marker=\".\")\n",
    "\n",
    "low_x, high_x = ax.get_xlim()\n",
    "low_y, high_y = ax.get_ylim()\n",
    "low = max(low_x, low_y)\n",
    "high = min(high_x, high_y)\n",
    "ax.plot([low, high], [low, high], ls=\"--\", c=\".3\", alpha=0.5)\n",
    "ax.set_xlabel(\"OLS regression coefficients\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"NNLS regression coefficients\", fontweight=\"bold\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
