{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================================\n",
    "Custom refit strategy of a grid search with cross-validation\n",
    "============================================================\n",
    "\n",
    "This examples shows how a classifier is optimized by cross-validation,\n",
    "which is done using the :class:`~sklearn.model_selection.GridSearchCV` object\n",
    "on a development set that comprises only half of the available labeled data.\n",
    "\n",
    "The performance of the selected hyper-parameters and trained model is\n",
    "then measured on a dedicated evaluation set that was not used during\n",
    "the model selection step.\n",
    "\n",
    "More details on tools available for model selection can be found in the\n",
    "sections on :ref:`cross_validation` and :ref:`grid_search`.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# The dataset\n",
    "# -----------\n",
    "#\n",
    "# We will work with the `digits` dataset. The goal is to classify handwritten\n",
    "# digits images.\n",
    "# We transform the problem into a binary classification for easier\n",
    "# understanding: the goal is to identify whether a digit is `8` or not.\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# %%\n",
    "# In order to train a classifier on images, we need to flatten them into vectors.\n",
    "# Each image of 8 by 8 pixels needs to be transformed to a vector of 64 pixels.\n",
    "# Thus, we will get a final data array of shape `(n_images, n_pixels)`.\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target == 8\n",
    "print(\n",
    "    f\"The number of images is {X.shape[0]} and each image contains {X.shape[1]} pixels\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# As presented in the introduction, the data will be split into a training\n",
    "# and a testing set of equal size.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Define our grid-search strategy\n",
    "# -------------------------------\n",
    "#\n",
    "# We will select a classifier by searching the best hyper-parameters on folds\n",
    "# of the training set. To do this, we need to define\n",
    "# the scores to select the best candidate.\n",
    "\n",
    "scores = [\"precision\", \"recall\"]\n",
    "\n",
    "# %%\n",
    "# We can also define a function to be passed to the `refit` parameter of the\n",
    "# :class:`~sklearn.model_selection.GridSearchCV` instance. It will implement the\n",
    "# custom strategy to select the best candidate from the `cv_results_` attribute\n",
    "# of the :class:`~sklearn.model_selection.GridSearchCV`. Once the candidate is\n",
    "# selected, it is automatically refitted by the\n",
    "# :class:`~sklearn.model_selection.GridSearchCV` instance.\n",
    "#\n",
    "# Here, the strategy is to short-list the models which are the best in terms of\n",
    "# precision and recall. From the selected models, we finally select the fastest\n",
    "# model at predicting. Notice that these custom choices are completely\n",
    "# arbitrary.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def print_dataframe(filtered_cv_results):\n",
    "    \"\"\"Pretty print for filtered dataframe\"\"\"\n",
    "    for mean_precision, std_precision, mean_recall, std_recall, params in zip(\n",
    "        filtered_cv_results[\"mean_test_precision\"],\n",
    "        filtered_cv_results[\"std_test_precision\"],\n",
    "        filtered_cv_results[\"mean_test_recall\"],\n",
    "        filtered_cv_results[\"std_test_recall\"],\n",
    "        filtered_cv_results[\"params\"],\n",
    "    ):\n",
    "        print(\n",
    "            f\"precision: {mean_precision:0.3f} (±{std_precision:0.03f}),\"\n",
    "            f\" recall: {mean_recall:0.3f} (±{std_recall:0.03f}),\"\n",
    "            f\" for {params}\"\n",
    "        )\n",
    "    print()\n",
    "\n",
    "\n",
    "def refit_strategy(cv_results):\n",
    "    \"\"\"Define the strategy to select the best estimator.\n",
    "\n",
    "    The strategy defined here is to filter-out all results below a precision threshold\n",
    "    of 0.98, rank the remaining by recall and keep all models with one standard\n",
    "    deviation of the best by recall. Once these models are selected, we can select the\n",
    "    fastest model to predict.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv_results : dict of numpy (masked) ndarrays\n",
    "        CV results as returned by the `GridSearchCV`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_index : int\n",
    "        The index of the best estimator as it appears in `cv_results`.\n",
    "    \"\"\"\n",
    "    # print the info about the grid-search for the different scores\n",
    "    precision_threshold = 0.98\n",
    "\n",
    "    cv_results_ = pd.DataFrame(cv_results)\n",
    "    print(\"All grid-search results:\")\n",
    "    print_dataframe(cv_results_)\n",
    "\n",
    "    # Filter-out all results below the threshold\n",
    "    high_precision_cv_results = cv_results_[\n",
    "        cv_results_[\"mean_test_precision\"] > precision_threshold\n",
    "    ]\n",
    "\n",
    "    print(f\"Models with a precision higher than {precision_threshold}:\")\n",
    "    print_dataframe(high_precision_cv_results)\n",
    "\n",
    "    high_precision_cv_results = high_precision_cv_results[\n",
    "        [\n",
    "            \"mean_score_time\",\n",
    "            \"mean_test_recall\",\n",
    "            \"std_test_recall\",\n",
    "            \"mean_test_precision\",\n",
    "            \"std_test_precision\",\n",
    "            \"rank_test_recall\",\n",
    "            \"rank_test_precision\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # Select the most performant models in terms of recall\n",
    "    # (within 1 sigma from the best)\n",
    "    best_recall_std = high_precision_cv_results[\"mean_test_recall\"].std()\n",
    "    best_recall = high_precision_cv_results[\"mean_test_recall\"].max()\n",
    "    best_recall_threshold = best_recall - best_recall_std\n",
    "\n",
    "    high_recall_cv_results = high_precision_cv_results[\n",
    "        high_precision_cv_results[\"mean_test_recall\"] > best_recall_threshold\n",
    "    ]\n",
    "    print(\n",
    "        \"Out of the previously selected high precision models, we keep all the\\n\"\n",
    "        \"the models within one standard deviation of the highest recall model:\"\n",
    "    )\n",
    "    print_dataframe(high_recall_cv_results)\n",
    "\n",
    "    # From the best candidates, select the fastest model to predict\n",
    "    fastest_top_recall_high_precision_index = high_recall_cv_results[\n",
    "        \"mean_score_time\"\n",
    "    ].idxmin()\n",
    "\n",
    "    print(\n",
    "        \"\\nThe selected final model is the fastest to predict out of the previously\\n\"\n",
    "        \"selected subset of best models based on precision and recall.\\n\"\n",
    "        \"Its scoring time is:\\n\\n\"\n",
    "        f\"{high_recall_cv_results.loc[fastest_top_recall_high_precision_index]}\"\n",
    "    )\n",
    "\n",
    "    return fastest_top_recall_high_precision_index\n",
    "\n",
    "\n",
    "# %%\n",
    "#\n",
    "# Tuning hyper-parameters\n",
    "# -----------------------\n",
    "#\n",
    "# Once we defined our strategy to select the best model, we define the values\n",
    "# of the hyper-parameters and create the grid-search instance:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n",
    "    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(), tuned_parameters, scoring=scores, refit=refit_strategy\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "#\n",
    "# The parameters selected by the grid-search with our custom strategy are:\n",
    "grid_search.best_params_\n",
    "\n",
    "# %%\n",
    "#\n",
    "# Finally, we evaluate the fine-tuned model on the left-out evaluation set: the\n",
    "# `grid_search` object **has automatically been refit** on the full training\n",
    "# set with the parameters selected by our custom refit strategy.\n",
    "#\n",
    "# We can use the classification report to compute standard classification\n",
    "# metrics on the left-out set:\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# %%\n",
    "# .. note::\n",
    "#    The problem is too easy: the hyperparameter plateau is too flat and the\n",
    "#    output model is the same for precision and recall with ties in quality.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
