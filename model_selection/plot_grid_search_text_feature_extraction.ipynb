{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================================================\n",
    "Sample pipeline for text feature extraction and evaluation\n",
    "==========================================================\n",
    "\n",
    "The dataset used in this example is :ref:`20newsgroups_dataset` which will be\n",
    "automatically downloaded, cached and reused for the document classification\n",
    "example.\n",
    "\n",
    "In this example, we tune the hyperparameters of a particular classifier using a\n",
    ":class:`~sklearn.model_selection.RandomizedSearchCV`. For a demo on the\n",
    "performance of some other classifiers, see the\n",
    ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
    "notebook.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Data loading\n",
    "# ------------\n",
    "# We load two categories from the training set. You can adjust the number of\n",
    "# categories by adding their names to the list or setting `categories=None` when\n",
    "# calling the dataset loader :func:`~sklearn.datasets.fetch_20newsgroups` to get\n",
    "# the 20 of them.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "data_test = fetch_20newsgroups(\n",
    "    subset=\"test\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "\n",
    "print(f\"Loading 20 newsgroups dataset for {len(data_train.target_names)} categories:\")\n",
    "print(data_train.target_names)\n",
    "print(f\"{len(data_train.data)} documents\")\n",
    "\n",
    "# %%\n",
    "# Pipeline with hyperparameter tuning\n",
    "# -----------------------------------\n",
    "#\n",
    "# We define a pipeline combining a text feature vectorizer with a simple\n",
    "# classifier yet effective for text classification.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer()),\n",
    "        (\"clf\", ComplementNB()),\n",
    "    ]\n",
    ")\n",
    "pipeline\n",
    "\n",
    "# %%\n",
    "# We define a grid of hyperparameters to be explored by the\n",
    "# :class:`~sklearn.model_selection.RandomizedSearchCV`. Using a\n",
    "# :class:`~sklearn.model_selection.GridSearchCV` instead would explore all the\n",
    "# possible combinations on the grid, which can be costly to compute, whereas the\n",
    "# parameter `n_iter` of the :class:`~sklearn.model_selection.RandomizedSearchCV`\n",
    "# controls the number of different random combination that are evaluated. Notice\n",
    "# that setting `n_iter` larger than the number of possible combinations in a\n",
    "# grid would lead to repeating already-explored combinations. We search for the\n",
    "# best parameter combination for both the feature extraction (`vect__`) and the\n",
    "# classifier (`clf__`).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parameter_grid = {\n",
    "    \"vect__max_df\": (0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "    \"vect__min_df\": (1, 3, 5, 10),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\"),\n",
    "    \"clf__alpha\": np.logspace(-6, 6, 13),\n",
    "}\n",
    "\n",
    "# %%\n",
    "# In this case `n_iter=40` is not an exhaustive search of the hyperparameters'\n",
    "# grid. In practice it would be interesting to increase the parameter `n_iter`\n",
    "# to get a more informative analysis. As a consequence, the computional time\n",
    "# increases. We can reduce it by taking advantage of the parallelisation over\n",
    "# the parameter combinations evaluation by increasing the number of CPUs used\n",
    "# via the parameter `n_jobs`.\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=parameter_grid,\n",
    "    n_iter=40,\n",
    "    random_state=0,\n",
    "    n_jobs=2,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Hyperparameters to be evaluated:\")\n",
    "pprint(parameter_grid)\n",
    "\n",
    "# %%\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "random_search.fit(data_train.data, data_train.target)\n",
    "print(f\"Done in {time() - t0:.3f}s\")\n",
    "\n",
    "# %%\n",
    "print(\"Best parameters combination found:\")\n",
    "best_parameters = random_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameter_grid.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")\n",
    "\n",
    "# %%\n",
    "test_accuracy = random_search.score(data_test.data, data_test.target)\n",
    "print(\n",
    "    \"Accuracy of the best parameters using the inner CV of \"\n",
    "    f\"the random search: {random_search.best_score_:.3f}\"\n",
    ")\n",
    "print(f\"Accuracy on test set: {test_accuracy:.3f}\")\n",
    "\n",
    "# %%\n",
    "# The prefixes `vect` and `clf` are required to avoid possible ambiguities in\n",
    "# the pipeline, but are not necessary for visualizing the results. Because of\n",
    "# this, we define a function that will rename the tuned hyperparameters and\n",
    "# improve the readability.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def shorten_param(param_name):\n",
    "    \"\"\"Remove components' prefixes in param_name.\"\"\"\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = pd.DataFrame(random_search.cv_results_)\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "\n",
    "# %%\n",
    "# We can use a `plotly.express.scatter\n",
    "# <https://plotly.com/python-api-reference/generated/plotly.express.scatter.html>`_\n",
    "# to visualize the trade-off between scoring time and mean test score (i.e. \"CV\n",
    "# score\"). Passing the cursor over a given point displays the corresponding\n",
    "# parameters. Error bars correspond to one standard deviation as computed in the\n",
    "# different folds of the cross-validation.\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "param_names = [shorten_param(name) for name in parameter_grid.keys()]\n",
    "labels = {\n",
    "    \"mean_score_time\": \"CV Score time (s)\",\n",
    "    \"mean_test_score\": \"CV score (accuracy)\",\n",
    "}\n",
    "fig = px.scatter(\n",
    "    cv_results,\n",
    "    x=\"mean_score_time\",\n",
    "    y=\"mean_test_score\",\n",
    "    error_x=\"std_score_time\",\n",
    "    error_y=\"std_test_score\",\n",
    "    hover_data=param_names,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"trade-off between scoring time and mean test score\",\n",
    "        \"y\": 0.95,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig\n",
    "\n",
    "# %%\n",
    "# Notice that the cluster of models in the upper-left corner of the plot have\n",
    "# the best trade-off between accuracy and scoring time. In this case, using\n",
    "# bigrams increases the required scoring time without improving considerably the\n",
    "# accuracy of the pipeline.\n",
    "#\n",
    "# .. note:: For more information on how to customize an automated tuning to\n",
    "#    maximize score and minimize scoring time, see the example notebook\n",
    "#    :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`.\n",
    "#\n",
    "# We can also use a `plotly.express.parallel_coordinates\n",
    "# <https://plotly.com/python-api-reference/generated/plotly.express.parallel_coordinates.html>`_\n",
    "# to further visualize the mean test score as a function of the tuned\n",
    "# hyperparameters. This helps finding interactions between more than two\n",
    "# hyperparameters and provide intuition on their relevance for improving the\n",
    "# performance of a pipeline.\n",
    "#\n",
    "# We apply a `math.log10` transformation on the `alpha` axis to spread the\n",
    "# active range and improve the readability of the plot. A value :math:`x` on\n",
    "# said axis is to be understood as :math:`10^x`.\n",
    "\n",
    "import math\n",
    "\n",
    "column_results = param_names + [\"mean_test_score\", \"mean_score_time\"]\n",
    "\n",
    "transform_funcs = dict.fromkeys(column_results, lambda x: x)\n",
    "# Using a logarithmic scale for alpha\n",
    "transform_funcs[\"alpha\"] = math.log10\n",
    "# L1 norms are mapped to index 1, and L2 norms to index 2\n",
    "transform_funcs[\"norm\"] = lambda x: 2 if x == \"l2\" else 1\n",
    "# Unigrams are mapped to index 1 and bigrams to index 2\n",
    "transform_funcs[\"ngram_range\"] = lambda x: x[1]\n",
    "\n",
    "fig = px.parallel_coordinates(\n",
    "    cv_results[column_results].apply(transform_funcs),\n",
    "    color=\"mean_test_score\",\n",
    "    color_continuous_scale=px.colors.sequential.Viridis_r,\n",
    "    labels=labels,\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": \"Parallel coordinates plot of text classifier pipeline\",\n",
    "        \"y\": 0.99,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "fig\n",
    "\n",
    "# %%\n",
    "# The parallel coordinates plot displays the values of the hyperparameters on\n",
    "# different columns while the performance metric is color coded. It is possible\n",
    "# to select a range of results by clicking and holding on any axis of the\n",
    "# parallel coordinate plot. You can then slide (move) the range selection and\n",
    "# cross two selections to see the intersections. You can undo a selection by\n",
    "# clicking once again on the same axis.\n",
    "#\n",
    "# In particular for this hyperparameter search, it is interesting to notice that\n",
    "# the top performing models do not seem to depend on the regularization `norm`,\n",
    "# but they do depend on a trade-off between `max_df`, `min_df` and the\n",
    "# regularization strength `alpha`. The reason is that including noisy features\n",
    "# (i.e. `max_df` close to :math:`1.0` or `min_df` close to :math:`0`) tend to\n",
    "# overfit and therefore require a stronger regularization to compensate. Having\n",
    "# less features require less regularization and less scoring time.\n",
    "#\n",
    "# The best accuracy scores are obtained when `alpha` is between :math:`10^{-6}`\n",
    "# and :math:`10^0`, regardless of the hyperparameter `norm`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
