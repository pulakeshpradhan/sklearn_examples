{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Successive Halving Iterations\n",
    "=============================\n",
    "\n",
    "This example illustrates how a successive halving search\n",
    "(:class:`~sklearn.model_selection.HalvingGridSearchCV` and\n",
    ":class:`~sklearn.model_selection.HalvingRandomSearchCV`)\n",
    "iteratively chooses the best parameter combination out of\n",
    "multiple candidates.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# %%\n",
    "# We first define the parameter space and train a\n",
    "# :class:`~sklearn.model_selection.HalvingRandomSearchCV` instance.\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=400, n_features=12, random_state=rng)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=rng)\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [3, None],\n",
    "    \"max_features\": randint(1, 6),\n",
    "    \"min_samples_split\": randint(2, 11),\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "}\n",
    "\n",
    "rsh = HalvingRandomSearchCV(\n",
    "    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n",
    ")\n",
    "rsh.fit(X, y)\n",
    "\n",
    "# %%\n",
    "# We can now use the `cv_results_` attribute of the search estimator to inspect\n",
    "# and plot the evolution of the search.\n",
    "\n",
    "results = pd.DataFrame(rsh.cv_results_)\n",
    "results[\"params_str\"] = results.params.apply(str)\n",
    "results.drop_duplicates(subset=(\"params_str\", \"iter\"), inplace=True)\n",
    "mean_scores = results.pivot(\n",
    "    index=\"iter\", columns=\"params_str\", values=\"mean_test_score\"\n",
    ")\n",
    "ax = mean_scores.plot(legend=False, alpha=0.6)\n",
    "\n",
    "labels = [\n",
    "    f\"iter={i}\\nn_samples={rsh.n_resources_[i]}\\nn_candidates={rsh.n_candidates_[i]}\"\n",
    "    for i in range(rsh.n_iterations_)\n",
    "]\n",
    "\n",
    "ax.set_xticks(range(rsh.n_iterations_))\n",
    "ax.set_xticklabels(labels, rotation=45, multialignment=\"left\")\n",
    "ax.set_title(\"Scores of candidates over iterations\")\n",
    "ax.set_ylabel(\"mean test score\", fontsize=15)\n",
    "ax.set_xlabel(\"iterations\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Number of candidates and amount of resource at each iteration\n",
    "# -------------------------------------------------------------\n",
    "#\n",
    "# At the first iteration, a small amount of resources is used. The resource\n",
    "# here is the number of samples that the estimators are trained on. All\n",
    "# candidates are evaluated.\n",
    "#\n",
    "# At the second iteration, only the best half of the candidates is evaluated.\n",
    "# The number of allocated resources is doubled: candidates are evaluated on\n",
    "# twice as many samples.\n",
    "#\n",
    "# This process is repeated until the last iteration, where only 2 candidates\n",
    "# are left. The best candidate is the candidate that has the best score at the\n",
    "# last iteration.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
