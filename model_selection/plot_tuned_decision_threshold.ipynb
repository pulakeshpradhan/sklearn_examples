{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852963c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Post-hoc tuning the cut-off point of decision function\n",
    "======================================================\n",
    "\n",
    "Once a binary classifier is trained, the :term:`predict` method outputs class label\n",
    "predictions corresponding to a thresholding of either the :term:`decision_function` or\n",
    "the :term:`predict_proba` output. The default threshold is defined as a posterior\n",
    "probability estimate of 0.5 or a decision score of 0.0. However, this default strategy\n",
    "may not be optimal for the task at hand.\n",
    "\n",
    "This example shows how to use the\n",
    ":class:`~sklearn.model_selection.TunedThresholdClassifierCV` to tune the decision\n",
    "threshold, depending on a metric of interest.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# The diabetes dataset\n",
    "# --------------------\n",
    "#\n",
    "# To illustrate the tuning of the decision threshold, we will use the diabetes dataset.\n",
    "# This dataset is available on OpenML: https://www.openml.org/d/37. We use the\n",
    "# :func:`~sklearn.datasets.fetch_openml` function to fetch this dataset.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "diabetes = fetch_openml(data_id=37, as_frame=True, parser=\"pandas\")\n",
    "data, target = diabetes.data, diabetes.target\n",
    "\n",
    "# %%\n",
    "# We look at the target to understand the type of problem we are dealing with.\n",
    "target.value_counts()\n",
    "\n",
    "# %%\n",
    "# We can see that we are dealing with a binary classification problem. Since the\n",
    "# labels are not encoded as 0 and 1, we make it explicit that we consider the class\n",
    "# labeled \"tested_negative\" as the negative class (which is also the most frequent)\n",
    "# and the class labeled \"tested_positive\" the positive as the positive class:\n",
    "neg_label, pos_label = target.value_counts().index\n",
    "\n",
    "# %%\n",
    "# We can also observe that this binary problem is slightly imbalanced where we have\n",
    "# around twice more samples from the negative class than from the positive class. When\n",
    "# it comes to evaluation, we should consider this aspect to interpret the results.\n",
    "#\n",
    "# Our vanilla classifier\n",
    "# ----------------------\n",
    "#\n",
    "# We define a basic predictive model composed of a scaler followed by a logistic\n",
    "# regression classifier.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "model\n",
    "\n",
    "# %%\n",
    "# We evaluate our model using cross-validation. We use the accuracy and the balanced\n",
    "# accuracy to report the performance of our model. The balanced accuracy is a metric\n",
    "# that is less sensitive to class imbalance and will allow us to put the accuracy\n",
    "# score in perspective.\n",
    "#\n",
    "# Cross-validation allows us to study the variance of the decision threshold across\n",
    "# different splits of the data. However, the dataset is rather small and it would be\n",
    "# detrimental to use more than 5 folds to evaluate the dispersion. Therefore, we use\n",
    "# a :class:`~sklearn.model_selection.RepeatedStratifiedKFold` where we apply several\n",
    "# repetitions of 5-fold cross-validation.\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "\n",
    "scoring = [\"accuracy\", \"balanced_accuracy\"]\n",
    "cv_scores = [\n",
    "    \"train_accuracy\",\n",
    "    \"test_accuracy\",\n",
    "    \"train_balanced_accuracy\",\n",
    "    \"test_balanced_accuracy\",\n",
    "]\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "cv_results_vanilla_model = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        model,\n",
    "        data,\n",
    "        target,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "    )\n",
    ")\n",
    "cv_results_vanilla_model[cv_scores].aggregate([\"mean\", \"std\"]).T\n",
    "\n",
    "# %%\n",
    "# Our predictive model succeeds to grasp the relationship between the data and the\n",
    "# target. The training and testing scores are close to each other, meaning that our\n",
    "# predictive model is not overfitting. We can also observe that the balanced accuracy is\n",
    "# lower than the accuracy, due to the class imbalance previously mentioned.\n",
    "#\n",
    "# For this classifier, we let the decision threshold, used convert the probability of\n",
    "# the positive class into a class prediction, to its default value: 0.5. However, this\n",
    "# threshold might not be optimal. If our interest is to maximize the balanced accuracy,\n",
    "# we should select another threshold that would maximize this metric.\n",
    "#\n",
    "# The :class:`~sklearn.model_selection.TunedThresholdClassifierCV` meta-estimator allows\n",
    "# to tune the decision threshold of a classifier given a metric of interest.\n",
    "#\n",
    "# Tuning the decision threshold\n",
    "# -----------------------------\n",
    "#\n",
    "# We create a :class:`~sklearn.model_selection.TunedThresholdClassifierCV` and\n",
    "# configure it to maximize the balanced accuracy. We evaluate the model using the same\n",
    "# cross-validation strategy as previously.\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "\n",
    "tuned_model = TunedThresholdClassifierCV(estimator=model, scoring=\"balanced_accuracy\")\n",
    "cv_results_tuned_model = pd.DataFrame(\n",
    "    cross_validate(\n",
    "        tuned_model,\n",
    "        data,\n",
    "        target,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "    )\n",
    ")\n",
    "cv_results_tuned_model[cv_scores].aggregate([\"mean\", \"std\"]).T\n",
    "\n",
    "# %%\n",
    "# In comparison with the vanilla model, we observe that the balanced accuracy score\n",
    "# increased. Of course, it comes at the cost of a lower accuracy score. It means that\n",
    "# our model is now more sensitive to the positive class but makes more mistakes on the\n",
    "# negative class.\n",
    "#\n",
    "# However, it is important to note that this tuned predictive model is internally the\n",
    "# same model as the vanilla model: they have the same fitted coefficients.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vanilla_model_coef = pd.DataFrame(\n",
    "    [est[-1].coef_.ravel() for est in cv_results_vanilla_model[\"estimator\"]],\n",
    "    columns=diabetes.feature_names,\n",
    ")\n",
    "tuned_model_coef = pd.DataFrame(\n",
    "    [est.estimator_[-1].coef_.ravel() for est in cv_results_tuned_model[\"estimator\"]],\n",
    "    columns=diabetes.feature_names,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(12, 4), sharex=True, sharey=True)\n",
    "vanilla_model_coef.boxplot(ax=ax[0])\n",
    "ax[0].set_ylabel(\"Coefficient value\")\n",
    "ax[0].set_title(\"Vanilla model\")\n",
    "tuned_model_coef.boxplot(ax=ax[1])\n",
    "ax[1].set_title(\"Tuned model\")\n",
    "_ = fig.suptitle(\"Coefficients of the predictive models\")\n",
    "\n",
    "# %%\n",
    "# Only the decision threshold of each model was changed during the cross-validation.\n",
    "decision_threshold = pd.Series(\n",
    "    [est.best_threshold_ for est in cv_results_tuned_model[\"estimator\"]],\n",
    ")\n",
    "ax = decision_threshold.plot.kde()\n",
    "ax.axvline(\n",
    "    decision_threshold.mean(),\n",
    "    color=\"k\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Mean decision threshold: {decision_threshold.mean():.2f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Decision threshold\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "_ = ax.set_title(\n",
    "    \"Distribution of the decision threshold \\nacross different cross-validation folds\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# In average, a decision threshold around 0.32 maximizes the balanced accuracy, which is\n",
    "# different from the default decision threshold of 0.5. Thus tuning the decision\n",
    "# threshold is particularly important when the output of the predictive model\n",
    "# is used to make decisions. Besides, the metric used to tune the decision threshold\n",
    "# should be chosen carefully. Here, we used the balanced accuracy but it might not be\n",
    "# the most appropriate metric for the problem at hand. The choice of the \"right\" metric\n",
    "# is usually problem-dependent and might require some domain knowledge. Refer to the\n",
    "# example entitled,\n",
    "# :ref:`sphx_glr_auto_examples_model_selection_plot_cost_sensitive_learning.py`,\n",
    "# for more details.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
