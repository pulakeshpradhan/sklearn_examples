{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe94290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "Statistical comparison of models using grid search\n",
    "==================================================\n",
    "\n",
    "This example illustrates how to statistically compare the performance of models\n",
    "trained and evaluated using :class:`~sklearn.model_selection.GridSearchCV`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# We will start by simulating moon shaped data (where the ideal separation\n",
    "# between classes is non-linear), adding to it a moderate degree of noise.\n",
    "# Datapoints will belong to one of two possible classes to be predicted by two\n",
    "# features. We will simulate 50 samples for each class:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(noise=0.352, random_state=1, n_samples=100)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=X[:, 0], y=X[:, 1], hue=y, marker=\"o\", s=25, edgecolor=\"k\", legend=False\n",
    ").set_title(\"Data\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# We will compare the performance of :class:`~sklearn.svm.SVC` estimators that\n",
    "# vary on their `kernel` parameter, to decide which choice of this\n",
    "# hyper-parameter predicts our simulated data best.\n",
    "# We will evaluate the performance of the models using\n",
    "# :class:`~sklearn.model_selection.RepeatedStratifiedKFold`, repeating 10 times\n",
    "# a 10-fold stratified cross validation using a different randomization of the\n",
    "# data in each repetition. The performance will be evaluated using\n",
    "# :class:`~sklearn.metrics.roc_auc_score`.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = [\n",
    "    {\"kernel\": [\"linear\"]},\n",
    "    {\"kernel\": [\"poly\"], \"degree\": [2, 3]},\n",
    "    {\"kernel\": [\"rbf\"]},\n",
    "]\n",
    "\n",
    "svc = SVC(random_state=0)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=0)\n",
    "\n",
    "search = GridSearchCV(estimator=svc, param_grid=param_grid, scoring=\"roc_auc\", cv=cv)\n",
    "search.fit(X, y)\n",
    "\n",
    "# %%\n",
    "# We can now inspect the results of our search, sorted by their\n",
    "# `mean_test_score`:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(search.cv_results_)\n",
    "results_df = results_df.sort_values(by=[\"rank_test_score\"])\n",
    "results_df = results_df.set_index(\n",
    "    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    ").rename_axis(\"kernel\")\n",
    "results_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n",
    "\n",
    "# %%\n",
    "# We can see that the estimator using the `'rbf'` kernel performed best,\n",
    "# closely followed by `'linear'`. Both estimators with a `'poly'` kernel\n",
    "# performed worse, with the one using a two-degree polynomial achieving a much\n",
    "# lower performance than all other models.\n",
    "#\n",
    "# Usually, the analysis just ends here, but half the story is missing. The\n",
    "# output of :class:`~sklearn.model_selection.GridSearchCV` does not provide\n",
    "# information on the certainty of the differences between the models.\n",
    "# We don't know if these are **statistically** significant.\n",
    "# To evaluate this, we need to conduct a statistical test.\n",
    "# Specifically, to contrast the performance of two models we should\n",
    "# statistically compare their AUC scores. There are 100 samples (AUC\n",
    "# scores) for each model as we repreated 10 times a 10-fold cross-validation.\n",
    "#\n",
    "# However, the scores of the models are not independent: all models are\n",
    "# evaluated on the **same** 100 partitions, increasing the correlation\n",
    "# between the performance of the models.\n",
    "# Since some partitions of the data can make the distinction of the classes\n",
    "# particularly easy or hard to find for all models, the models scores will\n",
    "# co-vary.\n",
    "#\n",
    "# Let's inspect this partition effect by plotting the performance of all models\n",
    "# in each fold, and calculating the correlation between models across folds:\n",
    "\n",
    "# create df of model scores ordered by performance\n",
    "model_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n",
    "\n",
    "# plot 30 examples of dependency between cv fold and AUC scores\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=model_scores.transpose().iloc[:30],\n",
    "    dashes=False,\n",
    "    palette=\"Set1\",\n",
    "    marker=\"o\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\n",
    "ax.set_ylabel(\"Model AUC\", size=12)\n",
    "ax.tick_params(bottom=True, labelbottom=False)\n",
    "plt.show()\n",
    "\n",
    "# print correlation of AUC scores across folds\n",
    "print(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n",
    "\n",
    "# %%\n",
    "# We can observe that the performance of the models highly depends on the fold.\n",
    "#\n",
    "# As a consequence, if we assume independence between samples we will be\n",
    "# underestimating the variance computed in our statistical tests, increasing\n",
    "# the number of false positive errors (i.e. detecting a significant difference\n",
    "# between models when such does not exist) [1]_.\n",
    "#\n",
    "# Several variance-corrected statistical tests have been developed for these\n",
    "# cases. In this example we will show how to implement one of them (the so\n",
    "# called Nadeau and Bengio's corrected t-test) under two different statistical\n",
    "# frameworks: frequentist and Bayesian.\n",
    "\n",
    "# %%\n",
    "# Comparing two models: frequentist approach\n",
    "# ------------------------------------------\n",
    "#\n",
    "# We can start by asking: \"Is the first model significantly better than the\n",
    "# second model (when ranked by `mean_test_score`)?\"\n",
    "#\n",
    "# To answer this question using a frequentist approach we could\n",
    "# run a paired t-test and compute the p-value. This is also known as\n",
    "# Diebold-Mariano test in the forecast literature [5]_.\n",
    "# Many variants of such a t-test have been developed to account for the\n",
    "# 'non-independence of samples problem'\n",
    "# described in the previous section. We will use the one proven to obtain the\n",
    "# highest replicability scores (which rate how similar the performance of a\n",
    "# model is when evaluating it on different random partitions of the same\n",
    "# dataset) while maintaining a low rate of false positives and false negatives:\n",
    "# the Nadeau and Bengio's corrected t-test [2]_ that uses a 10 times repeated\n",
    "# 10-fold cross validation [3]_.\n",
    "#\n",
    "# This corrected paired t-test is computed as:\n",
    "#\n",
    "# .. math::\n",
    "#    t=\\frac{\\frac{1}{k \\cdot r}\\sum_{i=1}^{k}\\sum_{j=1}^{r}x_{ij}}\n",
    "#    {\\sqrt{(\\frac{1}{k \\cdot r}+\\frac{n_{test}}{n_{train}})\\hat{\\sigma}^2}}\n",
    "#\n",
    "# where :math:`k` is the number of folds,\n",
    "# :math:`r` the number of repetitions in the cross-validation,\n",
    "# :math:`x` is the difference in performance of the models,\n",
    "# :math:`n_{test}` is the number of samples used for testing,\n",
    "# :math:`n_{train}` is the number of samples used for training,\n",
    "# and :math:`\\hat{\\sigma}^2` represents the variance of the observed\n",
    "# differences.\n",
    "#\n",
    "# Let's implement a corrected right-tailed paired t-test to evaluate if the\n",
    "# performance of the first model is significantly better than that of the\n",
    "# second model. Our null hypothesis is that the second model performs at least\n",
    "# as good as the first model.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def corrected_std(differences, n_train, n_test):\n",
    "    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : ndarray of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrected_std : float\n",
    "        Variance-corrected standard deviation of the set of differences.\n",
    "    \"\"\"\n",
    "    # kr = k times r, r times repeated k-fold crossvalidation,\n",
    "    # kr equals the number of times the model was evaluated\n",
    "    kr = len(differences)\n",
    "    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\n",
    "    corrected_std = np.sqrt(corrected_var)\n",
    "    return corrected_std\n",
    "\n",
    "\n",
    "def compute_corrected_ttest(differences, df, n_train, n_test):\n",
    "    \"\"\"Computes right-tailed paired t-test with corrected variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : array-like of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    df : int\n",
    "        Degrees of freedom.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Variance-corrected t-statistic.\n",
    "    p_val : float\n",
    "        Variance-corrected p-value.\n",
    "    \"\"\"\n",
    "    mean = np.mean(differences)\n",
    "    std = corrected_std(differences, n_train, n_test)\n",
    "    t_stat = mean / std\n",
    "    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n",
    "    return t_stat, p_val\n",
    "\n",
    "\n",
    "# %%\n",
    "model_1_scores = model_scores.iloc[0].values  # scores of the best model\n",
    "model_2_scores = model_scores.iloc[1].values  # scores of the second-best model\n",
    "\n",
    "differences = model_1_scores - model_2_scores\n",
    "\n",
    "n = differences.shape[0]  # number of test sets\n",
    "df = n - 1\n",
    "n_train = len(list(cv.split(X, y))[0][0])\n",
    "n_test = len(list(cv.split(X, y))[0][1])\n",
    "\n",
    "t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n",
    "print(f\"Corrected t-value: {t_stat:.3f}\\nCorrected p-value: {p_val:.3f}\")\n",
    "\n",
    "# %%\n",
    "# We can compare the corrected t- and p-values with the uncorrected ones:\n",
    "\n",
    "t_stat_uncorrected = np.mean(differences) / np.sqrt(np.var(differences, ddof=1) / n)\n",
    "p_val_uncorrected = t.sf(np.abs(t_stat_uncorrected), df)\n",
    "\n",
    "print(\n",
    "    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\n\"\n",
    "    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Using the conventional significance alpha level at `p=0.05`, we observe that\n",
    "# the uncorrected t-test concludes that the first model is significantly better\n",
    "# than the second.\n",
    "#\n",
    "# With the corrected approach, in contrast, we fail to detect this difference.\n",
    "#\n",
    "# In the latter case, however, the frequentist approach does not let us\n",
    "# conclude that the first and second model have an equivalent performance. If\n",
    "# we wanted to make this assertion we need to use a Bayesian approach.\n",
    "\n",
    "# %%\n",
    "# Comparing two models: Bayesian approach\n",
    "# ---------------------------------------\n",
    "# We can use Bayesian estimation to calculate the probability that the first\n",
    "# model is better than the second. Bayesian estimation will output a\n",
    "# distribution followed by the mean :math:`\\mu` of the differences in the\n",
    "# performance of two models.\n",
    "#\n",
    "# To obtain the posterior distribution we need to define a prior that models\n",
    "# our beliefs of how the mean is distributed before looking at the data,\n",
    "# and multiply it by a likelihood function that computes how likely our\n",
    "# observed differences are, given the values that the mean of differences\n",
    "# could take.\n",
    "#\n",
    "# Bayesian estimation can be carried out in many forms to answer our question,\n",
    "# but in this example we will implement the approach suggested by Benavoli and\n",
    "# colleagues [4]_.\n",
    "#\n",
    "# One way of defining our posterior using a closed-form expression is to select\n",
    "# a prior conjugate to the likelihood function. Benavoli and colleagues [4]_\n",
    "# show that when comparing the performance of two classifiers we can model the\n",
    "# prior as a Normal-Gamma distribution (with both mean and variance unknown)\n",
    "# conjugate to a normal likelihood, to thus express the posterior as a normal\n",
    "# distribution.\n",
    "# Marginalizing out the variance from this normal posterior, we can define the\n",
    "# posterior of the mean parameter as a Student's t-distribution. Specifically:\n",
    "#\n",
    "# .. math::\n",
    "#    St(\\mu;n-1,\\overline{x},(\\frac{1}{n}+\\frac{n_{test}}{n_{train}})\n",
    "#    \\hat{\\sigma}^2)\n",
    "#\n",
    "# where :math:`n` is the total number of samples,\n",
    "# :math:`\\overline{x}` represents the mean difference in the scores,\n",
    "# :math:`n_{test}` is the number of samples used for testing,\n",
    "# :math:`n_{train}` is the number of samples used for training,\n",
    "# and :math:`\\hat{\\sigma}^2` represents the variance of the observed\n",
    "# differences.\n",
    "#\n",
    "# Notice that we are using Nadeau and Bengio's corrected variance in our\n",
    "# Bayesian approach as well.\n",
    "#\n",
    "# Let's compute and plot the posterior:\n",
    "\n",
    "# initialize random variable\n",
    "t_post = t(\n",
    "    df, loc=np.mean(differences), scale=corrected_std(differences, n_train, n_test)\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Let's plot the posterior distribution:\n",
    "\n",
    "x = np.linspace(t_post.ppf(0.001), t_post.ppf(0.999), 100)\n",
    "\n",
    "plt.plot(x, t_post.pdf(x))\n",
    "plt.xticks(np.arange(-0.04, 0.06, 0.01))\n",
    "plt.fill_between(x, t_post.pdf(x), 0, facecolor=\"blue\", alpha=0.2)\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlabel(r\"Mean difference ($\\mu$)\")\n",
    "plt.title(\"Posterior distribution\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# We can calculate the probability that the first model is better than the\n",
    "# second by computing the area under the curve of the posterior distribution\n",
    "# from zero to infinity. And also the reverse: we can calculate the probability\n",
    "# that the second model is better than the first by computing the area under\n",
    "# the curve from minus infinity to zero.\n",
    "\n",
    "better_prob = 1 - t_post.cdf(0)\n",
    "\n",
    "print(\n",
    "    f\"Probability of {model_scores.index[0]} being more accurate than \"\n",
    "    f\"{model_scores.index[1]}: {better_prob:.3f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Probability of {model_scores.index[1]} being more accurate than \"\n",
    "    f\"{model_scores.index[0]}: {1 - better_prob:.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# In contrast with the frequentist approach, we can compute the probability\n",
    "# that one model is better than the other.\n",
    "#\n",
    "# Note that we obtained similar results as those in the frequentist approach.\n",
    "# Given our choice of priors, we are essentially performing the same\n",
    "# computations, but we are allowed to make different assertions.\n",
    "\n",
    "# %%\n",
    "# Region of Practical Equivalence\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Sometimes we are interested in determining the probabilities that our models\n",
    "# have an equivalent performance, where \"equivalent\" is defined in a practical\n",
    "# way. A naive approach [4]_ would be to define estimators as practically\n",
    "# equivalent when they differ by less than 1% in their accuracy. But we could\n",
    "# also define this practical equivalence taking into account the problem we are\n",
    "# trying to solve. For example, a difference of 5% in accuracy would mean an\n",
    "# increase of $1000 in sales, and we consider any quantity above that as\n",
    "# relevant for our business.\n",
    "#\n",
    "# In this example we are going to define the\n",
    "# Region of Practical Equivalence (ROPE) to be :math:`[-0.01, 0.01]`. That is,\n",
    "# we will consider two models as practically equivalent if they differ by less\n",
    "# than 1% in their performance.\n",
    "#\n",
    "# To compute the probabilities of the classifiers being practically equivalent,\n",
    "# we calculate the area under the curve of the posterior over the ROPE\n",
    "# interval:\n",
    "\n",
    "rope_interval = [-0.01, 0.01]\n",
    "rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n",
    "\n",
    "print(\n",
    "    f\"Probability of {model_scores.index[0]} and {model_scores.index[1]} \"\n",
    "    f\"being practically equivalent: {rope_prob:.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# We can plot how the posterior is distributed over the ROPE interval:\n",
    "\n",
    "x_rope = np.linspace(rope_interval[0], rope_interval[1], 100)\n",
    "\n",
    "plt.plot(x, t_post.pdf(x))\n",
    "plt.xticks(np.arange(-0.04, 0.06, 0.01))\n",
    "plt.vlines([-0.01, 0.01], ymin=0, ymax=(np.max(t_post.pdf(x)) + 1))\n",
    "plt.fill_between(x_rope, t_post.pdf(x_rope), 0, facecolor=\"blue\", alpha=0.2)\n",
    "plt.ylabel(\"Probability density\")\n",
    "plt.xlabel(r\"Mean difference ($\\mu$)\")\n",
    "plt.title(\"Posterior distribution under the ROPE\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# As suggested in [4]_, we can further interpret these probabilities using the\n",
    "# same criteria as the frequentist approach: is the probability of falling\n",
    "# inside the ROPE bigger than 95% (alpha value of 5%)?  In that case we can\n",
    "# conclude that both models are practically equivalent.\n",
    "\n",
    "# %%\n",
    "# The Bayesian estimation approach also allows us to compute how uncertain we\n",
    "# are about our estimation of the difference. This can be calculated using\n",
    "# credible intervals. For a given probability, they show the range of values\n",
    "# that the estimated quantity, in our case the mean difference in\n",
    "# performance, can take.\n",
    "# For example, a 50% credible interval [x, y] tells us that there is a 50%\n",
    "# probability that the true (mean) difference of performance between models is\n",
    "# between x and y.\n",
    "#\n",
    "# Let's determine the credible intervals of our data using 50%, 75% and 95%:\n",
    "\n",
    "cred_intervals = []\n",
    "intervals = [0.5, 0.75, 0.95]\n",
    "\n",
    "for interval in intervals:\n",
    "    cred_interval = list(t_post.interval(interval))\n",
    "    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n",
    "\n",
    "cred_int_df = pd.DataFrame(\n",
    "    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n",
    ").set_index(\"interval\")\n",
    "cred_int_df\n",
    "\n",
    "# %%\n",
    "# As shown in the table, there is a 50% probability that the true mean\n",
    "# difference between models will be between 0.000977 and 0.019023, 70%\n",
    "# probability that it will be between -0.005422 and 0.025422, and 95%\n",
    "# probability that it will be between -0.016445\tand 0.036445.\n",
    "\n",
    "# %%\n",
    "# Pairwise comparison of all models: frequentist approach\n",
    "# -------------------------------------------------------\n",
    "#\n",
    "# We could also be interested in comparing the performance of all our models\n",
    "# evaluated with :class:`~sklearn.model_selection.GridSearchCV`. In this case\n",
    "# we would be running our statistical test multiple times, which leads us to\n",
    "# the `multiple comparisons problem\n",
    "# <https://en.wikipedia.org/wiki/Multiple_comparisons_problem>`_.\n",
    "#\n",
    "# There are many possible ways to tackle this problem, but a standard approach\n",
    "# is to apply a `Bonferroni correction\n",
    "# <https://en.wikipedia.org/wiki/Bonferroni_correction>`_. Bonferroni can be\n",
    "# computed by multiplying the p-value by the number of comparisons we are\n",
    "# testing.\n",
    "#\n",
    "# Let's compare the performance of the models using the corrected t-test:\n",
    "\n",
    "from itertools import combinations\n",
    "from math import factorial\n",
    "\n",
    "n_comparisons = factorial(len(model_scores)) / (\n",
    "    factorial(2) * factorial(len(model_scores) - 2)\n",
    ")\n",
    "pairwise_t_test = []\n",
    "\n",
    "for model_i, model_k in combinations(range(len(model_scores)), 2):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    model_k_scores = model_scores.iloc[model_k].values\n",
    "    differences = model_i_scores - model_k_scores\n",
    "    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n",
    "    p_val *= n_comparisons  # implement Bonferroni correction\n",
    "    # Bonferroni can output p-values higher than 1\n",
    "    p_val = 1 if p_val > 1 else p_val\n",
    "    pairwise_t_test.append(\n",
    "        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n",
    "    )\n",
    "\n",
    "pairwise_comp_df = pd.DataFrame(\n",
    "    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n",
    ").round(3)\n",
    "pairwise_comp_df\n",
    "\n",
    "# %%\n",
    "# We observe that after correcting for multiple comparisons, the only model\n",
    "# that significantly differs from the others is `'2_poly'`.\n",
    "# `'rbf'`, the model ranked first by\n",
    "# :class:`~sklearn.model_selection.GridSearchCV`, does not significantly\n",
    "# differ from `'linear'` or `'3_poly'`.\n",
    "\n",
    "# %%\n",
    "# Pairwise comparison of all models: Bayesian approach\n",
    "# ----------------------------------------------------\n",
    "#\n",
    "# When using Bayesian estimation to compare multiple models, we don't need to\n",
    "# correct for multiple comparisons (for reasons why see [4]_).\n",
    "#\n",
    "# We can carry out our pairwise comparisons the same way as in the first\n",
    "# section:\n",
    "\n",
    "pairwise_bayesian = []\n",
    "\n",
    "for model_i, model_k in combinations(range(len(model_scores)), 2):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    model_k_scores = model_scores.iloc[model_k].values\n",
    "    differences = model_i_scores - model_k_scores\n",
    "    t_post = t(\n",
    "        df, loc=np.mean(differences), scale=corrected_std(differences, n_train, n_test)\n",
    "    )\n",
    "    worse_prob = t_post.cdf(rope_interval[0])\n",
    "    better_prob = 1 - t_post.cdf(rope_interval[1])\n",
    "    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n",
    "\n",
    "    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n",
    "\n",
    "pairwise_bayesian_df = pd.DataFrame(\n",
    "    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n",
    ").round(3)\n",
    "\n",
    "pairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\n",
    "pairwise_comp_df\n",
    "\n",
    "# %%\n",
    "# Using the Bayesian approach we can compute the probability that a model\n",
    "# performs better, worse or practically equivalent to another.\n",
    "#\n",
    "# Results show that the model ranked first by\n",
    "# :class:`~sklearn.model_selection.GridSearchCV` `'rbf'`, has approximately a\n",
    "# 6.8% chance of being worse than `'linear'`, and a 1.8% chance of being worse\n",
    "# than `'3_poly'`.\n",
    "# `'rbf'` and `'linear'` have a 43% probability of being practically\n",
    "# equivalent, while `'rbf'` and `'3_poly'` have a 10% chance of being so.\n",
    "#\n",
    "# Similarly to the conclusions obtained using the frequentist approach, all\n",
    "# models have a 100% probability of being better than `'2_poly'`, and none have\n",
    "# a practically equivalent performance with the latter.\n",
    "\n",
    "# %%\n",
    "# Take-home messages\n",
    "# ------------------\n",
    "# - Small differences in performance measures might easily turn out to be\n",
    "#   merely by chance, but not because one model predicts systematically better\n",
    "#   than the other. As shown in this example, statistics can tell you how\n",
    "#   likely that is.\n",
    "# - When statistically comparing the performance of two models evaluated in\n",
    "#   GridSearchCV, it is necessary to correct the calculated variance which\n",
    "#   could be underestimated since the scores of the models are not independent\n",
    "#   from each other.\n",
    "# - A frequentist approach that uses a (variance-corrected) paired t-test can\n",
    "#   tell us if the performance of one model is better than another with a\n",
    "#   degree of certainty above chance.\n",
    "# - A Bayesian approach can provide the probabilities of one model being\n",
    "#   better, worse or practically equivalent than another. It can also tell us\n",
    "#   how confident we are of knowing that the true differences of our models\n",
    "#   fall under a certain range of values.\n",
    "# - If multiple models are statistically compared, a multiple comparisons\n",
    "#   correction is needed when using the frequentist approach.\n",
    "\n",
    "# %%\n",
    "# .. rubric:: References\n",
    "#\n",
    "# .. [1] Dietterich, T. G. (1998). `Approximate statistical tests for\n",
    "#        comparing supervised classification learning algorithms\n",
    "#        <http://web.cs.iastate.edu/~jtian/cs573/Papers/Dietterich-98.pdf>`_.\n",
    "#        Neural computation, 10(7).\n",
    "# .. [2] Nadeau, C., & Bengio, Y. (2000). `Inference for the generalization\n",
    "#        error\n",
    "#        <https://papers.nips.cc/paper/1661-inference-for-the-generalization-error.pdf>`_.\n",
    "#        In Advances in neural information processing systems.\n",
    "# .. [3] Bouckaert, R. R., & Frank, E. (2004). `Evaluating the replicability\n",
    "#        of significance tests for comparing learning algorithms\n",
    "#        <https://www.cms.waikato.ac.nz/~ml/publications/2004/bouckaert-frank.pdf>`_.\n",
    "#        In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\n",
    "# .. [4] Benavoli, A., Corani, G., Dem≈°ar, J., & Zaffalon, M. (2017). `Time\n",
    "#        for a change: a tutorial for comparing multiple classifiers through\n",
    "#        Bayesian analysis\n",
    "#        <http://www.jmlr.org/papers/volume18/16-305/16-305.pdf>`_.\n",
    "#        The Journal of Machine Learning Research, 18(1). See the Python\n",
    "#        library that accompanies this paper `here\n",
    "#        <https://github.com/janezd/baycomp>`_.\n",
    "# .. [5] Diebold, F.X. & Mariano R.S. (1995). `Comparing predictive accuracy\n",
    "#        <http://www.est.uc3m.es/esp/nueva_docencia/comp_col_get/lade/tecnicas_prediccion/Practicas0708/Comparing%20Predictive%20Accuracy%20(Dielbold).pdf>`_\n",
    "#        Journal of Business & economic statistics, 20(1), 134-144.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
