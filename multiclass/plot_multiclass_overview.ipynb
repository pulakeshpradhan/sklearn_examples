{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314db819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================\n",
    "Overview of multiclass training meta-estimators\n",
    "===============================================\n",
    "\n",
    "In this example, we discuss the problem of classification when the target\n",
    "variable is composed of more than two classes. This is called multiclass\n",
    "classification.\n",
    "\n",
    "In scikit-learn, all estimators support multiclass classification out of the\n",
    "box: the most sensible strategy was implemented for the end-user. The\n",
    ":mod:`sklearn.multiclass` module implements various strategies that one can use\n",
    "for experimenting or developing third-party estimators that only support binary\n",
    "classification.\n",
    "\n",
    ":mod:`sklearn.multiclass` includes OvO/OvR strategies used to train a\n",
    "multiclass classifier by fitting a set of binary classifiers (the\n",
    ":class:`~sklearn.multiclass.OneVsOneClassifier` and\n",
    ":class:`~sklearn.multiclass.OneVsRestClassifier` meta-estimators). This example\n",
    "will review them.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# The Yeast UCI dataset\n",
    "# ---------------------\n",
    "#\n",
    "# In this example, we use a UCI dataset [1]_, generally referred as the Yeast\n",
    "# dataset. We use the :func:`sklearn.datasets.fetch_openml` function to load\n",
    "# the dataset from OpenML.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(data_id=181, as_frame=True, return_X_y=True)\n",
    "\n",
    "# %%\n",
    "# To know the type of data science problem we are dealing with, we can check\n",
    "# the target for which we want to build a predictive model.\n",
    "y.value_counts().sort_index()\n",
    "\n",
    "# %%\n",
    "# We see that the target is discrete and composed of 10 classes. We therefore\n",
    "# deal with a multiclass classification problem.\n",
    "#\n",
    "# Strategies comparison\n",
    "# ---------------------\n",
    "#\n",
    "# In the following experiment, we use a\n",
    "# :class:`~sklearn.tree.DecisionTreeClassifier` and a\n",
    "# :class:`~sklearn.model_selection.RepeatedStratifiedKFold` cross-validation\n",
    "# with 3 splits and 5 repetitions.\n",
    "#\n",
    "# We compare the following strategies:\n",
    "#\n",
    "# * :class:~sklearn.tree.DecisionTreeClassifier can handle multiclass\n",
    "#   classification without needing any special adjustments. It works by breaking\n",
    "#   down the training data into smaller subsets and focusing on the most common\n",
    "#   class in each subset. By repeating this process, the model can accurately\n",
    "#   classify input data into multiple different classes.\n",
    "# * :class:`~sklearn.multiclass.OneVsOneClassifier` trains a set of binary\n",
    "#   classifiers where each classifier is trained to distinguish between\n",
    "#   two classes.\n",
    "# * :class:`~sklearn.multiclass.OneVsRestClassifier`: trains a set of binary\n",
    "#   classifiers where each classifier is trained to distinguish between\n",
    "#   one class and the rest of the classes.\n",
    "# * :class:`~sklearn.multiclass.OutputCodeClassifier`: trains a set of binary\n",
    "#   classifiers where each classifier is trained to distinguish between\n",
    "#   a set of classes from the rest of the classes. The set of classes is\n",
    "#   defined by a codebook, which is randomly generated in scikit-learn. This\n",
    "#   method exposes a parameter `code_size` to control the size of the codebook.\n",
    "#   We set it above one since we are not interested in compressing the class\n",
    "#   representation.\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn.multiclass import (\n",
    "    OneVsOneClassifier,\n",
    "    OneVsRestClassifier,\n",
    "    OutputCodeClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "ovo_tree = OneVsOneClassifier(tree)\n",
    "ovr_tree = OneVsRestClassifier(tree)\n",
    "ecoc = OutputCodeClassifier(tree, code_size=2)\n",
    "\n",
    "cv_results_tree = cross_validate(tree, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ovo = cross_validate(ovo_tree, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ovr = cross_validate(ovr_tree, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ecoc = cross_validate(ecoc, X, y, cv=cv, n_jobs=2)\n",
    "\n",
    "# %%\n",
    "# We can now compare the statistical performance of the different strategies.\n",
    "# We plot the score distribution of the different strategies.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "scores = pd.DataFrame(\n",
    "    {\n",
    "        \"DecisionTreeClassifier\": cv_results_tree[\"test_score\"],\n",
    "        \"OneVsOneClassifier\": cv_results_ovo[\"test_score\"],\n",
    "        \"OneVsRestClassifier\": cv_results_ovr[\"test_score\"],\n",
    "        \"OutputCodeClassifier\": cv_results_ecoc[\"test_score\"],\n",
    "    }\n",
    ")\n",
    "ax = scores.plot.kde(legend=True)\n",
    "ax.set_xlabel(\"Accuracy score\")\n",
    "ax.set_xlim([0, 0.7])\n",
    "_ = ax.set_title(\n",
    "    \"Density of the accuracy scores for the different multiclass strategies\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# At a first glance, we can see that the built-in strategy of the decision\n",
    "# tree classifier is working quite well. One-vs-one and the error-correcting\n",
    "# output code strategies are working even better. However, the\n",
    "# one-vs-rest strategy is not working as well as the other strategies.\n",
    "#\n",
    "# Indeed, these results reproduce something reported in the literature\n",
    "# as in [2]_. However, the story is not as simple as it seems.\n",
    "#\n",
    "# The importance of hyperparameters search\n",
    "# ----------------------------------------\n",
    "#\n",
    "# It was later shown in [3]_ that the multiclass strategies would show similar\n",
    "# scores if the hyperparameters of the base classifiers are first optimized.\n",
    "#\n",
    "# Here we try to reproduce such result by at least optimizing the depth of the\n",
    "# base decision tree.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"max_depth\": [3, 5, 8]}\n",
    "tree_optimized = GridSearchCV(tree, param_grid=param_grid, cv=3)\n",
    "ovo_tree = OneVsOneClassifier(tree_optimized)\n",
    "ovr_tree = OneVsRestClassifier(tree_optimized)\n",
    "ecoc = OutputCodeClassifier(tree_optimized, code_size=2)\n",
    "\n",
    "cv_results_tree = cross_validate(tree_optimized, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ovo = cross_validate(ovo_tree, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ovr = cross_validate(ovr_tree, X, y, cv=cv, n_jobs=2)\n",
    "cv_results_ecoc = cross_validate(ecoc, X, y, cv=cv, n_jobs=2)\n",
    "\n",
    "scores = pd.DataFrame(\n",
    "    {\n",
    "        \"DecisionTreeClassifier\": cv_results_tree[\"test_score\"],\n",
    "        \"OneVsOneClassifier\": cv_results_ovo[\"test_score\"],\n",
    "        \"OneVsRestClassifier\": cv_results_ovr[\"test_score\"],\n",
    "        \"OutputCodeClassifier\": cv_results_ecoc[\"test_score\"],\n",
    "    }\n",
    ")\n",
    "ax = scores.plot.kde(legend=True)\n",
    "ax.set_xlabel(\"Accuracy score\")\n",
    "ax.set_xlim([0, 0.7])\n",
    "_ = ax.set_title(\n",
    "    \"Density of the accuracy scores for the different multiclass strategies\"\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# We can see that once the hyperparameters are optimized, all multiclass\n",
    "# strategies have similar performance as discussed in [3]_.\n",
    "#\n",
    "# Conclusion\n",
    "# ----------\n",
    "#\n",
    "# We can get some intuition behind those results.\n",
    "#\n",
    "# First, the reason for which one-vs-one and error-correcting output code are\n",
    "# outperforming the tree when the hyperparameters are not optimized relies on\n",
    "# fact that they ensemble a larger number of classifiers. The ensembling\n",
    "# improves the generalization performance. This is a bit similar why a bagging\n",
    "# classifier generally performs better than a single decision tree if no care\n",
    "# is taken to optimize the hyperparameters.\n",
    "#\n",
    "# Then, we see the importance of optimizing the hyperparameters. Indeed, it\n",
    "# should be regularly explored when developing predictive models even if\n",
    "# techniques such as ensembling help at reducing this impact.\n",
    "#\n",
    "# Finally, it is important to recall that the estimators in scikit-learn\n",
    "# are developed with a specific strategy to handle multiclass classification\n",
    "# out of the box. So for these estimators, it means that there is no need to\n",
    "# use different strategies. These strategies are mainly useful for third-party\n",
    "# estimators supporting only binary classification. In all cases, we also show\n",
    "# that the hyperparameters should be optimized.\n",
    "#\n",
    "# References\n",
    "# ----------\n",
    "#\n",
    "# .. [1] https://archive.ics.uci.edu/ml/datasets/Yeast\n",
    "#\n",
    "# .. [2] `\"Reducing multiclass to binary: A unifying approach for margin classifiers.\"\n",
    "#    Allwein, Erin L., Robert E. Schapire, and Yoram Singer.\n",
    "#    Journal of machine learning research. 1 Dec (2000): 113-141.\n",
    "#    <https://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf>`_\n",
    "#\n",
    "# .. [3] `\"In defense of one-vs-all classification.\"\n",
    "#    Journal of Machine Learning Research. 5 Jan (2004): 101-141.\n",
    "#    <https://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf>`_\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
