{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda760d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=============================================\n",
    "Comparison of kernel ridge regression and SVR\n",
    "=============================================\n",
    "\n",
    "Both kernel ridge regression (KRR) and SVR learn a non-linear function by\n",
    "employing the kernel trick, i.e., they learn a linear function in the space\n",
    "induced by the respective kernel which corresponds to a non-linear function in\n",
    "the original space. They differ in the loss functions (ridge versus\n",
    "epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in\n",
    "closed-form and is typically faster for medium-sized datasets. On the other\n",
    "hand, the learned model is non-sparse and thus slower than SVR at\n",
    "prediction-time.\n",
    "\n",
    "This example illustrates both methods on an artificial dataset, which\n",
    "consists of a sinusoidal target function and strong noise added to every fifth\n",
    "datapoint.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Generate sample data\n",
    "# --------------------\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "X = 5 * rng.rand(10000, 1)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n",
    "\n",
    "X_plot = np.linspace(0, 5, 100000)[:, None]\n",
    "\n",
    "# %%\n",
    "# Construct the kernel-based regression models\n",
    "# --------------------------------------------\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "train_size = 100\n",
    "\n",
    "svr = GridSearchCV(\n",
    "    SVR(kernel=\"rbf\", gamma=0.1),\n",
    "    param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)},\n",
    ")\n",
    "\n",
    "kr = GridSearchCV(\n",
    "    KernelRidge(kernel=\"rbf\", gamma=0.1),\n",
    "    param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3], \"gamma\": np.logspace(-2, 2, 5)},\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Compare times of SVR and Kernel Ridge Regression\n",
    "# ------------------------------------------------\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "svr.fit(X[:train_size], y[:train_size])\n",
    "svr_fit = time.time() - t0\n",
    "print(f\"Best SVR with params: {svr.best_params_} and R2 score: {svr.best_score_:.3f}\")\n",
    "print(\"SVR complexity and bandwidth selected and model fitted in %.3f s\" % svr_fit)\n",
    "\n",
    "t0 = time.time()\n",
    "kr.fit(X[:train_size], y[:train_size])\n",
    "kr_fit = time.time() - t0\n",
    "print(f\"Best KRR with params: {kr.best_params_} and R2 score: {kr.best_score_:.3f}\")\n",
    "print(\"KRR complexity and bandwidth selected and model fitted in %.3f s\" % kr_fit)\n",
    "\n",
    "sv_ratio = svr.best_estimator_.support_.shape[0] / train_size\n",
    "print(\"Support vector ratio: %.3f\" % sv_ratio)\n",
    "\n",
    "t0 = time.time()\n",
    "y_svr = svr.predict(X_plot)\n",
    "svr_predict = time.time() - t0\n",
    "print(\"SVR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], svr_predict))\n",
    "\n",
    "t0 = time.time()\n",
    "y_kr = kr.predict(X_plot)\n",
    "kr_predict = time.time() - t0\n",
    "print(\"KRR prediction for %d inputs in %.3f s\" % (X_plot.shape[0], kr_predict))\n",
    "\n",
    "# %%\n",
    "# Look at the results\n",
    "# -------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sv_ind = svr.best_estimator_.support_\n",
    "plt.scatter(\n",
    "    X[sv_ind],\n",
    "    y[sv_ind],\n",
    "    c=\"r\",\n",
    "    s=50,\n",
    "    label=\"SVR support vectors\",\n",
    "    zorder=2,\n",
    "    edgecolors=(0, 0, 0),\n",
    ")\n",
    "plt.scatter(X[:100], y[:100], c=\"k\", label=\"data\", zorder=1, edgecolors=(0, 0, 0))\n",
    "plt.plot(\n",
    "    X_plot,\n",
    "    y_svr,\n",
    "    c=\"r\",\n",
    "    label=\"SVR (fit: %.3fs, predict: %.3fs)\" % (svr_fit, svr_predict),\n",
    ")\n",
    "plt.plot(\n",
    "    X_plot, y_kr, c=\"g\", label=\"KRR (fit: %.3fs, predict: %.3fs)\" % (kr_fit, kr_predict)\n",
    ")\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"SVR versus Kernel Ridge\")\n",
    "_ = plt.legend()\n",
    "\n",
    "# %%\n",
    "# The previous figure compares the learned model of KRR and SVR when both\n",
    "# complexity/regularization and bandwidth of the RBF kernel are optimized using\n",
    "# grid-search. The learned functions are very similar; however, fitting KRR is\n",
    "# approximately 3-4 times faster than fitting SVR (both with grid-search).\n",
    "#\n",
    "# Prediction of 100000 target values could be in theory approximately three\n",
    "# times faster with SVR since it has learned a sparse model using only\n",
    "# approximately 1/3 of the training datapoints as support vectors. However, in\n",
    "# practice, this is not necessarily the case because of implementation details\n",
    "# in the way the kernel function is computed for each model that can make the\n",
    "# KRR model as fast or even faster despite computing more arithmetic\n",
    "# operations.\n",
    "\n",
    "# %%\n",
    "# Visualize training and prediction times\n",
    "# ---------------------------------------\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "sizes = np.logspace(1, 3.8, 7).astype(int)\n",
    "for name, estimator in {\n",
    "    \"KRR\": KernelRidge(kernel=\"rbf\", alpha=0.01, gamma=10),\n",
    "    \"SVR\": SVR(kernel=\"rbf\", C=1e2, gamma=10),\n",
    "}.items():\n",
    "    train_time = []\n",
    "    test_time = []\n",
    "    for train_test_size in sizes:\n",
    "        t0 = time.time()\n",
    "        estimator.fit(X[:train_test_size], y[:train_test_size])\n",
    "        train_time.append(time.time() - t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        estimator.predict(X_plot[:1000])\n",
    "        test_time.append(time.time() - t0)\n",
    "\n",
    "    plt.plot(\n",
    "        sizes,\n",
    "        train_time,\n",
    "        \"o-\",\n",
    "        color=\"r\" if name == \"SVR\" else \"g\",\n",
    "        label=\"%s (train)\" % name,\n",
    "    )\n",
    "    plt.plot(\n",
    "        sizes,\n",
    "        test_time,\n",
    "        \"o--\",\n",
    "        color=\"r\" if name == \"SVR\" else \"g\",\n",
    "        label=\"%s (test)\" % name,\n",
    "    )\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Train size\")\n",
    "plt.ylabel(\"Time (seconds)\")\n",
    "plt.title(\"Execution Time\")\n",
    "_ = plt.legend(loc=\"best\")\n",
    "\n",
    "# %%\n",
    "# This figure compares the time for fitting and prediction of KRR and SVR for\n",
    "# different sizes of the training set. Fitting KRR is faster than SVR for\n",
    "# medium-sized training sets (less than a few thousand samples); however, for\n",
    "# larger training sets SVR scales better. With regard to prediction time, SVR\n",
    "# should be faster than KRR for all sizes of the training set because of the\n",
    "# learned sparse solution, however this is not necessarily the case in practice\n",
    "# because of implementation details. Note that the degree of sparsity and thus\n",
    "# the prediction time depends on the parameters epsilon and C of the SVR.\n",
    "\n",
    "# %%\n",
    "# Visualize the learning curves\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "svr = SVR(kernel=\"rbf\", C=1e1, gamma=0.1)\n",
    "kr = KernelRidge(kernel=\"rbf\", alpha=0.1, gamma=0.1)\n",
    "\n",
    "common_params = {\n",
    "    \"X\": X[:100],\n",
    "    \"y\": y[:100],\n",
    "    \"train_sizes\": np.linspace(0.1, 1, 10),\n",
    "    \"scoring\": \"neg_mean_squared_error\",\n",
    "    \"negate_score\": True,\n",
    "    \"score_name\": \"Mean Squared Error\",\n",
    "    \"score_type\": \"test\",\n",
    "    \"std_display_style\": None,\n",
    "    \"ax\": ax,\n",
    "}\n",
    "\n",
    "LearningCurveDisplay.from_estimator(svr, **common_params)\n",
    "LearningCurveDisplay.from_estimator(kr, **common_params)\n",
    "ax.set_title(\"Learning curves\")\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
