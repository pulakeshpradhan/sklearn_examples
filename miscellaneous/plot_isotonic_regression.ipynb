{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15213316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================\n",
    "Isotonic Regression\n",
    "===================\n",
    "\n",
    "An illustration of the isotonic regression on generated data (non-linear\n",
    "monotonic trend with homoscedastic uniform noise).\n",
    "\n",
    "The isotonic regression algorithm finds a non-decreasing approximation of a\n",
    "function while minimizing the mean squared error on the training data. The\n",
    "benefit of such a non-parametric model is that it does not assume any shape for\n",
    "the target function besides monotonicity. For comparison a linear regression is\n",
    "also presented.\n",
    "\n",
    "The plot on the right-hand side shows the model prediction function that\n",
    "results from the linear interpolation of thresholds points. The thresholds\n",
    "points are a subset of the training input observations and their matching\n",
    "target values are computed by the isotonic non-parametric fit.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "n = 100\n",
    "x = np.arange(n)\n",
    "rs = check_random_state(0)\n",
    "y = rs.randint(-50, 50, size=(n,)) + 50.0 * np.log1p(np.arange(n))\n",
    "\n",
    "# %%\n",
    "# Fit IsotonicRegression and LinearRegression models:\n",
    "\n",
    "ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "y_ = ir.fit_transform(x, y)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n",
    "\n",
    "# %%\n",
    "# Plot results:\n",
    "\n",
    "segments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\n",
    "lc = LineCollection(segments, zorder=0)\n",
    "lc.set_array(np.ones(len(y)))\n",
    "lc.set_linewidths(np.full(n, 0.5))\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "ax0.plot(x, y, \"C0.\", markersize=12)\n",
    "ax0.plot(x, y_, \"C1.-\", markersize=12)\n",
    "ax0.plot(x, lr.predict(x[:, np.newaxis]), \"C2-\")\n",
    "ax0.add_collection(lc)\n",
    "ax0.legend((\"Training data\", \"Isotonic fit\", \"Linear fit\"), loc=\"lower right\")\n",
    "ax0.set_title(\"Isotonic regression fit on noisy data (n=%d)\" % n)\n",
    "\n",
    "x_test = np.linspace(-10, 110, 1000)\n",
    "ax1.plot(x_test, ir.predict(x_test), \"C1-\")\n",
    "ax1.plot(ir.X_thresholds_, ir.y_thresholds_, \"C1.\", markersize=12)\n",
    "ax1.set_title(\"Prediction function (%d thresholds)\" % len(ir.X_thresholds_))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Note that we explicitly passed `out_of_bounds=\"clip\"` to the constructor of\n",
    "# `IsotonicRegression` to control the way the model extrapolates outside of the\n",
    "# range of data observed in the training set. This \"clipping\" extrapolation can\n",
    "# be seen on the plot of the decision function on the right-hand.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
