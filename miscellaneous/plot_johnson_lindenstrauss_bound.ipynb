{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"\n",
    "=====================================================================\n",
    "The Johnson-Lindenstrauss bound for embedding with random projections\n",
    "=====================================================================\n",
    "\n",
    "\n",
    "The `Johnson-Lindenstrauss lemma`_ states that any high dimensional\n",
    "dataset can be randomly projected into a lower dimensional Euclidean\n",
    "space while controlling the distortion in the pairwise distances.\n",
    "\n",
    ".. _`Johnson-Lindenstrauss lemma`: https://en.wikipedia.org/wiki/\\\n",
    "    Johnson%E2%80%93Lindenstrauss_lemma\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups_vectorized, load_digits\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.random_projection import (\n",
    "    SparseRandomProjection,\n",
    "    johnson_lindenstrauss_min_dim,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Theoretical bounds\n",
    "# ==================\n",
    "# The distortion introduced by a random projection `p` is asserted by\n",
    "# the fact that `p` is defining an eps-embedding with good probability\n",
    "# as defined by:\n",
    "#\n",
    "# .. math::\n",
    "#    (1 - eps) \\|u - v\\|^2 < \\|p(u) - p(v)\\|^2 < (1 + eps) \\|u - v\\|^2\n",
    "#\n",
    "# Where `u` and `v` are any rows taken from a dataset of shape `(n_samples,\n",
    "# n_features)` and `p` is a projection by a random Gaussian `N(0, 1)` matrix\n",
    "# of shape `(n_components, n_features)` (or a sparse Achlioptas matrix).\n",
    "#\n",
    "# The minimum number of components to guarantees the eps-embedding is\n",
    "# given by:\n",
    "#\n",
    "# .. math::\n",
    "#    n\\_components \\geq 4 log(n\\_samples) / (eps^2 / 2 - eps^3 / 3)\n",
    "#\n",
    "#\n",
    "# The first plot shows that with an increasing number of samples ``n_samples``,\n",
    "# the minimal number of dimensions ``n_components`` increased logarithmically\n",
    "# in order to guarantee an ``eps``-embedding.\n",
    "\n",
    "# range of admissible distortions\n",
    "eps_range = np.linspace(0.1, 0.99, 5)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\n",
    "\n",
    "# range of number of samples (observation) to embed\n",
    "n_samples_range = np.logspace(1, 9, 9)\n",
    "\n",
    "plt.figure()\n",
    "for eps, color in zip(eps_range, colors):\n",
    "    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n",
    "    plt.loglog(n_samples_range, min_n_components, color=color)\n",
    "\n",
    "plt.legend([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n",
    "plt.xlabel(\"Number of observations to eps-embed\")\n",
    "plt.ylabel(\"Minimum number of dimensions\")\n",
    "plt.title(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# The second plot shows that an increase of the admissible\n",
    "# distortion ``eps`` allows to reduce drastically the minimal number of\n",
    "# dimensions ``n_components`` for a given number of samples ``n_samples``\n",
    "\n",
    "# range of admissible distortions\n",
    "eps_range = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# range of number of samples (observation) to embed\n",
    "n_samples_range = np.logspace(2, 6, 5)\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\n",
    "\n",
    "plt.figure()\n",
    "for n_samples, color in zip(n_samples_range, colors):\n",
    "    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n",
    "    plt.semilogy(eps_range, min_n_components, color=color)\n",
    "\n",
    "plt.legend([f\"n_samples = {n}\" for n in n_samples_range], loc=\"upper right\")\n",
    "plt.xlabel(\"Distortion eps\")\n",
    "plt.ylabel(\"Minimum number of dimensions\")\n",
    "plt.title(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Empirical validation\n",
    "# ====================\n",
    "#\n",
    "# We validate the above bounds on the 20 newsgroups text document\n",
    "# (TF-IDF word frequencies) dataset or on the digits dataset:\n",
    "#\n",
    "# - for the 20 newsgroups dataset some 300 documents with 100k\n",
    "#   features in total are projected using a sparse random matrix to smaller\n",
    "#   euclidean spaces with various values for the target number of dimensions\n",
    "#   ``n_components``.\n",
    "#\n",
    "# - for the digits dataset, some 8x8 gray level pixels data for 300\n",
    "#   handwritten digits pictures are randomly projected to spaces for various\n",
    "#   larger number of dimensions ``n_components``.\n",
    "#\n",
    "# The default dataset is the 20 newsgroups dataset. To run the example on the\n",
    "# digits dataset, pass the ``--use-digits-dataset`` command line argument to\n",
    "# this script.\n",
    "\n",
    "if \"--use-digits-dataset\" in sys.argv:\n",
    "    data = load_digits().data[:300]\n",
    "else:\n",
    "    data = fetch_20newsgroups_vectorized().data[:300]\n",
    "\n",
    "# %%\n",
    "# For each value of ``n_components``, we plot:\n",
    "#\n",
    "# - 2D distribution of sample pairs with pairwise distances in original\n",
    "#   and projected spaces as x- and y-axis respectively.\n",
    "#\n",
    "# - 1D histogram of the ratio of those distances (projected / original).\n",
    "\n",
    "n_samples, n_features = data.shape\n",
    "print(\n",
    "    f\"Embedding {n_samples} samples with dim {n_features} using various \"\n",
    "    \"random projections\"\n",
    ")\n",
    "\n",
    "n_components_range = np.array([300, 1_000, 10_000])\n",
    "dists = euclidean_distances(data, squared=True).ravel()\n",
    "\n",
    "# select only non-identical samples pairs\n",
    "nonzero = dists != 0\n",
    "dists = dists[nonzero]\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    t0 = time()\n",
    "    rp = SparseRandomProjection(n_components=n_components)\n",
    "    projected_data = rp.fit_transform(data)\n",
    "    print(\n",
    "        f\"Projected {n_samples} samples from {n_features} to {n_components} in \"\n",
    "        f\"{time() - t0:0.3f}s\"\n",
    "    )\n",
    "    if hasattr(rp, \"components_\"):\n",
    "        n_bytes = rp.components_.data.nbytes\n",
    "        n_bytes += rp.components_.indices.nbytes\n",
    "        print(f\"Random matrix with size: {n_bytes / 1e6:0.3f} MB\")\n",
    "\n",
    "    projected_dists = euclidean_distances(projected_data, squared=True).ravel()[nonzero]\n",
    "\n",
    "    plt.figure()\n",
    "    min_dist = min(projected_dists.min(), dists.min())\n",
    "    max_dist = max(projected_dists.max(), dists.max())\n",
    "    plt.hexbin(\n",
    "        dists,\n",
    "        projected_dists,\n",
    "        gridsize=100,\n",
    "        cmap=plt.cm.PuBu,\n",
    "        extent=[min_dist, max_dist, min_dist, max_dist],\n",
    "    )\n",
    "    plt.xlabel(\"Pairwise squared distances in original space\")\n",
    "    plt.ylabel(\"Pairwise squared distances in projected space\")\n",
    "    plt.title(\"Pairwise distances distribution for n_components=%d\" % n_components)\n",
    "    cb = plt.colorbar()\n",
    "    cb.set_label(\"Sample pairs counts\")\n",
    "\n",
    "    rates = projected_dists / dists\n",
    "    print(f\"Mean distances rate: {np.mean(rates):.2f} ({np.std(rates):.2f})\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(rates, bins=50, range=(0.0, 2.0), edgecolor=\"k\", density=True)\n",
    "    plt.xlabel(\"Squared distances rate: projected / original\")\n",
    "    plt.ylabel(\"Distribution of samples pairs\")\n",
    "    plt.title(\"Histogram of pairwise distance rates for n_components=%d\" % n_components)\n",
    "\n",
    "    # TODO: compute the expected value of eps and add them to the previous plot\n",
    "    # as vertical lines / region\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %%\n",
    "# We can see that for low values of ``n_components`` the distribution is wide\n",
    "# with many distorted pairs and a skewed distribution (due to the hard\n",
    "# limit of zero ratio on the left as distances are always positives)\n",
    "# while for larger values of `n_components` the distortion is controlled\n",
    "# and the distances are well preserved by the random projection.\n",
    "#\n",
    "# Remarks\n",
    "# =======\n",
    "#\n",
    "# According to the JL lemma, projecting 300 samples without too much distortion\n",
    "# will require at least several thousands dimensions, irrespective of the\n",
    "# number of features of the original dataset.\n",
    "#\n",
    "# Hence using random projections on the digits dataset which only has 64\n",
    "# features in the input space does not make sense: it does not allow\n",
    "# for dimensionality reduction in this case.\n",
    "#\n",
    "# On the twenty newsgroups on the other hand the dimensionality can be\n",
    "# decreased from 56,436 down to 10,000 while reasonably preserving\n",
    "# pairwise distances.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
