{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35022254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================\n",
    "Metadata Routing\n",
    "================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "This document shows how you can use the :ref:`metadata routing mechanism\n",
    "<metadata_routing>` in scikit-learn to route metadata to the estimators,\n",
    "scorers, and CV splitters consuming them.\n",
    "\n",
    "To better understand the following document, we need to introduce two concepts:\n",
    "routers and consumers. A router is an object which forwards some given data and\n",
    "metadata to other objects. In most cases, a router is a :term:`meta-estimator`,\n",
    "i.e. an estimator which takes another estimator as a parameter. A function such\n",
    "as :func:`sklearn.model_selection.cross_validate` which takes an estimator as a\n",
    "parameter and forwards data and metadata, is also a router.\n",
    "\n",
    "A consumer, on the other hand, is an object which accepts and uses some given\n",
    "metadata. For instance, an estimator taking into account ``sample_weight`` in\n",
    "its :term:`fit` method is a consumer of ``sample_weight``.\n",
    "\n",
    "It is possible for an object to be both a router and a consumer. For instance,\n",
    "a meta-estimator may take into account ``sample_weight`` in certain\n",
    "calculations, but it may also route it to the underlying estimator.\n",
    "\n",
    "First a few imports and some random data for the rest of the script.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    ClassifierMixin,\n",
    "    MetaEstimatorMixin,\n",
    "    RegressorMixin,\n",
    "    TransformerMixin,\n",
    "    clone,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils import metadata_routing\n",
    "from sklearn.utils.metadata_routing import (\n",
    "    MetadataRouter,\n",
    "    MethodMapping,\n",
    "    get_routing_for_object,\n",
    "    process_routing,\n",
    ")\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "n_samples, n_features = 100, 4\n",
    "rng = np.random.RandomState(42)\n",
    "X = rng.rand(n_samples, n_features)\n",
    "y = rng.randint(0, 2, size=n_samples)\n",
    "my_groups = rng.randint(0, 10, size=n_samples)\n",
    "my_weights = rng.rand(n_samples)\n",
    "my_other_weights = rng.rand(n_samples)\n",
    "\n",
    "# %%\n",
    "# Metadata routing is only available if explicitly enabled:\n",
    "set_config(enable_metadata_routing=True)\n",
    "\n",
    "\n",
    "# %%\n",
    "# This utility function is a dummy to check if a metadata is passed:\n",
    "def check_metadata(obj, **kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        if value is not None:\n",
    "            print(\n",
    "                f\"Received {key} of length = {len(value)} in {obj.__class__.__name__}.\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{key} is None in {obj.__class__.__name__}.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# A utility function to nicely print the routing information of an object:\n",
    "def print_routing(obj):\n",
    "    pprint(obj.get_metadata_routing()._serialize())\n",
    "\n",
    "\n",
    "# %%\n",
    "# Consuming Estimator\n",
    "# -------------------\n",
    "# Here we demonstrate how an estimator can expose the required API to support\n",
    "# metadata routing as a consumer. Imagine a simple classifier accepting\n",
    "# ``sample_weight`` as a metadata on its ``fit`` and ``groups`` in its\n",
    "# ``predict`` method:\n",
    "\n",
    "\n",
    "class ExampleClassifier(ClassifierMixin, BaseEstimator):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        check_metadata(self, sample_weight=sample_weight)\n",
    "        # all classifiers need to expose a classes_ attribute once they're fit.\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, groups=None):\n",
    "        check_metadata(self, groups=groups)\n",
    "        # return a constant value of 1, not a very smart classifier!\n",
    "        return np.ones(len(X))\n",
    "\n",
    "\n",
    "# %%\n",
    "# The above estimator now has all it needs to consume metadata. This is\n",
    "# accomplished by some magic done in :class:`~base.BaseEstimator`. There are\n",
    "# now three methods exposed by the above class: ``set_fit_request``,\n",
    "# ``set_predict_request``, and ``get_metadata_routing``. There is also a\n",
    "# ``set_score_request`` for ``sample_weight`` which is present since\n",
    "# :class:`~base.ClassifierMixin` implements a ``score`` method accepting\n",
    "# ``sample_weight``. The same applies to regressors which inherit from\n",
    "# :class:`~base.RegressorMixin`.\n",
    "#\n",
    "# By default, no metadata is requested, which we can see as:\n",
    "\n",
    "print_routing(ExampleClassifier())\n",
    "\n",
    "# %%\n",
    "# The above output means that ``sample_weight`` and ``groups`` are not\n",
    "# requested by `ExampleClassifier`, and if a router is given those metadata, it\n",
    "# should raise an error, since the user has not explicitly set whether they are\n",
    "# required or not. The same is true for ``sample_weight`` in the ``score``\n",
    "# method, which is inherited from :class:`~base.ClassifierMixin`. In order to\n",
    "# explicitly set request values for those metadata, we can use these methods:\n",
    "\n",
    "est = (\n",
    "    ExampleClassifier()\n",
    "    .set_fit_request(sample_weight=False)\n",
    "    .set_predict_request(groups=True)\n",
    "    .set_score_request(sample_weight=False)\n",
    ")\n",
    "print_routing(est)\n",
    "\n",
    "# %%\n",
    "# .. note ::\n",
    "#     Please note that as long as the above estimator is not used in a\n",
    "#     meta-estimator, the user does not need to set any requests for the\n",
    "#     metadata and the set values are ignored, since a consumer does not\n",
    "#     validate or route given metadata. A simple usage of the above estimator\n",
    "#     would work as expected.\n",
    "\n",
    "est = ExampleClassifier()\n",
    "est.fit(X, y, sample_weight=my_weights)\n",
    "est.predict(X[:3, :], groups=my_groups)\n",
    "\n",
    "# %%\n",
    "# Routing Meta-Estimator\n",
    "# ----------------------\n",
    "# Now, we show how to design a meta-estimator to be a router. As a simplified\n",
    "# example, here is a meta-estimator, which doesn't do much other than routing\n",
    "# the metadata.\n",
    "\n",
    "\n",
    "class MetaClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        # This method defines the routing for this meta-estimator.\n",
    "        # In order to do so, a `MetadataRouter` instance is created, and the\n",
    "        # routing is added to it. More explanations follow below.\n",
    "        router = MetadataRouter(owner=self.__class__.__name__).add(\n",
    "            estimator=self.estimator,\n",
    "            method_mapping=MethodMapping()\n",
    "            .add(caller=\"fit\", callee=\"fit\")\n",
    "            .add(caller=\"predict\", callee=\"predict\")\n",
    "            .add(caller=\"score\", callee=\"score\"),\n",
    "        )\n",
    "        return router\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        # `get_routing_for_object` returns a copy of the `MetadataRouter`\n",
    "        # constructed by the above `get_metadata_routing` method, that is\n",
    "        # internally called.\n",
    "        request_router = get_routing_for_object(self)\n",
    "        # Meta-estimators are responsible for validating the given metadata.\n",
    "        # `method` refers to the parent's method, i.e. `fit` in this example.\n",
    "        request_router.validate_metadata(params=fit_params, method=\"fit\")\n",
    "        # `MetadataRouter.route_params` maps the given metadata to the metadata\n",
    "        # required by the underlying estimator based on the routing information\n",
    "        # defined by the MetadataRouter. The output of type `Bunch` has a key\n",
    "        # for each consuming object and those hold keys for their consuming\n",
    "        # methods, which then contain key for the metadata which should be\n",
    "        # routed to them.\n",
    "        routed_params = request_router.route_params(params=fit_params, caller=\"fit\")\n",
    "\n",
    "        # A sub-estimator is fitted and its classes are attributed to the\n",
    "        # meta-estimator.\n",
    "        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\n",
    "        self.classes_ = self.estimator_.classes_\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, **predict_params):\n",
    "        check_is_fitted(self)\n",
    "        # As in `fit`, we get a copy of the object's MetadataRouter,\n",
    "        request_router = get_routing_for_object(self)\n",
    "        # then we validate the given metadata,\n",
    "        request_router.validate_metadata(params=predict_params, method=\"predict\")\n",
    "        # and then prepare the input to the underlying `predict` method.\n",
    "        routed_params = request_router.route_params(\n",
    "            params=predict_params, caller=\"predict\"\n",
    "        )\n",
    "        return self.estimator_.predict(X, **routed_params.estimator.predict)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Let's break down different parts of the above code.\n",
    "#\n",
    "# First, the :meth:`~utils.metadata_routing.get_routing_for_object` takes our\n",
    "# meta-estimator (``self``) and returns a\n",
    "# :class:`~utils.metadata_routing.MetadataRouter` or, a\n",
    "# :class:`~utils.metadata_routing.MetadataRequest` if the object is a consumer,\n",
    "# based on the output of the estimator's ``get_metadata_routing`` method.\n",
    "#\n",
    "# Then in each method, we use the ``route_params`` method to construct a\n",
    "# dictionary of the form ``{\"object_name\": {\"method_name\": {\"metadata\":\n",
    "# value}}}`` to pass to the underlying estimator's method. The ``object_name``\n",
    "# (``estimator`` in the above ``routed_params.estimator.fit`` example) is the\n",
    "# same as the one added in the ``get_metadata_routing``. ``validate_metadata``\n",
    "# makes sure all given metadata are requested to avoid silent bugs.\n",
    "#\n",
    "# Next, we illustrate the different behaviors and notably the type of errors\n",
    "# raised.\n",
    "\n",
    "meta_est = MetaClassifier(\n",
    "    estimator=ExampleClassifier().set_fit_request(sample_weight=True)\n",
    ")\n",
    "meta_est.fit(X, y, sample_weight=my_weights)\n",
    "\n",
    "# %%\n",
    "# Note that the above example is calling our utility function\n",
    "# `check_metadata()` via the `ExampleClassifier`. It checks that\n",
    "# ``sample_weight`` is correctly passed to it. If it is not, like in the\n",
    "# following example, it would print that ``sample_weight`` is ``None``:\n",
    "\n",
    "meta_est.fit(X, y)\n",
    "\n",
    "# %%\n",
    "# If we pass an unknown metadata, an error is raised:\n",
    "try:\n",
    "    meta_est.fit(X, y, test=my_weights)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "\n",
    "# %%\n",
    "# And if we pass a metadata which is not explicitly requested:\n",
    "try:\n",
    "    meta_est.fit(X, y, sample_weight=my_weights).predict(X, groups=my_groups)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# %%\n",
    "# Also, if we explicitly set it as not requested, but it is provided:\n",
    "meta_est = MetaClassifier(\n",
    "    estimator=ExampleClassifier()\n",
    "    .set_fit_request(sample_weight=True)\n",
    "    .set_predict_request(groups=False)\n",
    ")\n",
    "try:\n",
    "    meta_est.fit(X, y, sample_weight=my_weights).predict(X[:3, :], groups=my_groups)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "\n",
    "# %%\n",
    "# Another concept to introduce is **aliased metadata**. This is when an\n",
    "# estimator requests a metadata with a different variable name than the default\n",
    "# variable name. For instance, in a setting where there are two estimators in a\n",
    "# pipeline, one could request ``sample_weight1`` and the other\n",
    "# ``sample_weight2``. Note that this doesn't change what the estimator expects,\n",
    "# it only tells the meta-estimator how to map the provided metadata to what is\n",
    "# required. Here's an example, where we pass ``aliased_sample_weight`` to the\n",
    "# meta-estimator, but the meta-estimator understands that\n",
    "# ``aliased_sample_weight`` is an alias for ``sample_weight``, and passes it as\n",
    "# ``sample_weight`` to the underlying estimator:\n",
    "meta_est = MetaClassifier(\n",
    "    estimator=ExampleClassifier().set_fit_request(sample_weight=\"aliased_sample_weight\")\n",
    ")\n",
    "meta_est.fit(X, y, aliased_sample_weight=my_weights)\n",
    "\n",
    "# %%\n",
    "# Passing ``sample_weight`` here will fail since it is requested with an\n",
    "# alias and ``sample_weight`` with that name is not requested:\n",
    "try:\n",
    "    meta_est.fit(X, y, sample_weight=my_weights)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "\n",
    "# %%\n",
    "# This leads us to the ``get_metadata_routing``. The way routing works in\n",
    "# scikit-learn is that consumers request what they need, and routers pass that\n",
    "# along. Additionally, a router exposes what it requires itself so that it can\n",
    "# be used inside another router, e.g. a pipeline inside a grid search object.\n",
    "# The output of the ``get_metadata_routing`` which is a dictionary\n",
    "# representation of a :class:`~utils.metadata_routing.MetadataRouter`, includes\n",
    "# the complete tree of requested metadata by all nested objects and their\n",
    "# corresponding method routings, i.e. which method of a sub-estimator is used\n",
    "# in which method of a meta-estimator:\n",
    "\n",
    "print_routing(meta_est)\n",
    "\n",
    "# %%\n",
    "# As you can see, the only metadata requested for method ``fit`` is\n",
    "# ``\"sample_weight\"`` with ``\"aliased_sample_weight\"`` as the alias. The\n",
    "# ``~utils.metadata_routing.MetadataRouter`` class enables us to easily create\n",
    "# the routing object which would create the output we need for our\n",
    "# ``get_metadata_routing``.\n",
    "#\n",
    "# In order to understand how aliases work in meta-estimators, imagine our\n",
    "# meta-estimator inside another one:\n",
    "\n",
    "meta_meta_est = MetaClassifier(estimator=meta_est).fit(\n",
    "    X, y, aliased_sample_weight=my_weights\n",
    ")\n",
    "\n",
    "# %%\n",
    "# In the above example, this is how the ``fit`` method of `meta_meta_est`\n",
    "# will call their sub-estimator's ``fit`` methods::\n",
    "#\n",
    "#     # user feeds `my_weights` as `aliased_sample_weight` into `meta_meta_est`:\n",
    "#     meta_meta_est.fit(X, y, aliased_sample_weight=my_weights):\n",
    "#         ...\n",
    "#\n",
    "#         # the first sub-estimator (`meta_est`) expects `aliased_sample_weight`\n",
    "#         self.estimator_.fit(X, y, aliased_sample_weight=aliased_sample_weight):\n",
    "#             ...\n",
    "#\n",
    "#             # the second sub-estimator (`est`) expects `sample_weight`\n",
    "#             self.estimator_.fit(X, y, sample_weight=aliased_sample_weight):\n",
    "#                 ...\n",
    "\n",
    "# %%\n",
    "# Consuming and routing Meta-Estimator\n",
    "# ------------------------------------\n",
    "# For a slightly more complex example, consider a meta-estimator that routes\n",
    "# metadata to an underlying estimator as before, but it also uses some metadata\n",
    "# in its own methods. This meta-estimator is a consumer and a router at the\n",
    "# same time. Implementing one is very similar to what we had before, but with a\n",
    "# few tweaks.\n",
    "\n",
    "\n",
    "class RouterConsumerClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        router = (\n",
    "            MetadataRouter(owner=self.__class__.__name__)\n",
    "            # defining metadata routing request values for usage in the meta-estimator\n",
    "            .add_self_request(self)\n",
    "            # defining metadata routing request values for usage in the sub-estimator\n",
    "            .add(\n",
    "                estimator=self.estimator,\n",
    "                method_mapping=MethodMapping()\n",
    "                .add(caller=\"fit\", callee=\"fit\")\n",
    "                .add(caller=\"predict\", callee=\"predict\")\n",
    "                .add(caller=\"score\", callee=\"score\"),\n",
    "            )\n",
    "        )\n",
    "        return router\n",
    "\n",
    "    # Since `sample_weight` is used and consumed here, it should be defined as\n",
    "    # an explicit argument in the method's signature. All other metadata which\n",
    "    # are only routed, will be passed as `**fit_params`:\n",
    "    def fit(self, X, y, sample_weight, **fit_params):\n",
    "        if self.estimator is None:\n",
    "            raise ValueError(\"estimator cannot be None!\")\n",
    "\n",
    "        check_metadata(self, sample_weight=sample_weight)\n",
    "\n",
    "        # We add `sample_weight` to the `fit_params` dictionary.\n",
    "        if sample_weight is not None:\n",
    "            fit_params[\"sample_weight\"] = sample_weight\n",
    "\n",
    "        request_router = get_routing_for_object(self)\n",
    "        request_router.validate_metadata(params=fit_params, method=\"fit\")\n",
    "        routed_params = request_router.route_params(params=fit_params, caller=\"fit\")\n",
    "        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\n",
    "        self.classes_ = self.estimator_.classes_\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, **predict_params):\n",
    "        check_is_fitted(self)\n",
    "        # As in `fit`, we get a copy of the object's MetadataRouter,\n",
    "        request_router = get_routing_for_object(self)\n",
    "        # we validate the given metadata,\n",
    "        request_router.validate_metadata(params=predict_params, method=\"predict\")\n",
    "        # and then prepare the input to the underlying ``predict`` method.\n",
    "        routed_params = request_router.route_params(\n",
    "            params=predict_params, caller=\"predict\"\n",
    "        )\n",
    "        return self.estimator_.predict(X, **routed_params.estimator.predict)\n",
    "\n",
    "\n",
    "# %%\n",
    "# The key parts where the above meta-estimator differs from our previous\n",
    "# meta-estimator is accepting ``sample_weight`` explicitly in ``fit`` and\n",
    "# including it in ``fit_params``. Since ``sample_weight`` is an explicit\n",
    "# argument, we can be sure that ``set_fit_request(sample_weight=...)`` is\n",
    "# present for this method. The meta-estimator is both a consumer, as well as a\n",
    "# router of ``sample_weight``.\n",
    "#\n",
    "# In ``get_metadata_routing``, we add ``self`` to the routing using\n",
    "# ``add_self_request`` to indicate this estimator is consuming\n",
    "# ``sample_weight`` as well as being a router; which also adds a\n",
    "# ``$self_request`` key to the routing info as illustrated below. Now let's\n",
    "# look at some examples:\n",
    "\n",
    "# %%\n",
    "# - No metadata requested\n",
    "meta_est = RouterConsumerClassifier(estimator=ExampleClassifier())\n",
    "print_routing(meta_est)\n",
    "\n",
    "\n",
    "# %%\n",
    "# - ``sample_weight`` requested by sub-estimator\n",
    "meta_est = RouterConsumerClassifier(\n",
    "    estimator=ExampleClassifier().set_fit_request(sample_weight=True)\n",
    ")\n",
    "print_routing(meta_est)\n",
    "\n",
    "# %%\n",
    "# - ``sample_weight`` requested by meta-estimator\n",
    "meta_est = RouterConsumerClassifier(estimator=ExampleClassifier()).set_fit_request(\n",
    "    sample_weight=True\n",
    ")\n",
    "print_routing(meta_est)\n",
    "\n",
    "# %%\n",
    "# Note the difference in the requested metadata representations above.\n",
    "#\n",
    "# - We can also alias the metadata to pass different values to the fit methods\n",
    "#   of the meta- and the sub-estimator:\n",
    "\n",
    "meta_est = RouterConsumerClassifier(\n",
    "    estimator=ExampleClassifier().set_fit_request(sample_weight=\"clf_sample_weight\"),\n",
    ").set_fit_request(sample_weight=\"meta_clf_sample_weight\")\n",
    "print_routing(meta_est)\n",
    "\n",
    "# %%\n",
    "# However, ``fit`` of the meta-estimator only needs the alias for the\n",
    "# sub-estimator and addresses their own sample weight as `sample_weight`, since\n",
    "# it doesn't validate and route its own required metadata:\n",
    "meta_est.fit(X, y, sample_weight=my_weights, clf_sample_weight=my_other_weights)\n",
    "\n",
    "# %%\n",
    "# - Alias only on the sub-estimator:\n",
    "#\n",
    "# This is useful when we don't want the meta-estimator to use the metadata, but\n",
    "# the sub-estimator should.\n",
    "meta_est = RouterConsumerClassifier(\n",
    "    estimator=ExampleClassifier().set_fit_request(sample_weight=\"aliased_sample_weight\")\n",
    ")\n",
    "print_routing(meta_est)\n",
    "# %%\n",
    "# The meta-estimator cannot use `aliased_sample_weight`, because it expects\n",
    "# it passed as `sample_weight`. This would apply even if\n",
    "# `set_fit_request(sample_weight=True)` was set on it.\n",
    "\n",
    "# %%\n",
    "# Simple Pipeline\n",
    "# ---------------\n",
    "# A slightly more complicated use-case is a meta-estimator resembling a\n",
    "# :class:`~pipeline.Pipeline`. Here is a meta-estimator, which accepts a\n",
    "# transformer and a classifier. When calling its `fit` method, it applies the\n",
    "# transformer's `fit` and `transform` before running the classifier on the\n",
    "# transformed data. Upon `predict`, it applies the transformer's `transform`\n",
    "# before predicting with the classifier's `predict` method on the transformed\n",
    "# new data.\n",
    "\n",
    "\n",
    "class SimplePipeline(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, transformer, classifier):\n",
    "        self.transformer = transformer\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        router = (\n",
    "            MetadataRouter(owner=self.__class__.__name__)\n",
    "            # We add the routing for the transformer.\n",
    "            .add(\n",
    "                transformer=self.transformer,\n",
    "                method_mapping=MethodMapping()\n",
    "                # The metadata is routed such that it retraces how\n",
    "                # `SimplePipeline` internally calls the transformer's `fit` and\n",
    "                # `transform` methods in its own methods (`fit` and `predict`).\n",
    "                .add(caller=\"fit\", callee=\"fit\")\n",
    "                .add(caller=\"fit\", callee=\"transform\")\n",
    "                .add(caller=\"predict\", callee=\"transform\"),\n",
    "            )\n",
    "            # We add the routing for the classifier.\n",
    "            .add(\n",
    "                classifier=self.classifier,\n",
    "                method_mapping=MethodMapping()\n",
    "                .add(caller=\"fit\", callee=\"fit\")\n",
    "                .add(caller=\"predict\", callee=\"predict\"),\n",
    "            )\n",
    "        )\n",
    "        return router\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        routed_params = process_routing(self, \"fit\", **fit_params)\n",
    "\n",
    "        self.transformer_ = clone(self.transformer).fit(\n",
    "            X, y, **routed_params.transformer.fit\n",
    "        )\n",
    "        X_transformed = self.transformer_.transform(\n",
    "            X, **routed_params.transformer.transform\n",
    "        )\n",
    "\n",
    "        self.classifier_ = clone(self.classifier).fit(\n",
    "            X_transformed, y, **routed_params.classifier.fit\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, **predict_params):\n",
    "        routed_params = process_routing(self, \"predict\", **predict_params)\n",
    "\n",
    "        X_transformed = self.transformer_.transform(\n",
    "            X, **routed_params.transformer.transform\n",
    "        )\n",
    "        return self.classifier_.predict(\n",
    "            X_transformed, **routed_params.classifier.predict\n",
    "        )\n",
    "\n",
    "\n",
    "# %%\n",
    "# Note the usage of :class:`~utils.metadata_routing.MethodMapping` to\n",
    "# declare which methods of the child estimator (callee) are used in which\n",
    "# methods of the meta estimator (caller). As you can see, `SimplePipeline` uses\n",
    "# the transformer's ``transform`` and ``fit`` methods in ``fit``, and its\n",
    "# ``transform`` method in ``predict``, and that's what you see implemented in\n",
    "# the routing structure of the pipeline class.\n",
    "#\n",
    "# Another difference in the above example with the previous ones is the usage\n",
    "# of :func:`~utils.metadata_routing.process_routing`, which processes the input\n",
    "# parameters, does the required validation, and returns the `routed_params`\n",
    "# which we had created in previous examples. This reduces the boilerplate code\n",
    "# a developer needs to write in each meta-estimator's method. Developers are\n",
    "# strongly recommended to use this function unless there is a good reason\n",
    "# against it.\n",
    "#\n",
    "# In order to test the above pipeline, let's add an example transformer.\n",
    "\n",
    "\n",
    "class ExampleTransformer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        check_metadata(self, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, groups=None):\n",
    "        check_metadata(self, groups=groups)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y, sample_weight=None, groups=None):\n",
    "        return self.fit(X, y, sample_weight).transform(X, groups)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Note that in the above example, we have implemented ``fit_transform`` which\n",
    "# calls ``fit`` and ``transform`` with the appropriate metadata. This is only\n",
    "# required if ``transform`` accepts metadata, since the default ``fit_transform``\n",
    "# implementation in :class:`~base.TransformerMixin` doesn't pass metadata to\n",
    "# ``transform``.\n",
    "#\n",
    "# Now we can test our pipeline, and see if metadata is correctly passed around.\n",
    "# This example uses our `SimplePipeline`, our `ExampleTransformer`, and our\n",
    "# `RouterConsumerClassifier` which uses our `ExampleClassifier`.\n",
    "\n",
    "pipe = SimplePipeline(\n",
    "    transformer=ExampleTransformer()\n",
    "    # we set transformer's fit to receive sample_weight\n",
    "    .set_fit_request(sample_weight=True)\n",
    "    # we set transformer's transform to receive groups\n",
    "    .set_transform_request(groups=True),\n",
    "    classifier=RouterConsumerClassifier(\n",
    "        estimator=ExampleClassifier()\n",
    "        # we want this sub-estimator to receive sample_weight in fit\n",
    "        .set_fit_request(sample_weight=True)\n",
    "        # but not groups in predict\n",
    "        .set_predict_request(groups=False),\n",
    "    )\n",
    "    # and we want the meta-estimator to receive sample_weight as well\n",
    "    .set_fit_request(sample_weight=True),\n",
    ")\n",
    "pipe.fit(X, y, sample_weight=my_weights, groups=my_groups).predict(\n",
    "    X[:3], groups=my_groups\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Deprecation / Default Value Change\n",
    "# ----------------------------------\n",
    "# In this section we show how one should handle the case where a router becomes\n",
    "# also a consumer, especially when it consumes the same metadata as its\n",
    "# sub-estimator, or a consumer starts consuming a metadata which it wasn't in\n",
    "# an older release. In this case, a warning should be raised for a while, to\n",
    "# let users know the behavior is changed from previous versions.\n",
    "\n",
    "\n",
    "class MetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        routed_params = process_routing(self, \"fit\", **fit_params)\n",
    "        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        router = MetadataRouter(owner=self.__class__.__name__).add(\n",
    "            estimator=self.estimator,\n",
    "            method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\n",
    "        )\n",
    "        return router\n",
    "\n",
    "\n",
    "# %%\n",
    "# As explained above, this is a valid usage if `my_weights` aren't supposed\n",
    "# to be passed as `sample_weight` to `MetaRegressor`:\n",
    "\n",
    "reg = MetaRegressor(estimator=LinearRegression().set_fit_request(sample_weight=True))\n",
    "reg.fit(X, y, sample_weight=my_weights)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Now imagine we further develop ``MetaRegressor`` and it now also *consumes*\n",
    "# ``sample_weight``:\n",
    "\n",
    "\n",
    "class WeightedMetaRegressor(MetaEstimatorMixin, RegressorMixin, BaseEstimator):\n",
    "    # show warning to remind user to explicitly set the value with\n",
    "    # `.set_{method}_request(sample_weight={boolean})`\n",
    "    __metadata_request__fit = {\"sample_weight\": metadata_routing.WARN}\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        routed_params = process_routing(\n",
    "            self, \"fit\", sample_weight=sample_weight, **fit_params\n",
    "        )\n",
    "        check_metadata(self, sample_weight=sample_weight)\n",
    "        self.estimator_ = clone(self.estimator).fit(X, y, **routed_params.estimator.fit)\n",
    "\n",
    "    def get_metadata_routing(self):\n",
    "        router = (\n",
    "            MetadataRouter(owner=self.__class__.__name__)\n",
    "            .add_self_request(self)\n",
    "            .add(\n",
    "                estimator=self.estimator,\n",
    "                method_mapping=MethodMapping().add(caller=\"fit\", callee=\"fit\"),\n",
    "            )\n",
    "        )\n",
    "        return router\n",
    "\n",
    "\n",
    "# %%\n",
    "# The above implementation is almost the same as ``MetaRegressor``, and\n",
    "# because of the default request value defined in ``__metadata_request__fit``\n",
    "# there is a warning raised when fitted.\n",
    "\n",
    "with warnings.catch_warnings(record=True) as record:\n",
    "    WeightedMetaRegressor(\n",
    "        estimator=LinearRegression().set_fit_request(sample_weight=False)\n",
    "    ).fit(X, y, sample_weight=my_weights)\n",
    "for w in record:\n",
    "    print(w.message)\n",
    "\n",
    "\n",
    "# %%\n",
    "# When an estimator consumes a metadata which it didn't consume before, the\n",
    "# following pattern can be used to warn the users about it.\n",
    "\n",
    "\n",
    "class ExampleRegressor(RegressorMixin, BaseEstimator):\n",
    "    __metadata_request__fit = {\"sample_weight\": metadata_routing.WARN}\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        check_metadata(self, sample_weight=sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.zeros(shape=(len(X)))\n",
    "\n",
    "\n",
    "with warnings.catch_warnings(record=True) as record:\n",
    "    MetaRegressor(estimator=ExampleRegressor()).fit(X, y, sample_weight=my_weights)\n",
    "for w in record:\n",
    "    print(w.message)\n",
    "\n",
    "# %%\n",
    "# At the end we disable the configuration flag for metadata routing:\n",
    "\n",
    "set_config(enable_metadata_routing=False)\n",
    "\n",
    "# %%\n",
    "# Third Party Development and scikit-learn Dependency\n",
    "# ---------------------------------------------------\n",
    "#\n",
    "# As seen above, information is communicated between classes using\n",
    "# :class:`~utils.metadata_routing.MetadataRequest` and\n",
    "# :class:`~utils.metadata_routing.MetadataRouter`. It is strongly not advised,\n",
    "# but possible to vendor the tools related to metadata-routing if you strictly\n",
    "# want to have a scikit-learn compatible estimator, without depending on the\n",
    "# scikit-learn package. If all of the following conditions are met, you do NOT\n",
    "# need to modify your code at all:\n",
    "#\n",
    "# - your estimator inherits from :class:`~base.BaseEstimator`\n",
    "# - the parameters consumed by your estimator's methods, e.g. ``fit``, are\n",
    "#   explicitly defined in the method's signature, as opposed to being\n",
    "#   ``*args`` or ``*kwargs``.\n",
    "# - your estimator does not route any metadata to the underlying objects, i.e.\n",
    "#   it's not a *router*.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
