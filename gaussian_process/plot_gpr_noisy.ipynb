{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9400e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================================\n",
    "Ability of Gaussian process regression (GPR) to estimate data noise-level\n",
    "=========================================================================\n",
    "\n",
    "This example shows the ability of the\n",
    ":class:`~sklearn.gaussian_process.kernels.WhiteKernel` to estimate the noise\n",
    "level in the data. Moreover, we show the importance of kernel hyperparameters\n",
    "initialization.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Data generation\n",
    "# ---------------\n",
    "#\n",
    "# We will work in a setting where `X` will contain a single feature. We create a\n",
    "# function that will generate the target to be predicted. We will add an\n",
    "# option to add some noise to the generated target.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def target_generator(X, add_noise=False):\n",
    "    target = 0.5 + np.sin(3 * X)\n",
    "    if add_noise:\n",
    "        rng = np.random.RandomState(1)\n",
    "        target += rng.normal(0, 0.3, size=target.shape)\n",
    "    return target.squeeze()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Let's have a look to the target generator where we will not add any noise to\n",
    "# observe the signal that we would like to predict.\n",
    "X = np.linspace(0, 5, num=80).reshape(-1, 1)\n",
    "y = target_generator(X, add_noise=False)\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, y, label=\"Expected signal\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "_ = plt.ylabel(\"y\")\n",
    "\n",
    "# %%\n",
    "# The target is transforming the input `X` using a sine function. Now, we will\n",
    "# generate few noisy training samples. To illustrate the noise level, we will\n",
    "# plot the true signal together with the noisy training samples.\n",
    "rng = np.random.RandomState(0)\n",
    "X_train = rng.uniform(0, 5, size=20).reshape(-1, 1)\n",
    "y_train = target_generator(X_train, add_noise=True)\n",
    "\n",
    "# %%\n",
    "plt.plot(X, y, label=\"Expected signal\")\n",
    "plt.scatter(\n",
    "    x=X_train[:, 0],\n",
    "    y=y_train,\n",
    "    color=\"black\",\n",
    "    alpha=0.4,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "_ = plt.ylabel(\"y\")\n",
    "\n",
    "# %%\n",
    "# Optimisation of kernel hyperparameters in GPR\n",
    "# ---------------------------------------------\n",
    "#\n",
    "# Now, we will create a\n",
    "# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`\n",
    "# using an additive kernel adding a\n",
    "# :class:`~sklearn.gaussian_process.kernels.RBF` and\n",
    "# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` kernels.\n",
    "# The :class:`~sklearn.gaussian_process.kernels.WhiteKernel` is a kernel that\n",
    "# will able to estimate the amount of noise present in the data while the\n",
    "# :class:`~sklearn.gaussian_process.kernels.RBF` will serve at fitting the\n",
    "# non-linearity between the data and the target.\n",
    "#\n",
    "# However, we will show that the hyperparameter space contains several local\n",
    "# minima. It will highlights the importance of initial hyperparameter values.\n",
    "#\n",
    "# We will create a model using a kernel with a high noise level and a large\n",
    "# length scale, which will explain all variations in the data by noise.\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\n",
    "    noise_level=1, noise_level_bounds=(1e-10, 1e1)\n",
    ")\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
    "gpr.fit(X_train, y_train)\n",
    "y_mean, y_std = gpr.predict(X, return_std=True)\n",
    "\n",
    "# %%\n",
    "plt.plot(X, y, label=\"Expected signal\")\n",
    "plt.scatter(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\n",
    "plt.errorbar(X, y_mean, y_std, label=\"Posterior mean ± std\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\n",
    "    (\n",
    "        f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n",
    "        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\"\n",
    "    ),\n",
    "    fontsize=8,\n",
    ")\n",
    "# %%\n",
    "# We see that the optimum kernel found still has a high noise level and an even\n",
    "# larger length scale. The length scale reaches the maximum bound that we\n",
    "# allowed for this parameter and we got a warning as a result.\n",
    "#\n",
    "# More importantly, we observe that the model does not provide useful\n",
    "# predictions: the mean prediction seems to be constant: it does not follow the\n",
    "# expected noise-free signal.\n",
    "#\n",
    "# Now, we will initialize the :class:`~sklearn.gaussian_process.kernels.RBF`\n",
    "# with a larger `length_scale` initial value and the\n",
    "# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` with a smaller initial\n",
    "# noise level lower while keeping the parameter bounds unchanged.\n",
    "kernel = 1.0 * RBF(length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\n",
    "    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)\n",
    ")\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
    "gpr.fit(X_train, y_train)\n",
    "y_mean, y_std = gpr.predict(X, return_std=True)\n",
    "\n",
    "# %%\n",
    "plt.plot(X, y, label=\"Expected signal\")\n",
    "plt.scatter(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\n",
    "plt.errorbar(X, y_mean, y_std, label=\"Posterior mean ± std\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\n",
    "    (\n",
    "        f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n",
    "        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\"\n",
    "    ),\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# First, we see that the model's predictions are more precise than the\n",
    "# previous model's: this new model is able to estimate the noise-free\n",
    "# functional relationship.\n",
    "#\n",
    "# Looking at the kernel hyperparameters, we see that the best combination found\n",
    "# has a smaller noise level and shorter length scale than the first model.\n",
    "#\n",
    "# We can inspect the Log-Marginal-Likelihood (LML) of\n",
    "# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`\n",
    "# for different hyperparameters to get a sense of the local minima.\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "length_scale = np.logspace(-2, 4, num=80)\n",
    "noise_level = np.logspace(-2, 1, num=80)\n",
    "length_scale_grid, noise_level_grid = np.meshgrid(length_scale, noise_level)\n",
    "\n",
    "log_marginal_likelihood = [\n",
    "    gpr.log_marginal_likelihood(theta=np.log([0.36, scale, noise]))\n",
    "    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())\n",
    "]\n",
    "log_marginal_likelihood = np.reshape(log_marginal_likelihood, noise_level_grid.shape)\n",
    "\n",
    "# %%\n",
    "vmin, vmax = (-log_marginal_likelihood).min(), 50\n",
    "level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), num=20), decimals=1)\n",
    "plt.contour(\n",
    "    length_scale_grid,\n",
    "    noise_level_grid,\n",
    "    -log_marginal_likelihood,\n",
    "    levels=level,\n",
    "    norm=LogNorm(vmin=vmin, vmax=vmax),\n",
    ")\n",
    "plt.colorbar()\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Length-scale\")\n",
    "plt.ylabel(\"Noise-level\")\n",
    "plt.title(\"Log-marginal-likelihood\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "#\n",
    "# We see that there are two local minima that correspond to the combination of\n",
    "# hyperparameters previously found. Depending on the initial values for the\n",
    "# hyperparameters, the gradient-based optimization might or might not\n",
    "# converge to the best model. It is thus important to repeat the optimization\n",
    "# several times for different initializations. This can be done by setting the\n",
    "# `n_restarts_optimizer` parameter of the\n",
    "# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` class.\n",
    "#\n",
    "# Let's try again to fit our model with the bad initial values but this time\n",
    "# with 10 random restarts.\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(\n",
    "    noise_level=1, noise_level_bounds=(1e-10, 1e1)\n",
    ")\n",
    "gpr = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=0.0, n_restarts_optimizer=10, random_state=0\n",
    ")\n",
    "gpr.fit(X_train, y_train)\n",
    "y_mean, y_std = gpr.predict(X, return_std=True)\n",
    "\n",
    "# %%\n",
    "plt.plot(X, y, label=\"Expected signal\")\n",
    "plt.scatter(x=X_train[:, 0], y=y_train, color=\"black\", alpha=0.4, label=\"Observations\")\n",
    "plt.errorbar(X, y_mean, y_std, label=\"Posterior mean ± std\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "_ = plt.title(\n",
    "    (\n",
    "        f\"Initial: {kernel}\\nOptimum: {gpr.kernel_}\\nLog-Marginal-Likelihood: \"\n",
    "        f\"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}\"\n",
    "    ),\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "# %%\n",
    "#\n",
    "# As we hoped, random restarts allow the optimization to find the best set\n",
    "# of hyperparameters despite the bad initial values.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
