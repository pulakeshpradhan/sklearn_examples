{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62782a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================================================================\n",
    "Illustration of prior and posterior Gaussian process for different kernels\n",
    "==========================================================================\n",
    "\n",
    "This example illustrates the prior and posterior of a\n",
    ":class:`~sklearn.gaussian_process.GaussianProcessRegressor` with different\n",
    "kernels. Mean, standard deviation, and 5 samples are shown for both prior\n",
    "and posterior distributions.\n",
    "\n",
    "Here, we only give some illustration. To know more about kernels' formulation,\n",
    "refer to the :ref:`User Guide <gp_kernels>`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Helper function\n",
    "# ---------------\n",
    "#\n",
    "# Before presenting each individual kernel available for Gaussian processes,\n",
    "# we will define an helper function allowing us plotting samples drawn from\n",
    "# the Gaussian process.\n",
    "#\n",
    "# This function will take a\n",
    "# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model and will\n",
    "# drawn sample from the Gaussian process. If the model was not fit, the samples\n",
    "# are drawn from the prior distribution while after model fitting, the samples are\n",
    "# drawn from the posterior distribution.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_gpr_samples(gpr_model, n_samples, ax):\n",
    "    \"\"\"Plot samples drawn from the Gaussian process model.\n",
    "\n",
    "    If the Gaussian process model is not trained then the drawn samples are\n",
    "    drawn from the prior distribution. Otherwise, the samples are drawn from\n",
    "    the posterior distribution. Be aware that a sample here corresponds to a\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gpr_model : `GaussianProcessRegressor`\n",
    "        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.\n",
    "    n_samples : int\n",
    "        The number of samples to draw from the Gaussian process distribution.\n",
    "    ax : matplotlib axis\n",
    "        The matplotlib axis where to plot the samples.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, 5, 100)\n",
    "    X = x.reshape(-1, 1)\n",
    "\n",
    "    y_mean, y_std = gpr_model.predict(X, return_std=True)\n",
    "    y_samples = gpr_model.sample_y(X, n_samples)\n",
    "\n",
    "    for idx, single_prior in enumerate(y_samples.T):\n",
    "        ax.plot(\n",
    "            x,\n",
    "            single_prior,\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=f\"Sampled function #{idx + 1}\",\n",
    "        )\n",
    "    ax.plot(x, y_mean, color=\"black\", label=\"Mean\")\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        y_mean - y_std,\n",
    "        y_mean + y_std,\n",
    "        alpha=0.1,\n",
    "        color=\"black\",\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_ylim([-3, 3])\n",
    "\n",
    "\n",
    "# %%\n",
    "# Dataset and Gaussian process generation\n",
    "# ---------------------------------------\n",
    "# We will create a training dataset that we will use in the different sections.\n",
    "rng = np.random.RandomState(4)\n",
    "X_train = rng.uniform(0, 5, 10).reshape(-1, 1)\n",
    "y_train = np.sin((X_train[:, 0] - 2.5) ** 2)\n",
    "n_samples = 5\n",
    "\n",
    "# %%\n",
    "# Kernel cookbook\n",
    "# ---------------\n",
    "#\n",
    "# In this section, we illustrate some samples drawn from the prior and posterior\n",
    "# distributions of the Gaussian process with different kernels.\n",
    "#\n",
    "# Radial Basis Function kernel\n",
    "# ............................\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "fig.suptitle(\"Radial Basis Function kernel\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
    "print(\n",
    "    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n",
    "    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Rational Quadratic kernel\n",
    "# .........................\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "\n",
    "kernel = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "fig.suptitle(\"Rational Quadratic kernel\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
    "print(\n",
    "    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n",
    "    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Exp-Sine-Squared kernel\n",
    "# .......................\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "\n",
    "kernel = 1.0 * ExpSineSquared(\n",
    "    length_scale=1.0,\n",
    "    periodicity=3.0,\n",
    "    length_scale_bounds=(0.1, 10.0),\n",
    "    periodicity_bounds=(1.0, 10.0),\n",
    ")\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "fig.suptitle(\"Exp-Sine-Squared kernel\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
    "print(\n",
    "    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n",
    "    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Dot-product kernel\n",
    "# ..................\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct\n",
    "\n",
    "kernel = ConstantKernel(0.1, (0.01, 10.0)) * (\n",
    "    DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2\n",
    ")\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0, normalize_y=True)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "fig.suptitle(\"Dot-product kernel\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
    "print(\n",
    "    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n",
    "    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Mat√©rn kernel\n",
    "# ..............\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "kernel = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "\n",
    "# plot prior\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])\n",
    "axs[0].set_title(\"Samples from prior distribution\")\n",
    "\n",
    "# plot posterior\n",
    "gpr.fit(X_train, y_train)\n",
    "plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])\n",
    "axs[1].scatter(X_train[:, 0], y_train, color=\"red\", zorder=10, label=\"Observations\")\n",
    "axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc=\"upper left\")\n",
    "axs[1].set_title(\"Samples from posterior distribution\")\n",
    "\n",
    "fig.suptitle(\"Mat√©rn kernel\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "print(f\"Kernel parameters before fit:\\n{kernel})\")\n",
    "print(\n",
    "    f\"Kernel parameters after fit: \\n{gpr.kernel_} \\n\"\n",
    "    f\"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
