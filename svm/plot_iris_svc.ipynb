{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2004c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "Plot different SVM classifiers in the iris dataset\n",
    "==================================================\n",
    "\n",
    "Comparison of different linear SVM classifiers on a 2D projection of the iris\n",
    "dataset. We only consider the first 2 features of this dataset:\n",
    "\n",
    "- Sepal length\n",
    "- Sepal width\n",
    "\n",
    "This example shows how to plot the decision surface for four SVM classifiers\n",
    "with different kernels.\n",
    "\n",
    "The linear models ``LinearSVC()`` and ``SVC(kernel='linear')`` yield slightly\n",
    "different decision boundaries. This can be a consequence of the following\n",
    "differences:\n",
    "\n",
    "- ``LinearSVC`` minimizes the squared hinge loss while ``SVC`` minimizes the\n",
    "  regular hinge loss.\n",
    "\n",
    "- ``LinearSVC`` uses the One-vs-All (also known as One-vs-Rest) multiclass\n",
    "  reduction while ``SVC`` uses the One-vs-One multiclass reduction.\n",
    "\n",
    "Both linear models have linear decision boundaries (intersecting hyperplanes)\n",
    "while the non-linear kernel models (polynomial or Gaussian RBF) have more\n",
    "flexible non-linear decision boundaries with shapes that depend on the kind of\n",
    "kernel and its parameters.\n",
    "\n",
    ".. NOTE:: while plotting the decision function of classifiers for toy 2D\n",
    "   datasets can help get an intuitive understanding of their respective\n",
    "   expressive power, be aware that those intuitions don't always generalize to\n",
    "   more realistic high-dimensional problems.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (\n",
    "    svm.SVC(kernel=\"linear\", C=C),\n",
    "    svm.LinearSVC(C=C, max_iter=10000),\n",
    "    svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
    "    svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C),\n",
    ")\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = (\n",
    "    \"SVC with linear kernel\",\n",
    "    \"LinearSVC (linear kernel)\",\n",
    "    \"SVC with RBF kernel\",\n",
    "    \"SVC with polynomial (degree 3) kernel\",\n",
    ")\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "        clf,\n",
    "        X,\n",
    "        response_method=\"predict\",\n",
    "        cmap=plt.cm.coolwarm,\n",
    "        alpha=0.8,\n",
    "        ax=ax,\n",
    "        xlabel=iris.feature_names[0],\n",
    "        ylabel=iris.feature_names[1],\n",
    "    )\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
