{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================================================\n",
    "Selecting dimensionality reduction with Pipeline and GridSearchCV\n",
    "=================================================================\n",
    "\n",
    "This example constructs a pipeline that does dimensionality\n",
    "reduction followed by prediction with a support vector\n",
    "classifier. It demonstrates the use of ``GridSearchCV`` and\n",
    "``Pipeline`` to optimize over different classes of estimators in a\n",
    "single CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality\n",
    "reductions are compared to univariate feature selection during\n",
    "the grid search.\n",
    "\n",
    "Additionally, ``Pipeline`` can be instantiated with the ``memory``\n",
    "argument to memoize the transformers within the pipeline, avoiding to fit\n",
    "again the same transformers over and over.\n",
    "\n",
    "Note that the use of ``memory`` to enable caching becomes interesting when the\n",
    "fitting of a transformer is costly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Illustration of ``Pipeline`` and ``GridSearchCV``\n",
    "###############################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaling\", MinMaxScaler()),\n",
    "        # the reduce_dim stage is populated by the param_grid\n",
    "        (\"reduce_dim\", \"passthrough\"),\n",
    "        (\"classify\", LinearSVC(dual=False, max_iter=10000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "N_FEATURES_OPTIONS = [2, 4, 8]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "param_grid = [\n",
    "    {\n",
    "        \"reduce_dim\": [PCA(iterated_power=7), NMF(max_iter=1_000)],\n",
    "        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "    {\n",
    "        \"reduce_dim\": [SelectKBest(mutual_info_classif)],\n",
    "        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n",
    "        \"classify__C\": C_OPTIONS,\n",
    "    },\n",
    "]\n",
    "reducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n",
    "\n",
    "grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, y)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_[\"mean_test_score\"])\n",
    "# scores are in the order of param_grid iteration, which is alphabetical\n",
    "mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n",
    "# select score for best C\n",
    "mean_scores = mean_scores.max(axis=0)\n",
    "# create a dataframe to ease plotting\n",
    "mean_scores = pd.DataFrame(\n",
    "    mean_scores.T, index=N_FEATURES_OPTIONS, columns=reducer_labels\n",
    ")\n",
    "\n",
    "ax = mean_scores.plot.bar()\n",
    "ax.set_title(\"Comparing feature reduction techniques\")\n",
    "ax.set_xlabel(\"Reduced number of features\")\n",
    "ax.set_ylabel(\"Digit classification accuracy\")\n",
    "ax.set_ylim((0, 1))\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Caching transformers within a ``Pipeline``\n",
    "# ##########################################\n",
    "#\n",
    "# It is sometimes worthwhile storing the state of a specific transformer\n",
    "# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers\n",
    "# such situations. Therefore, we use the argument ``memory`` to enable caching.\n",
    "#\n",
    "# .. warning::\n",
    "#     Note that this example is, however, only an illustration since for this\n",
    "#     specific case fitting PCA is not necessarily slower than loading the\n",
    "#     cache. Hence, use the ``memory`` constructor parameter when the fitting\n",
    "#     of a transformer is costly.\n",
    "\n",
    "from shutil import rmtree\n",
    "\n",
    "from joblib import Memory\n",
    "\n",
    "# Create a temporary folder to store the transformers of the pipeline\n",
    "location = \"cachedir\"\n",
    "memory = Memory(location=location, verbose=10)\n",
    "cached_pipe = Pipeline(\n",
    "    [(\"reduce_dim\", PCA()), (\"classify\", LinearSVC(dual=False, max_iter=10000))],\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# This time, a cached pipeline will be used within the grid search\n",
    "\n",
    "\n",
    "# Delete the temporary cache before exiting\n",
    "memory.clear(warn=False)\n",
    "rmtree(location)\n",
    "\n",
    "# %%\n",
    "# The ``PCA`` fitting is only computed at the evaluation of the first\n",
    "# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The\n",
    "# other configurations of ``C`` will trigger the loading of the cached ``PCA``\n",
    "# estimator data, leading to save processing time. Therefore, the use of\n",
    "# caching the pipeline using ``memory`` is highly beneficial when fitting\n",
    "# a transformer is costly.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
