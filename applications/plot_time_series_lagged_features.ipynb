{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===========================================\n",
    "Lagged features for time series forecasting\n",
    "===========================================\n",
    "\n",
    "This example demonstrates how Polars-engineered lagged features can be used\n",
    "for time series forecasting with\n",
    ":class:`~sklearn.ensemble.HistGradientBoostingRegressor` on the Bike Sharing\n",
    "Demand dataset.\n",
    "\n",
    "See the example on\n",
    ":ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`\n",
    "for some data exploration on this dataset and a demo on periodic feature\n",
    "engineering.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Analyzing the Bike Sharing Demand dataset\n",
    "# -----------------------------------------\n",
    "#\n",
    "# We start by loading the data from the OpenML repository as a raw parquet file\n",
    "# to illustrate how to work with an arbitrary parquet file instead of hiding this\n",
    "# step in a convenience tool such as `sklearn.datasets.fetch_openml`.\n",
    "#\n",
    "# The URL of the parquet file can be found in the JSON description of the\n",
    "# Bike Sharing Demand dataset with id 44063 on openml.org\n",
    "# (https://openml.org/search?type=data&status=active&id=44063).\n",
    "#\n",
    "# The `sha256` hash of the file is also provided to ensure the integrity of the\n",
    "# downloaded file.\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.datasets import fetch_file\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(20)\n",
    "\n",
    "bike_sharing_data_file = fetch_file(\n",
    "    \"https://openml1.win.tue.nl/datasets/0004/44063/dataset_44063.pq\",\n",
    "    sha256=\"d120af76829af0d256338dc6dd4be5df4fd1f35bf3a283cab66a51c1c6abd06a\",\n",
    ")\n",
    "bike_sharing_data_file\n",
    "\n",
    "# %%\n",
    "# We load the parquet file with Polars for feature engineering. Polars\n",
    "# automatically caches common subexpressions which are reused in multiple\n",
    "# expressions (like `pl.col(\"count\").shift(1)` below). See\n",
    "# https://docs.pola.rs/user-guide/lazy/optimizations/ for more information.\n",
    "\n",
    "df = pl.read_parquet(bike_sharing_data_file)\n",
    "\n",
    "# %%\n",
    "# Next, we take a look at the statistical summary of the dataset\n",
    "# so that we can better understand the data that we are working with.\n",
    "import polars.selectors as cs\n",
    "\n",
    "summary = df.select(cs.numeric()).describe()\n",
    "summary\n",
    "\n",
    "# %%\n",
    "# Let us look at the count of the seasons `\"fall\"`, `\"spring\"`, `\"summer\"`\n",
    "# and `\"winter\"` present in the dataset to confirm they are balanced.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"season\"].value_counts()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Generating Polars-engineered lagged features\n",
    "# --------------------------------------------\n",
    "# Let's consider the problem of predicting the demand at the\n",
    "# next hour given past demands. Since the demand is a continuous\n",
    "# variable, one could intuitively use any regression model. However, we do\n",
    "# not have the usual `(X_train, y_train)` dataset. Instead, we just have\n",
    "# the `y_train` demand data sequentially organized by time.\n",
    "lagged_df = df.select(\n",
    "    \"count\",\n",
    "    *[pl.col(\"count\").shift(i).alias(f\"lagged_count_{i}h\") for i in [1, 2, 3]],\n",
    "    lagged_count_1d=pl.col(\"count\").shift(24),\n",
    "    lagged_count_1d_1h=pl.col(\"count\").shift(24 + 1),\n",
    "    lagged_count_7d=pl.col(\"count\").shift(7 * 24),\n",
    "    lagged_count_7d_1h=pl.col(\"count\").shift(7 * 24 + 1),\n",
    "    lagged_mean_24h=pl.col(\"count\").shift(1).rolling_mean(24),\n",
    "    lagged_max_24h=pl.col(\"count\").shift(1).rolling_max(24),\n",
    "    lagged_min_24h=pl.col(\"count\").shift(1).rolling_min(24),\n",
    "    lagged_mean_7d=pl.col(\"count\").shift(1).rolling_mean(7 * 24),\n",
    "    lagged_max_7d=pl.col(\"count\").shift(1).rolling_max(7 * 24),\n",
    "    lagged_min_7d=pl.col(\"count\").shift(1).rolling_min(7 * 24),\n",
    ")\n",
    "lagged_df.tail(10)\n",
    "\n",
    "# %%\n",
    "# Watch out however, the first lines have undefined values because their own\n",
    "# past is unknown. This depends on how much lag we used:\n",
    "lagged_df.head(10)\n",
    "\n",
    "# %%\n",
    "# We can now separate the lagged features in a matrix `X` and the target variable\n",
    "# (the counts to predict) in an array of the same first dimension `y`.\n",
    "lagged_df = lagged_df.drop_nulls()\n",
    "X = lagged_df.drop(\"count\")\n",
    "y = lagged_df[\"count\"]\n",
    "print(\"X shape: {}\\ny shape: {}\".format(X.shape, y.shape))\n",
    "\n",
    "# %%\n",
    "# Naive evaluation of the next hour bike demand regression\n",
    "# --------------------------------------------------------\n",
    "# Let's randomly split our tabularized dataset to train a gradient\n",
    "# boosting regression tree (GBRT) model and evaluate it using Mean\n",
    "# Absolute Percentage Error (MAPE). If our model is aimed at forecasting\n",
    "# (i.e., predicting future data from past data), we should not use training\n",
    "# data that are ulterior to the testing data. In time series machine learning\n",
    "# the \"i.i.d\" (independent and identically distributed) assumption does not\n",
    "# hold true as the data points are not independent and have a temporal\n",
    "# relationship.\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = HistGradientBoostingRegressor().fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Taking a look at the performance of the model.\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# %%\n",
    "# Proper next hour forecasting evaluation\n",
    "# ---------------------------------------\n",
    "# Let's use a proper evaluation splitting strategies that takes into account\n",
    "# the temporal structure of the dataset to evaluate our model's ability to\n",
    "# predict data points in the future (to avoid cheating by reading values from\n",
    "# the lagged features in the training set).\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "ts_cv = TimeSeriesSplit(\n",
    "    n_splits=3,  # to keep the notebook fast enough on common laptops\n",
    "    gap=48,  # 2 days data gap between train and test\n",
    "    max_train_size=10000,  # keep train sets of comparable sizes\n",
    "    test_size=3000,  # for 2 or 3 digits of precision in scores\n",
    ")\n",
    "all_splits = list(ts_cv.split(X, y))\n",
    "\n",
    "# %%\n",
    "# Training the model and evaluating its performance based on MAPE.\n",
    "train_idx, test_idx = all_splits[0]\n",
    "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "model = HistGradientBoostingRegressor().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# %%\n",
    "# The generalization error measured via a shuffled trained test split\n",
    "# is too optimistic. The generalization via a time-based split is likely to\n",
    "# be more representative of the true performance of the regression model.\n",
    "# Let's assess this variability of our error evaluation with proper\n",
    "# cross-validation:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_mape_scores = -cross_val_score(\n",
    "    model, X, y, cv=ts_cv, scoring=\"neg_mean_absolute_percentage_error\"\n",
    ")\n",
    "cv_mape_scores\n",
    "\n",
    "# %%\n",
    "# The variability across splits is quite large! In a real life setting\n",
    "# it would be advised to use more splits to better assess the variability.\n",
    "# Let's report the mean CV scores and their standard deviation from now on.\n",
    "print(f\"CV MAPE: {cv_mape_scores.mean():.3f} ± {cv_mape_scores.std():.3f}\")\n",
    "\n",
    "# %%\n",
    "# We can compute several combinations of evaluation metrics and loss functions,\n",
    "# which are reported a bit below.\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_pinball_loss,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "def consolidate_scores(cv_results, scores, metric):\n",
    "    if metric == \"MAPE\":\n",
    "        scores[metric].append(f\"{value.mean():.2f} ± {value.std():.2f}\")\n",
    "    else:\n",
    "        scores[metric].append(f\"{value.mean():.1f} ± {value.std():.1f}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scoring = {\n",
    "    \"MAPE\": make_scorer(mean_absolute_percentage_error),\n",
    "    \"RMSE\": make_scorer(root_mean_squared_error),\n",
    "    \"MAE\": make_scorer(mean_absolute_error),\n",
    "    \"pinball_loss_05\": make_scorer(mean_pinball_loss, alpha=0.05),\n",
    "    \"pinball_loss_50\": make_scorer(mean_pinball_loss, alpha=0.50),\n",
    "    \"pinball_loss_95\": make_scorer(mean_pinball_loss, alpha=0.95),\n",
    "}\n",
    "loss_functions = [\"squared_error\", \"poisson\", \"absolute_error\"]\n",
    "scores = defaultdict(list)\n",
    "for loss_func in loss_functions:\n",
    "    model = HistGradientBoostingRegressor(loss=loss_func)\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=ts_cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=2,\n",
    "    )\n",
    "    time = cv_results[\"fit_time\"]\n",
    "    scores[\"loss\"].append(loss_func)\n",
    "    scores[\"fit_time\"].append(f\"{time.mean():.2f} ± {time.std():.2f} s\")\n",
    "\n",
    "    for key, value in cv_results.items():\n",
    "        if key.startswith(\"test_\"):\n",
    "            metric = key.split(\"test_\")[1]\n",
    "            scores = consolidate_scores(cv_results, scores, metric)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Modeling predictive uncertainty via quantile regression\n",
    "# -------------------------------------------------------\n",
    "# Instead of modeling the expected value of the distribution of\n",
    "# :math:`Y|X` like the least squares and Poisson losses do, one could try to\n",
    "# estimate quantiles of the conditional distribution.\n",
    "#\n",
    "# :math:`Y|X=x_i` is expected to be a random variable for a given data point\n",
    "# :math:`x_i` because we expect that the number of rentals cannot be 100%\n",
    "# accurately predicted from the features. It can be influenced by other\n",
    "# variables not properly captured by the existing lagged features. For\n",
    "# instance whether or not it will rain in the next hour cannot be fully\n",
    "# anticipated from the past hours bike rental data. This is what we\n",
    "# call aleatoric uncertainty.\n",
    "#\n",
    "# Quantile regression makes it possible to give a finer description of that\n",
    "# distribution without making strong assumptions on its shape.\n",
    "quantile_list = [0.05, 0.5, 0.95]\n",
    "\n",
    "for quantile in quantile_list:\n",
    "    model = HistGradientBoostingRegressor(loss=\"quantile\", quantile=quantile)\n",
    "    cv_results = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=ts_cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=2,\n",
    "    )\n",
    "    time = cv_results[\"fit_time\"]\n",
    "    scores[\"fit_time\"].append(f\"{time.mean():.2f} ± {time.std():.2f} s\")\n",
    "\n",
    "    scores[\"loss\"].append(f\"quantile {int(quantile*100)}\")\n",
    "    for key, value in cv_results.items():\n",
    "        if key.startswith(\"test_\"):\n",
    "            metric = key.split(\"test_\")[1]\n",
    "            scores = consolidate_scores(cv_results, scores, metric)\n",
    "\n",
    "scores_df = pl.DataFrame(scores)\n",
    "scores_df\n",
    "\n",
    "\n",
    "# %%\n",
    "# Let us take a look at the losses that minimise each metric.\n",
    "def min_arg(col):\n",
    "    col_split = pl.col(col).str.split(\" \")\n",
    "    return pl.arg_sort_by(\n",
    "        col_split.list.get(0).cast(pl.Float64),\n",
    "        col_split.list.get(2).cast(pl.Float64),\n",
    "    ).first()\n",
    "\n",
    "\n",
    "scores_df.select(\n",
    "    pl.col(\"loss\").get(min_arg(col_name)).alias(col_name)\n",
    "    for col_name in scores_df.columns\n",
    "    if col_name != \"loss\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Even if the score distributions overlap due to the variance in the dataset,\n",
    "# it is true that the average RMSE is lower when `loss=\"squared_error\"`, whereas\n",
    "# the average MAPE is lower when `loss=\"absolute_error\"` as expected. That is\n",
    "# also the case for the Mean Pinball Loss with the quantiles 5 and 95. The score\n",
    "# corresponding to the 50 quantile loss is overlapping with the score obtained\n",
    "# by minimizing other loss functions, which is also the case for the MAE.\n",
    "#\n",
    "# A qualitative look at the predictions\n",
    "# -------------------------------------\n",
    "# We can now visualize the performance of the model with regards\n",
    "# to the 5th percentile, median and the 95th percentile:\n",
    "all_splits = list(ts_cv.split(X, y))\n",
    "train_idx, test_idx = all_splits[0]\n",
    "\n",
    "X_train, X_test = X[train_idx, :], X[test_idx, :]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "max_iter = 50\n",
    "gbrt_mean_poisson = HistGradientBoostingRegressor(loss=\"poisson\", max_iter=max_iter)\n",
    "gbrt_mean_poisson.fit(X_train, y_train)\n",
    "mean_predictions = gbrt_mean_poisson.predict(X_test)\n",
    "\n",
    "gbrt_median = HistGradientBoostingRegressor(\n",
    "    loss=\"quantile\", quantile=0.5, max_iter=max_iter\n",
    ")\n",
    "gbrt_median.fit(X_train, y_train)\n",
    "median_predictions = gbrt_median.predict(X_test)\n",
    "\n",
    "gbrt_percentile_5 = HistGradientBoostingRegressor(\n",
    "    loss=\"quantile\", quantile=0.05, max_iter=max_iter\n",
    ")\n",
    "gbrt_percentile_5.fit(X_train, y_train)\n",
    "percentile_5_predictions = gbrt_percentile_5.predict(X_test)\n",
    "\n",
    "gbrt_percentile_95 = HistGradientBoostingRegressor(\n",
    "    loss=\"quantile\", quantile=0.95, max_iter=max_iter\n",
    ")\n",
    "gbrt_percentile_95.fit(X_train, y_train)\n",
    "percentile_95_predictions = gbrt_percentile_95.predict(X_test)\n",
    "\n",
    "# %%\n",
    "# We can now take a look at the predictions made by the regression models:\n",
    "last_hours = slice(-96, None)\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "plt.title(\"Predictions by regression models\")\n",
    "ax.plot(\n",
    "    y_test[last_hours],\n",
    "    \"x-\",\n",
    "    alpha=0.2,\n",
    "    label=\"Actual demand\",\n",
    "    color=\"black\",\n",
    ")\n",
    "ax.plot(\n",
    "    median_predictions[last_hours],\n",
    "    \"^-\",\n",
    "    label=\"GBRT median\",\n",
    ")\n",
    "ax.plot(\n",
    "    mean_predictions[last_hours],\n",
    "    \"x-\",\n",
    "    label=\"GBRT mean (Poisson)\",\n",
    ")\n",
    "ax.fill_between(\n",
    "    np.arange(96),\n",
    "    percentile_5_predictions[last_hours],\n",
    "    percentile_95_predictions[last_hours],\n",
    "    alpha=0.3,\n",
    "    label=\"GBRT 90% interval\",\n",
    ")\n",
    "_ = ax.legend()\n",
    "\n",
    "# %%\n",
    "# Here it's interesting to notice that the blue area between the 5% and 95%\n",
    "# percentile estimators has a width that varies with the time of the day:\n",
    "#\n",
    "# - At night, the blue band is much narrower: the pair of models is quite\n",
    "#   certain that there will be a small number of bike rentals. And furthermore\n",
    "#   these seem correct in the sense that the actual demand stays in that blue\n",
    "#   band.\n",
    "# - During the day, the blue band is much wider: the uncertainty grows, probably\n",
    "#   because of the variability of the weather that can have a very large impact,\n",
    "#   especially on week-ends.\n",
    "# - We can also see that during week-days, the commute pattern is still visible in\n",
    "#   the 5% and 95% estimations.\n",
    "# - Finally, it is expected that 10% of the time, the actual demand does not lie\n",
    "#   between the 5% and 95% percentile estimates. On this test span, the actual\n",
    "#   demand seems to be higher, especially during the rush hours. It might reveal that\n",
    "#   our 95% percentile estimator underestimates the demand peaks. This could be be\n",
    "#   quantitatively confirmed by computing empirical coverage numbers as done in\n",
    "#   the :ref:`calibration of confidence intervals <calibration-section>`.\n",
    "#\n",
    "# Looking at the performance of non-linear regression models vs\n",
    "# the best models:\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(15, 6), sharey=True)\n",
    "fig.suptitle(\"Non-linear regression models\")\n",
    "predictions = [\n",
    "    median_predictions,\n",
    "    percentile_5_predictions,\n",
    "    percentile_95_predictions,\n",
    "]\n",
    "labels = [\n",
    "    \"Median\",\n",
    "    \"5th percentile\",\n",
    "    \"95th percentile\",\n",
    "]\n",
    "for ax, pred, label in zip(axes, predictions, labels):\n",
    "    PredictionErrorDisplay.from_predictions(\n",
    "        y_true=y_test,\n",
    "        y_pred=pred,\n",
    "        kind=\"residual_vs_predicted\",\n",
    "        scatter_kwargs={\"alpha\": 0.3},\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set(xlabel=\"Predicted demand\", ylabel=\"True demand\")\n",
    "    ax.legend([\"Best model\", label])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Conclusion\n",
    "# ----------\n",
    "# Through this example we explored time series forecasting using lagged\n",
    "# features. We compared a naive regression (using the standardized\n",
    "# :class:`~sklearn.model_selection.train_test_split`) with a proper time\n",
    "# series evaluation strategy using\n",
    "# :class:`~sklearn.model_selection.TimeSeriesSplit`. We observed that the\n",
    "# model trained using :class:`~sklearn.model_selection.train_test_split`,\n",
    "# having a default value of `shuffle` set to `True` produced an overly\n",
    "# optimistic Mean Average Percentage Error (MAPE). The results\n",
    "# produced from the time-based split better represent the performance\n",
    "# of our time-series regression model. We also analyzed the predictive uncertainty\n",
    "# of our model via Quantile Regression. Predictions based on the 5th and\n",
    "# 95th percentile using `loss=\"quantile\"` provide us with a quantitative estimate\n",
    "# of the uncertainty of the forecasts made by our time series regression model.\n",
    "# Uncertainty estimation can also be performed\n",
    "# using `MAPIE <https://mapie.readthedocs.io/en/latest/index.html>`_,\n",
    "# that provides an implementation based on recent work on conformal prediction\n",
    "# methods and estimates both aleatoric and epistemic uncertainty at the same time.\n",
    "# Furthermore, functionalities provided\n",
    "# by `sktime <https://www.sktime.net/en/latest/users.html>`_\n",
    "# can be used to extend scikit-learn estimators by making use of recursive time\n",
    "# series forecasting, that enables dynamic predictions of future values.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
