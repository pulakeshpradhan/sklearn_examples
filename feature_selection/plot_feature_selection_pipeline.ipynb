{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be261b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================\n",
    "Pipeline ANOVA SVM\n",
    "==================\n",
    "\n",
    "This example shows how a feature selection can be easily integrated within\n",
    "a machine learning pipeline.\n",
    "\n",
    "We also show that you can easily inspect part of the pipeline.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# We will start by generating a binary classification dataset. Subsequently, we\n",
    "# will divide the dataset into two subsets.\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=20,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# %%\n",
    "# A common mistake done with feature selection is to search a subset of\n",
    "# discriminative features on the full dataset, instead of only using the\n",
    "# training set. The usage of scikit-learn :func:`~sklearn.pipeline.Pipeline`\n",
    "# prevents to make such mistake.\n",
    "#\n",
    "# Here, we will demonstrate how to build a pipeline where the first step will\n",
    "# be the feature selection.\n",
    "#\n",
    "# When calling `fit` on the training data, a subset of feature will be selected\n",
    "# and the index of these selected features will be stored. The feature selector\n",
    "# will subsequently reduce the number of features, and pass this subset to the\n",
    "# classifier which will be trained.\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "anova_filter = SelectKBest(f_classif, k=3)\n",
    "clf = LinearSVC()\n",
    "anova_svm = make_pipeline(anova_filter, clf)\n",
    "anova_svm.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Once the training is complete, we can predict on new unseen samples. In this\n",
    "# case, the feature selector will only select the most discriminative features\n",
    "# based on the information stored during training. Then, the data will be\n",
    "# passed to the classifier which will make the prediction.\n",
    "#\n",
    "# Here, we show the final metrics via a classification report.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = anova_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# %%\n",
    "# Be aware that you can inspect a step in the pipeline. For instance, we might\n",
    "# be interested about the parameters of the classifier. Since we selected\n",
    "# three features, we expect to have three coefficients.\n",
    "\n",
    "anova_svm[-1].coef_\n",
    "\n",
    "# %%\n",
    "# However, we do not know which features were selected from the original\n",
    "# dataset. We could proceed by several manners. Here, we will invert the\n",
    "# transformation of these coefficients to get information about the original\n",
    "# space.\n",
    "\n",
    "anova_svm[:-1].inverse_transform(anova_svm[-1].coef_)\n",
    "\n",
    "# %%\n",
    "# We can see that the features with non-zero coefficients are the selected\n",
    "# features by the first step.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
