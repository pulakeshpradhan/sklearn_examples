{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "Model-based and sequential feature selection\n",
    "============================================\n",
    "\n",
    "This example illustrates and compares two approaches for feature selection:\n",
    ":class:`~sklearn.feature_selection.SelectFromModel` which is based on feature\n",
    "importance, and\n",
    ":class:`~sklearn.feature_selection.SequentialFeatureSelector` which relies\n",
    "on a greedy approach.\n",
    "\n",
    "We use the Diabetes dataset, which consists of 10 features collected from 442\n",
    "diabetes patients.\n",
    "\n",
    "Authors: `Manoj Kumar <mks542@nyu.edu>`_,\n",
    "`Maria Telenczuk <https://github.com/maikia>`_, Nicolas Hug.\n",
    "\n",
    "License: BSD 3 clause\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Loading the data\n",
    "# ----------------\n",
    "#\n",
    "# We first load the diabetes dataset which is available from within\n",
    "# scikit-learn, and print its description:\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "print(diabetes.DESCR)\n",
    "\n",
    "# %%\n",
    "# Feature importance from coefficients\n",
    "# ------------------------------------\n",
    "#\n",
    "# To get an idea of the importance of the features, we are going to use the\n",
    "# :class:`~sklearn.linear_model.RidgeCV` estimator. The features with the\n",
    "# highest absolute `coef_` value are considered the most important.\n",
    "# We can observe the coefficients directly without needing to scale them (or\n",
    "# scale the data) because from the description above, we know that the features\n",
    "# were already standardized.\n",
    "# For a more complete example on the interpretations of the coefficients of\n",
    "# linear models, you may refer to\n",
    "# :ref:`sphx_glr_auto_examples_inspection_plot_linear_model_coefficient_interpretation.py`.  # noqa: E501\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = RidgeCV(alphas=np.logspace(-6, 6, num=5)).fit(X, y)\n",
    "importance = np.abs(ridge.coef_)\n",
    "feature_names = np.array(diabetes.feature_names)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Selecting features based on importance\n",
    "# --------------------------------------\n",
    "#\n",
    "# Now we want to select the two features which are the most important according\n",
    "# to the coefficients. The :class:`~sklearn.feature_selection.SelectFromModel`\n",
    "# is meant just for that. :class:`~sklearn.feature_selection.SelectFromModel`\n",
    "# accepts a `threshold` parameter and will select the features whose importance\n",
    "# (defined by the coefficients) are above this threshold.\n",
    "#\n",
    "# Since we want to select only 2 features, we will set this threshold slightly\n",
    "# above the coefficient of third most important feature.\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "threshold = np.sort(importance)[-3] + 0.01\n",
    "\n",
    "tic = time()\n",
    "sfm = SelectFromModel(ridge, threshold=threshold).fit(X, y)\n",
    "toc = time()\n",
    "print(f\"Features selected by SelectFromModel: {feature_names[sfm.get_support()]}\")\n",
    "print(f\"Done in {toc - tic:.3f}s\")\n",
    "\n",
    "# %%\n",
    "# Selecting features with Sequential Feature Selection\n",
    "# ----------------------------------------------------\n",
    "#\n",
    "# Another way of selecting features is to use\n",
    "# :class:`~sklearn.feature_selection.SequentialFeatureSelector`\n",
    "# (SFS). SFS is a greedy procedure where, at each iteration, we choose the best\n",
    "# new feature to add to our selected features based a cross-validation score.\n",
    "# That is, we start with 0 features and choose the best single feature with the\n",
    "# highest score. The procedure is repeated until we reach the desired number of\n",
    "# selected features.\n",
    "#\n",
    "# We can also go in the reverse direction (backward SFS), *i.e.* start with all\n",
    "# the features and greedily choose features to remove one by one. We illustrate\n",
    "# both approaches here.\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "tic_fwd = time()\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=2, direction=\"forward\"\n",
    ").fit(X, y)\n",
    "toc_fwd = time()\n",
    "\n",
    "tic_bwd = time()\n",
    "sfs_backward = SequentialFeatureSelector(\n",
    "    ridge, n_features_to_select=2, direction=\"backward\"\n",
    ").fit(X, y)\n",
    "toc_bwd = time()\n",
    "\n",
    "print(\n",
    "    \"Features selected by forward sequential selection: \"\n",
    "    f\"{feature_names[sfs_forward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_fwd - tic_fwd:.3f}s\")\n",
    "print(\n",
    "    \"Features selected by backward sequential selection: \"\n",
    "    f\"{feature_names[sfs_backward.get_support()]}\"\n",
    ")\n",
    "print(f\"Done in {toc_bwd - tic_bwd:.3f}s\")\n",
    "\n",
    "# %%\n",
    "# Interestingly, forward and backward selection have selected the same set of\n",
    "# features. In general, this isn't the case and the two methods would lead to\n",
    "# different results.\n",
    "#\n",
    "# We also note that the features selected by SFS differ from those selected by\n",
    "# feature importance: SFS selects `bmi` instead of `s1`. This does sound\n",
    "# reasonable though, since `bmi` corresponds to the third most important\n",
    "# feature according to the coefficients. It is quite remarkable considering\n",
    "# that SFS makes no use of the coefficients at all.\n",
    "#\n",
    "# To finish with, we should note that\n",
    "# :class:`~sklearn.feature_selection.SelectFromModel` is significantly faster\n",
    "# than SFS. Indeed, :class:`~sklearn.feature_selection.SelectFromModel` only\n",
    "# needs to fit a model once, while SFS needs to cross-validate many different\n",
    "# models for each of the iterations. SFS however works with any model, while\n",
    "# :class:`~sklearn.feature_selection.SelectFromModel` requires the underlying\n",
    "# estimator to expose a `coef_` attribute or a `feature_importances_`\n",
    "# attribute. The forward SFS is faster than the backward SFS because it only\n",
    "# needs to perform `n_features_to_select = 2` iterations, while the backward\n",
    "# SFS needs to perform `n_features - n_features_to_select = 8` iterations.\n",
    "#\n",
    "# Using negative tolerance values\n",
    "# -------------------------------\n",
    "#\n",
    "# :class:`~sklearn.feature_selection.SequentialFeatureSelector` can be used\n",
    "# to remove features present in the dataset and return a\n",
    "# smaller subset of the original features with `direction=\"backward\"`\n",
    "# and a negative value of `tol`.\n",
    "#\n",
    "# We begin by loading the Breast Cancer dataset, consisting of 30 different\n",
    "# features and 569 samples.\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer_data = load_breast_cancer()\n",
    "X, y = breast_cancer_data.data, breast_cancer_data.target\n",
    "feature_names = np.array(breast_cancer_data.feature_names)\n",
    "print(breast_cancer_data.DESCR)\n",
    "\n",
    "# %%\n",
    "# We will make use of the :class:`~sklearn.linear_model.LogisticRegression`\n",
    "# estimator with :class:`~sklearn.feature_selection.SequentialFeatureSelector`\n",
    "# to perform the feature selection.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for tol in [-1e-2, -1e-3, -1e-4]:\n",
    "    start = time()\n",
    "    feature_selector = SequentialFeatureSelector(\n",
    "        LogisticRegression(),\n",
    "        n_features_to_select=\"auto\",\n",
    "        direction=\"backward\",\n",
    "        scoring=\"roc_auc\",\n",
    "        tol=tol,\n",
    "        n_jobs=2,\n",
    "    )\n",
    "    model = make_pipeline(StandardScaler(), feature_selector, LogisticRegression())\n",
    "    model.fit(X, y)\n",
    "    end = time()\n",
    "    print(f\"\\ntol: {tol}\")\n",
    "    print(f\"Features selected: {feature_names[model[1].get_support()]}\")\n",
    "    print(f\"ROC AUC score: {roc_auc_score(y, model.predict_proba(X)[:, 1]):.3f}\")\n",
    "    print(f\"Done in {end - start:.3f}s\")\n",
    "\n",
    "# %%\n",
    "# We can see that the number of features selected tend to increase as negative\n",
    "# values of `tol` approach to zero. The time taken for feature selection also\n",
    "# decreases as the values of `tol` come closer to zero.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
