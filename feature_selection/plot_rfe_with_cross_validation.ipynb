{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b148307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================================\n",
    "Recursive feature elimination with cross-validation\n",
    "===================================================\n",
    "\n",
    "A Recursive Feature Elimination (RFE) example with automatic tuning of the\n",
    "number of features selected with cross-validation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Data generation\n",
    "# ---------------\n",
    "#\n",
    "# We build a classification task using 3 informative features. The introduction\n",
    "# of 2 additional redundant (i.e. correlated) features has the effect that the\n",
    "# selected features vary depending on the cross-validation fold. The remaining\n",
    "# features are non-informative as they are drawn at random.\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=15,\n",
    "    n_informative=3,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=8,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=0.8,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Model training and selection\n",
    "# ----------------------------\n",
    "#\n",
    "# We create the RFE object and compute the cross-validated scores. The scoring\n",
    "# strategy \"accuracy\" optimizes the proportion of correctly classified samples.\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = LogisticRegression()\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    step=1,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    n_jobs=2,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "\n",
    "# %%\n",
    "# In the present case, the model with 3 features (which corresponds to the true\n",
    "# generative model) is found to be the most optimal.\n",
    "#\n",
    "# Plot number of features VS. cross-validation scores\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "cv_results = pd.DataFrame(rfecv.cv_results_)\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.errorbar(\n",
    "    x=cv_results[\"n_features\"],\n",
    "    y=cv_results[\"mean_test_score\"],\n",
    "    yerr=cv_results[\"std_test_score\"],\n",
    ")\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# From the plot above one can further notice a plateau of equivalent scores\n",
    "# (similar mean value and overlapping errorbars) for 3 to 5 selected features.\n",
    "# This is the result of introducing correlated features. Indeed, the optimal\n",
    "# model selected by the RFE can lie within this range, depending on the\n",
    "# cross-validation technique. The test accuracy decreases above 5 selected\n",
    "# features, this is, keeping non-informative features leads to over-fitting and\n",
    "# is therefore detrimental for the statistical performance of the models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
