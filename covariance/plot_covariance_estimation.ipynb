{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=======================================================================\n",
    "Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n",
    "=======================================================================\n",
    "\n",
    "When working with covariance estimation, the usual approach is to use\n",
    "a maximum likelihood estimator, such as the\n",
    ":class:`~sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it\n",
    "converges to the true (population) covariance when given many\n",
    "observations. However, it can also be beneficial to regularize it, in\n",
    "order to reduce its variance; this, in turn, introduces some bias. This\n",
    "example illustrates the simple regularization used in\n",
    ":ref:`shrunk_covariance` estimators. In particular, it focuses on how to\n",
    "set the amount of regularization, i.e. how to choose the bias-variance\n",
    "trade-off.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Generate sample data\n",
    "# --------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "n_features, n_samples = 40, 20\n",
    "np.random.seed(42)\n",
    "base_X_train = np.random.normal(size=(n_samples, n_features))\n",
    "base_X_test = np.random.normal(size=(n_samples, n_features))\n",
    "\n",
    "# Color samples\n",
    "coloring_matrix = np.random.normal(size=(n_features, n_features))\n",
    "X_train = np.dot(base_X_train, coloring_matrix)\n",
    "X_test = np.dot(base_X_test, coloring_matrix)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Compute the likelihood on test data\n",
    "# -----------------------------------\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.covariance import ShrunkCovariance, empirical_covariance, log_likelihood\n",
    "\n",
    "# spanning a range of possible shrinkage coefficient values\n",
    "shrinkages = np.logspace(-2, 0, 30)\n",
    "negative_logliks = [\n",
    "    -ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages\n",
    "]\n",
    "\n",
    "# under the ground-truth model, which we would not have access to in real\n",
    "# settings\n",
    "real_cov = np.dot(coloring_matrix.T, coloring_matrix)\n",
    "emp_cov = empirical_covariance(X_train)\n",
    "loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n",
    "\n",
    "\n",
    "# %%\n",
    "# Compare different approaches to setting the regularization parameter\n",
    "# --------------------------------------------------------------------\n",
    "#\n",
    "# Here we compare 3 approaches:\n",
    "#\n",
    "# * Setting the parameter by cross-validating the likelihood on three folds\n",
    "#   according to a grid of potential shrinkage parameters.\n",
    "#\n",
    "# * A close formula proposed by Ledoit and Wolf to compute\n",
    "#   the asymptotically optimal regularization parameter (minimizing a MSE\n",
    "#   criterion), yielding the :class:`~sklearn.covariance.LedoitWolf`\n",
    "#   covariance estimate.\n",
    "#\n",
    "# * An improvement of the Ledoit-Wolf shrinkage, the\n",
    "#   :class:`~sklearn.covariance.OAS`, proposed by Chen et al. Its\n",
    "#   convergence is significantly better under the assumption that the data\n",
    "#   are Gaussian, in particular for small samples.\n",
    "\n",
    "\n",
    "from sklearn.covariance import OAS, LedoitWolf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# GridSearch for an optimal shrinkage coefficient\n",
    "tuned_parameters = [{\"shrinkage\": shrinkages}]\n",
    "cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\n",
    "cv.fit(X_train)\n",
    "\n",
    "# Ledoit-Wolf optimal shrinkage coefficient estimate\n",
    "lw = LedoitWolf()\n",
    "loglik_lw = lw.fit(X_train).score(X_test)\n",
    "\n",
    "# OAS coefficient estimate\n",
    "oa = OAS()\n",
    "loglik_oa = oa.fit(X_train).score(X_test)\n",
    "\n",
    "# %%\n",
    "# Plot results\n",
    "# ------------\n",
    "#\n",
    "#\n",
    "# To quantify estimation error, we plot the likelihood of unseen data for\n",
    "# different values of the shrinkage parameter. We also show the choices by\n",
    "# cross-validation, or with the LedoitWolf and OAS estimates.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Regularized covariance: likelihood and shrinkage coefficient\")\n",
    "plt.xlabel(\"Regularization parameter: shrinkage coefficient\")\n",
    "plt.ylabel(\"Error: negative log-likelihood on test data\")\n",
    "# range shrinkage curve\n",
    "plt.loglog(shrinkages, negative_logliks, label=\"Negative log-likelihood\")\n",
    "\n",
    "plt.plot(plt.xlim(), 2 * [loglik_real], \"--r\", label=\"Real covariance likelihood\")\n",
    "\n",
    "# adjust view\n",
    "lik_max = np.amax(negative_logliks)\n",
    "lik_min = np.amin(negative_logliks)\n",
    "ymin = lik_min - 6.0 * np.log((plt.ylim()[1] - plt.ylim()[0]))\n",
    "ymax = lik_max + 10.0 * np.log(lik_max - lik_min)\n",
    "xmin = shrinkages[0]\n",
    "xmax = shrinkages[-1]\n",
    "# LW likelihood\n",
    "plt.vlines(\n",
    "    lw.shrinkage_,\n",
    "    ymin,\n",
    "    -loglik_lw,\n",
    "    color=\"magenta\",\n",
    "    linewidth=3,\n",
    "    label=\"Ledoit-Wolf estimate\",\n",
    ")\n",
    "# OAS likelihood\n",
    "plt.vlines(\n",
    "    oa.shrinkage_, ymin, -loglik_oa, color=\"purple\", linewidth=3, label=\"OAS estimate\"\n",
    ")\n",
    "# best CV estimator likelihood\n",
    "plt.vlines(\n",
    "    cv.best_estimator_.shrinkage,\n",
    "    ymin,\n",
    "    -cv.best_estimator_.score(X_test),\n",
    "    color=\"cyan\",\n",
    "    linewidth=3,\n",
    "    label=\"Cross-validation best estimate\",\n",
    ")\n",
    "\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# .. note::\n",
    "#\n",
    "#    The maximum likelihood estimate corresponds to no shrinkage,\n",
    "#    and thus performs poorly. The Ledoit-Wolf estimate performs really well,\n",
    "#    as it is close to the optimal and is not computationally costly. In this\n",
    "#    example, the OAS estimate is a bit further away. Interestingly, both\n",
    "#    approaches outperform cross-validation, which is significantly most\n",
    "#    computationally costly.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
