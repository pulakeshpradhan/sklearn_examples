{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a651ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================\n",
    "Pipelining: chaining a PCA and a logistic regression\n",
    "=========================================================\n",
    "\n",
    "The PCA does an unsupervised dimensionality reduction, while the logistic\n",
    "regression does the prediction.\n",
    "\n",
    "We use a GridSearchCV to set the dimensionality of the PCA\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "pca = PCA()\n",
    "# Define a Standard Scaler to normalize inputs\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[(\"scaler\", scaler), (\"pca\", pca), (\"logistic\", logistic)])\n",
    "\n",
    "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
    "# Parameters of pipelines can be set using '__' separated parameter names:\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [5, 15, 30, 45, 60],\n",
    "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=2)\n",
    "search.fit(X_digits, y_digits)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_digits)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(\n",
    "    np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, \"+\", linewidth=2\n",
    ")\n",
    "ax0.set_ylabel(\"PCA explained variance ratio\")\n",
    "\n",
    "ax0.axvline(\n",
    "    search.best_estimator_.named_steps[\"pca\"].n_components,\n",
    "    linestyle=\":\",\n",
    "    label=\"n_components chosen\",\n",
    ")\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "components_col = \"param_pca__n_components\"\n",
    "is_max_test_score = pl.col(\"mean_test_score\") == pl.col(\"mean_test_score\").max()\n",
    "best_clfs = (\n",
    "    pl.LazyFrame(search.cv_results_)\n",
    "    .filter(is_max_test_score.over(components_col))\n",
    "    .unique(components_col)\n",
    "    .sort(components_col)\n",
    "    .collect()\n",
    ")\n",
    "ax1.errorbar(\n",
    "    best_clfs[components_col],\n",
    "    best_clfs[\"mean_test_score\"],\n",
    "    yerr=best_clfs[\"std_test_score\"],\n",
    ")\n",
    "ax1.set_ylabel(\"Classification accuracy (val)\")\n",
    "ax1.set_xlabel(\"n_components\")\n",
    "\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
