{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997cea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Effect of transforming the targets in regression model\n",
    "======================================================\n",
    "\n",
    "In this example, we give an overview of\n",
    ":class:`~sklearn.compose.TransformedTargetRegressor`. We use two examples\n",
    "to illustrate the benefit of transforming the targets before learning a linear\n",
    "regression model. The first example uses synthetic data while the second\n",
    "example is based on the Ames housing data set.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# %%\n",
    "# Synthetic example\n",
    "# #################\n",
    "#\n",
    "# A synthetic random regression dataset is generated. The targets ``y`` are\n",
    "# modified by:\n",
    "#\n",
    "# 1. translating all targets such that all entries are\n",
    "#    non-negative (by adding the absolute value of the lowest ``y``) and\n",
    "# 2. applying an exponential function to obtain non-linear\n",
    "#    targets which cannot be fitted using a simple linear model.\n",
    "#\n",
    "# Therefore, a logarithmic (`np.log1p`) and an exponential function\n",
    "# (`np.expm1`) will be used to transform the targets before training a linear\n",
    "# regression model and using it for prediction.\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=10_000, noise=100, random_state=0)\n",
    "y = np.expm1((y + abs(y.min())) / 200)\n",
    "y_trans = np.log1p(y)\n",
    "\n",
    "# %%\n",
    "# Below we plot the probability density functions of the target\n",
    "# before and after applying the logarithmic functions.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "ax0.hist(y, bins=100, density=True)\n",
    "ax0.set_xlim([0, 2000])\n",
    "ax0.set_ylabel(\"Probability\")\n",
    "ax0.set_xlabel(\"Target\")\n",
    "ax0.set_title(\"Target distribution\")\n",
    "\n",
    "ax1.hist(y_trans, bins=100, density=True)\n",
    "ax1.set_ylabel(\"Probability\")\n",
    "ax1.set_xlabel(\"Target\")\n",
    "ax1.set_title(\"Transformed target distribution\")\n",
    "\n",
    "f.suptitle(\"Synthetic data\", y=1.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# %%\n",
    "# At first, a linear model will be applied on the original targets. Due to the\n",
    "# non-linearity, the model trained will not be precise during\n",
    "# prediction. Subsequently, a logarithmic function is used to linearize the\n",
    "# targets, allowing better prediction even with a similar linear model as\n",
    "# reported by the median absolute error (MedAE).\n",
    "from sklearn.metrics import median_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def compute_score(y_true, y_pred):\n",
    "    return {\n",
    "        \"R2\": f\"{r2_score(y_true, y_pred):.3f}\",\n",
    "        \"MedAE\": f\"{median_absolute_error(y_true, y_pred):.3f}\",\n",
    "    }\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "ridge_cv = RidgeCV().fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_cv.predict(X_test)\n",
    "\n",
    "ridge_cv_with_trans_target = TransformedTargetRegressor(\n",
    "    regressor=RidgeCV(), func=np.log1p, inverse_func=np.expm1\n",
    ").fit(X_train, y_train)\n",
    "y_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    ax=ax0,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge_with_trans_target,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    ax=ax1,\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "\n",
    "# Add the score in the legend of each axis\n",
    "for ax, y_pred in zip([ax0, ax1], [y_pred_ridge, y_pred_ridge_with_trans_target]):\n",
    "    for name, score in compute_score(y_test, y_pred).items():\n",
    "        ax.plot([], [], \" \", label=f\"{name}={score}\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax0.set_title(\"Ridge regression \\n without target transformation\")\n",
    "ax1.set_title(\"Ridge regression \\n with target transformation\")\n",
    "f.suptitle(\"Synthetic data\", y=1.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "# Real-world data set\n",
    "# ###################\n",
    "#\n",
    "# In a similar manner, the Ames housing data set is used to show the impact\n",
    "# of transforming the targets before learning a model. In this example, the\n",
    "# target to be predicted is the selling price of each house.\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import quantile_transform\n",
    "\n",
    "ames = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "# Keep only numeric columns\n",
    "X = ames.data.select_dtypes(np.number)\n",
    "# Remove columns with NaN or Inf values\n",
    "X = X.drop(columns=[\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"])\n",
    "# Let the price be in k$\n",
    "y = ames.target / 1000\n",
    "y_trans = quantile_transform(\n",
    "    y.to_frame(), n_quantiles=900, output_distribution=\"normal\", copy=True\n",
    ").squeeze()\n",
    "\n",
    "# %%\n",
    "# A :class:`~sklearn.preprocessing.QuantileTransformer` is used to normalize\n",
    "# the target distribution before applying a\n",
    "# :class:`~sklearn.linear_model.RidgeCV` model.\n",
    "f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "ax0.hist(y, bins=100, density=True)\n",
    "ax0.set_ylabel(\"Probability\")\n",
    "ax0.set_xlabel(\"Target\")\n",
    "ax0.set_title(\"Target distribution\")\n",
    "\n",
    "ax1.hist(y_trans, bins=100, density=True)\n",
    "ax1.set_ylabel(\"Probability\")\n",
    "ax1.set_xlabel(\"Target\")\n",
    "ax1.set_title(\"Transformed target distribution\")\n",
    "\n",
    "f.suptitle(\"Ames housing data: selling price\", y=1.05)\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# %%\n",
    "# The effect of the transformer is weaker than on the synthetic data. However,\n",
    "# the transformation results in an increase in :math:`R^2` and large decrease\n",
    "# of the MedAE. The residual plot (predicted target - true target vs predicted\n",
    "# target) without target transformation takes on a curved, 'reverse smile'\n",
    "# shape due to residual values that vary depending on the value of predicted\n",
    "# target. With target transformation, the shape is more linear indicating\n",
    "# better model fit.\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(2, 2, sharey=\"row\", figsize=(6.5, 8))\n",
    "\n",
    "ridge_cv = RidgeCV().fit(X_train, y_train)\n",
    "y_pred_ridge = ridge_cv.predict(X_test)\n",
    "\n",
    "ridge_cv_with_trans_target = TransformedTargetRegressor(\n",
    "    regressor=RidgeCV(),\n",
    "    transformer=QuantileTransformer(n_quantiles=900, output_distribution=\"normal\"),\n",
    ").fit(X_train, y_train)\n",
    "y_pred_ridge_with_trans_target = ridge_cv_with_trans_target.predict(X_test)\n",
    "\n",
    "# plot the actual vs predicted values\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    ax=ax0[0],\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge_with_trans_target,\n",
    "    kind=\"actual_vs_predicted\",\n",
    "    ax=ax0[1],\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "\n",
    "# Add the score in the legend of each axis\n",
    "for ax, y_pred in zip([ax0[0], ax0[1]], [y_pred_ridge, y_pred_ridge_with_trans_target]):\n",
    "    for name, score in compute_score(y_test, y_pred).items():\n",
    "        ax.plot([], [], \" \", label=f\"{name}={score}\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "ax0[0].set_title(\"Ridge regression \\n without target transformation\")\n",
    "ax0[1].set_title(\"Ridge regression \\n with target transformation\")\n",
    "\n",
    "# plot the residuals vs the predicted values\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge,\n",
    "    kind=\"residual_vs_predicted\",\n",
    "    ax=ax1[0],\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_test,\n",
    "    y_pred_ridge_with_trans_target,\n",
    "    kind=\"residual_vs_predicted\",\n",
    "    ax=ax1[1],\n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    ")\n",
    "ax1[0].set_title(\"Ridge regression \\n without target transformation\")\n",
    "ax1[1].set_title(\"Ridge regression \\n with target transformation\")\n",
    "\n",
    "f.suptitle(\"Ames housing data: selling price\", y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
