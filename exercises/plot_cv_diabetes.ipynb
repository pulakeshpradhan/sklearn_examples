{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eda114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================\n",
    "Cross-validation on diabetes Dataset Exercise\n",
    "===============================================\n",
    "\n",
    "A tutorial exercise which uses cross-validation with linear models.\n",
    "\n",
    "This exercise is used in the :ref:`cv_estimators_tut` part of the\n",
    ":ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Load dataset and apply GridSearchCV\n",
    "# -----------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = X[:150]\n",
    "y = y[:150]\n",
    "\n",
    "lasso = Lasso(random_state=0, max_iter=10000)\n",
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "\n",
    "tuned_parameters = [{\"alpha\": alphas}]\n",
    "n_folds = 5\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)\n",
    "clf.fit(X, y)\n",
    "scores = clf.cv_results_[\"mean_test_score\"]\n",
    "scores_std = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "# %%\n",
    "# Plot error lines showing +/- std. errors of the scores\n",
    "# ------------------------------------------------------\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "std_error = scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, scores + std_error, \"b--\")\n",
    "plt.semilogx(alphas, scores - std_error, \"b--\")\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel(\"CV score +/- std error\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.axhline(np.max(scores), linestyle=\"--\", color=\".5\")\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "\n",
    "# %%\n",
    "# Bonus: how much can you trust the selection of alpha?\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# To answer this question we use the LassoCV object that sets its alpha\n",
    "# parameter automatically from the data by internal cross-validation (i.e. it\n",
    "# performs cross-validation on the training data it receives).\n",
    "# We use external cross-validation to see how much the automatically obtained\n",
    "# alphas differ across different cross-validation folds.\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lasso_cv = LassoCV(alphas=alphas, random_state=0, max_iter=10000)\n",
    "k_fold = KFold(3)\n",
    "\n",
    "print(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\n",
    "print()\n",
    "print(\"Alpha parameters maximising the generalization score on different\")\n",
    "print(\"subsets of the data:\")\n",
    "for k, (train, test) in enumerate(k_fold.split(X, y)):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\n",
    "        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n",
    "            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n",
    "        )\n",
    "    )\n",
    "print()\n",
    "print(\"Answer: Not very much since we obtained different alphas for different\")\n",
    "print(\"subsets of the data and moreover, the scores for these alphas differ\")\n",
    "print(\"quite substantially.\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
