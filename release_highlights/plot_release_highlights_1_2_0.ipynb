{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbdf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "=======================================\n",
    "Release Highlights for scikit-learn 1.2\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 1.2! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_1_2>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Pandas output with `set_output` API\n",
    "# -----------------------------------\n",
    "# scikit-learn's transformers now support pandas output with the `set_output` API.\n",
    "# To learn more about the `set_output` API see the example:\n",
    "# :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` and\n",
    "# # this `video, pandas DataFrame output for scikit-learn transformers\n",
    "# (some examples) <https://youtu.be/5bCg8VfX2x8>`__.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "sepal_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
    "petal_cols = [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler(), sepal_cols),\n",
    "        (\"kbin\", KBinsDiscretizer(encode=\"ordinal\"), petal_cols),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "X_out = preprocessor.fit_transform(X)\n",
    "X_out.sample(n=5, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Interaction constraints in Histogram-based Gradient Boosting Trees\n",
    "# ------------------------------------------------------------------\n",
    "# :class:`~ensemble.HistGradientBoostingRegressor` and\n",
    "# :class:`~ensemble.HistGradientBoostingClassifier` now supports interaction constraints\n",
    "# with the `interaction_cst` parameter. For details, see the\n",
    "# :ref:`User Guide <interaction_cst_hgbt>`. In the following example, features are not\n",
    "# allowed to interact.\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "hist_no_interact = HistGradientBoostingRegressor(\n",
    "    interaction_cst=[[i] for i in range(X.shape[1])], random_state=0\n",
    ")\n",
    "hist_no_interact.fit(X, y)\n",
    "\n",
    "# %%\n",
    "# New and enhanced displays\n",
    "# -------------------------\n",
    "# :class:`~metrics.PredictionErrorDisplay` provides a way to analyze regression\n",
    "# models in a qualitative manner.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "_ = PredictionErrorDisplay.from_estimator(\n",
    "    hist_no_interact, X, y, kind=\"actual_vs_predicted\", ax=axs[0]\n",
    ")\n",
    "_ = PredictionErrorDisplay.from_estimator(\n",
    "    hist_no_interact, X, y, kind=\"residual_vs_predicted\", ax=axs[1]\n",
    ")\n",
    "\n",
    "# %%\n",
    "# :class:`~model_selection.LearningCurveDisplay` is now available to plot\n",
    "# results from :func:`~model_selection.learning_curve`.\n",
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "\n",
    "_ = LearningCurveDisplay.from_estimator(\n",
    "    hist_no_interact, X, y, cv=5, n_jobs=2, train_sizes=np.linspace(0.1, 1, 5)\n",
    ")\n",
    "\n",
    "# %%\n",
    "# :class:`~inspection.PartialDependenceDisplay` exposes a new parameter\n",
    "# `categorical_features` to display partial dependence for categorical features\n",
    "# using bar plots and heatmaps.\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n",
    ")\n",
    "X = X.select_dtypes([\"number\", \"category\"]).drop(columns=[\"body\"])\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "categorical_features = [\"pclass\", \"sex\", \"embarked\"]\n",
    "model = make_pipeline(\n",
    "    ColumnTransformer(\n",
    "        transformers=[(\"cat\", OrdinalEncoder(), categorical_features)],\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    HistGradientBoostingRegressor(random_state=0),\n",
    ").fit(X, y)\n",
    "\n",
    "# %%\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4), constrained_layout=True)\n",
    "_ = PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X,\n",
    "    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n",
    "    categorical_features=categorical_features,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Faster parser in :func:`~datasets.fetch_openml`\n",
    "# -----------------------------------------------\n",
    "# :func:`~datasets.fetch_openml` now supports a new `\"pandas\"` parser that is\n",
    "# more memory and CPU efficient. In v1.4, the default will change to\n",
    "# `parser=\"auto\"` which will automatically use the `\"pandas\"` parser for dense\n",
    "# data and `\"liac-arff\"` for sparse data.\n",
    "X, y = fetch_openml(\n",
    "    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n",
    ")\n",
    "X.head()\n",
    "\n",
    "# %%\n",
    "# Experimental Array API support in :class:`~discriminant_analysis.LinearDiscriminantAnalysis`\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# Experimental support for the `Array API <https://data-apis.org/array-api/latest/>`_\n",
    "# specification was added to :class:`~discriminant_analysis.LinearDiscriminantAnalysis`.\n",
    "# The estimator can now run on any Array API compliant libraries such as\n",
    "# `CuPy <https://docs.cupy.dev/en/stable/overview.html>`__, a GPU-accelerated array\n",
    "# library. For details, see the :ref:`User Guide <array_api>`.\n",
    "\n",
    "# %%\n",
    "# Improved efficiency of many estimators\n",
    "# --------------------------------------\n",
    "# In version 1.1 the efficiency of many estimators relying on the computation of\n",
    "# pairwise distances (essentially estimators related to clustering, manifold\n",
    "# learning and neighbors search algorithms) was greatly improved for float64\n",
    "# dense input. Efficiency improvement especially were a reduced memory footprint\n",
    "# and a much better scalability on multi-core machines.\n",
    "# In version 1.2, the efficiency of these estimators was further improved for all\n",
    "# combinations of dense and sparse inputs on float32 and float64 datasets, except\n",
    "# the sparse-dense and dense-sparse combinations for the Euclidean and Squared\n",
    "# Euclidean Distance metrics.\n",
    "# A detailed list of the impacted estimators can be found in the\n",
    "# :ref:`changelog <release_notes_1_2>`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
