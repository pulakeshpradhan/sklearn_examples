{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e549a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "=======================================\n",
    "Release Highlights for scikit-learn 1.1\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 1.1! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_1_1>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# .. _quantile_support_hgbdt:\n",
    "#\n",
    "# Quantile loss in :class:`~ensemble.HistGradientBoostingRegressor`\n",
    "# -----------------------------------------------------------------\n",
    "# :class:`~ensemble.HistGradientBoostingRegressor` can model quantiles with\n",
    "# `loss=\"quantile\"` and the new parameter `quantile`.\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple regression function for X * cos(X)\n",
    "rng = np.random.RandomState(42)\n",
    "X_1d = np.linspace(0, 10, num=2000)\n",
    "X = X_1d.reshape(-1, 1)\n",
    "y = X_1d * np.cos(X_1d) + rng.normal(scale=X_1d / 3)\n",
    "\n",
    "quantiles = [0.95, 0.5, 0.05]\n",
    "parameters = dict(loss=\"quantile\", max_bins=32, max_iter=50)\n",
    "hist_quantiles = {\n",
    "    f\"quantile={quantile:.2f}\": HistGradientBoostingRegressor(\n",
    "        **parameters, quantile=quantile\n",
    "    ).fit(X, y)\n",
    "    for quantile in quantiles\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_1d, y, \"o\", alpha=0.5, markersize=1)\n",
    "for quantile, hist in hist_quantiles.items():\n",
    "    ax.plot(X_1d, hist.predict(X), label=quantile)\n",
    "_ = ax.legend(loc=\"lower left\")\n",
    "\n",
    "# %%\n",
    "# For a usecase example, see\n",
    "# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\n",
    "\n",
    "# %%\n",
    "# `get_feature_names_out` Available in all Transformers\n",
    "# -----------------------------------------------------\n",
    "# :term:`get_feature_names_out` is now available in all Transformers. This enables\n",
    "# :class:`~pipeline.Pipeline` to construct the output feature names for more complex\n",
    "# pipelines:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n",
    ")\n",
    "numeric_features = [\"age\", \"fare\"]\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "categorical_features = [\"embarked\", \"pclass\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            categorical_features,\n",
    "        ),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "log_reg = make_pipeline(preprocessor, SelectKBest(k=7), LogisticRegression())\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Here we slice the pipeline to include all the steps but the last one. The output\n",
    "# feature names of this pipeline slice are the features put into logistic\n",
    "# regression. These names correspond directly to the coefficients in the logistic\n",
    "# regression:\n",
    "import pandas as pd\n",
    "\n",
    "log_reg_input_features = log_reg[:-1].get_feature_names_out()\n",
    "pd.Series(log_reg[-1].coef_.ravel(), index=log_reg_input_features).plot.bar()\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# %%\n",
    "# Grouping infrequent categories in :class:`~preprocessing.OneHotEncoder`\n",
    "# -----------------------------------------------------------------------\n",
    "# :class:`~preprocessing.OneHotEncoder` supports aggregating infrequent\n",
    "# categories into a single output for each feature. The parameters to enable\n",
    "# the gathering of infrequent categories are `min_frequency` and\n",
    "# `max_categories`. See the :ref:`User Guide <encoder_infrequent_categories>`\n",
    "# for more details.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(\n",
    "    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\n",
    ").T\n",
    "enc = OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\n",
    "enc.infrequent_categories_\n",
    "\n",
    "# %%\n",
    "# Since dog and snake are infrequent categories, they are grouped together when\n",
    "# transformed:\n",
    "encoded = enc.transform(np.array([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n",
    "pd.DataFrame(encoded, columns=enc.get_feature_names_out())\n",
    "\n",
    "# %%\n",
    "# Performance improvements\n",
    "# ------------------------\n",
    "# Reductions on pairwise distances for dense float64 datasets has been refactored\n",
    "# to better take advantage of non-blocking thread parallelism. For example,\n",
    "# :meth:`neighbors.NearestNeighbors.kneighbors` and\n",
    "# :meth:`neighbors.NearestNeighbors.radius_neighbors` can respectively be up to ×20 and\n",
    "# ×5 faster than previously. In summary, the following functions and estimators\n",
    "# now benefit from improved performance:\n",
    "#\n",
    "# - :func:`metrics.pairwise_distances_argmin`\n",
    "# - :func:`metrics.pairwise_distances_argmin_min`\n",
    "# - :class:`cluster.AffinityPropagation`\n",
    "# - :class:`cluster.Birch`\n",
    "# - :class:`cluster.MeanShift`\n",
    "# - :class:`cluster.OPTICS`\n",
    "# - :class:`cluster.SpectralClustering`\n",
    "# - :func:`feature_selection.mutual_info_regression`\n",
    "# - :class:`neighbors.KNeighborsClassifier`\n",
    "# - :class:`neighbors.KNeighborsRegressor`\n",
    "# - :class:`neighbors.RadiusNeighborsClassifier`\n",
    "# - :class:`neighbors.RadiusNeighborsRegressor`\n",
    "# - :class:`neighbors.LocalOutlierFactor`\n",
    "# - :class:`neighbors.NearestNeighbors`\n",
    "# - :class:`manifold.Isomap`\n",
    "# - :class:`manifold.LocallyLinearEmbedding`\n",
    "# - :class:`manifold.TSNE`\n",
    "# - :func:`manifold.trustworthiness`\n",
    "# - :class:`semi_supervised.LabelPropagation`\n",
    "# - :class:`semi_supervised.LabelSpreading`\n",
    "#\n",
    "# To know more about the technical details of this work, you can read\n",
    "# `this suite of blog posts <https://blog.scikit-learn.org/technical/performances/>`_.\n",
    "#\n",
    "# Moreover, the computation of loss functions has been refactored using\n",
    "# Cython resulting in performance improvements for the following estimators:\n",
    "#\n",
    "# - :class:`linear_model.LogisticRegression`\n",
    "# - :class:`linear_model.GammaRegressor`\n",
    "# - :class:`linear_model.PoissonRegressor`\n",
    "# - :class:`linear_model.TweedieRegressor`\n",
    "\n",
    "# %%\n",
    "# :class:`~decomposition.MiniBatchNMF`: an online version of NMF\n",
    "# --------------------------------------------------------------\n",
    "# The new class :class:`~decomposition.MiniBatchNMF` implements a faster but\n",
    "# less accurate version of non-negative matrix factorization\n",
    "# (:class:`~decomposition.NMF`). :class:`~decomposition.MiniBatchNMF` divides the\n",
    "# data into mini-batches and optimizes the NMF model in an online manner by\n",
    "# cycling over the mini-batches, making it better suited for large datasets. In\n",
    "# particular, it implements `partial_fit`, which can be used for online\n",
    "# learning when the data is not readily available from the start, or when the\n",
    "# data does not fit into memory.\n",
    "import numpy as np\n",
    "from sklearn.decomposition import MiniBatchNMF\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "n_samples, n_features, n_components = 10, 10, 5\n",
    "true_W = rng.uniform(size=(n_samples, n_components))\n",
    "true_H = rng.uniform(size=(n_components, n_features))\n",
    "X = true_W @ true_H\n",
    "\n",
    "nmf = MiniBatchNMF(n_components=n_components, random_state=0)\n",
    "\n",
    "for _ in range(10):\n",
    "    nmf.partial_fit(X)\n",
    "\n",
    "W = nmf.transform(X)\n",
    "H = nmf.components_\n",
    "X_reconstructed = W @ H\n",
    "\n",
    "print(\n",
    "    f\"relative reconstruction error: \",\n",
    "    f\"{np.sum((X - X_reconstructed) ** 2) / np.sum(X**2):.5f}\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# :class:`~cluster.BisectingKMeans`: divide and cluster\n",
    "# -----------------------------------------------------\n",
    "# The new class :class:`~cluster.BisectingKMeans` is a variant of\n",
    "# :class:`~cluster.KMeans`, using divisive hierarchical clustering. Instead of\n",
    "# creating all centroids at once, centroids are picked progressively based on a\n",
    "# previous clustering: a cluster is split into two new clusters repeatedly\n",
    "# until the target number of clusters is reached, giving a hierarchical\n",
    "# structure to the clustering.\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, BisectingKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=1000, centers=2, random_state=0)\n",
    "\n",
    "km = KMeans(n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\n",
    "bisect_km = BisectingKMeans(n_clusters=5, random_state=0).fit(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\n",
    "ax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\n",
    "ax[0].set_title(\"KMeans\")\n",
    "\n",
    "ax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\n",
    "ax[1].scatter(\n",
    "    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n",
    ")\n",
    "_ = ax[1].set_title(\"BisectingKMeans\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
