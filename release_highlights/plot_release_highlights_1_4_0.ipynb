{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ce30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "=======================================\n",
    "Release Highlights for scikit-learn 1.4\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 1.4! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_1_4>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# HistGradientBoosting Natively Supports Categorical DTypes in DataFrames\n",
    "# -----------------------------------------------------------------------\n",
    "# :class:`ensemble.HistGradientBoostingClassifier` and\n",
    "# :class:`ensemble.HistGradientBoostingRegressor` now directly supports dataframes with\n",
    "# categorical features.  Here we have a dataset with a mixture of\n",
    "# categorical and numerical features:\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X_adult, y_adult = fetch_openml(\"adult\", version=2, return_X_y=True)\n",
    "\n",
    "# Remove redundant and non-feature columns\n",
    "X_adult = X_adult.drop([\"education-num\", \"fnlwgt\"], axis=\"columns\")\n",
    "X_adult.dtypes\n",
    "\n",
    "# %%\n",
    "# By setting `categorical_features=\"from_dtype\"`, the gradient boosting classifier\n",
    "# treats the columns with categorical dtypes as categorical features in the\n",
    "# algorithm:\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_adult, y_adult, random_state=0)\n",
    "hist = HistGradientBoostingClassifier(categorical_features=\"from_dtype\")\n",
    "\n",
    "hist.fit(X_train, y_train)\n",
    "y_decision = hist.decision_function(X_test)\n",
    "print(f\"ROC AUC score is {roc_auc_score(y_test, y_decision)}\")\n",
    "\n",
    "# %%\n",
    "# Polars output in `set_output`\n",
    "# -----------------------------\n",
    "# scikit-learn's transformers now support polars output with the `set_output` API.\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df = pl.DataFrame(\n",
    "    {\"height\": [120, 140, 150, 110, 100], \"pet\": [\"dog\", \"cat\", \"dog\", \"cat\", \"cat\"]}\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical\", StandardScaler(), [\"height\"]),\n",
    "        (\"categorical\", OneHotEncoder(sparse_output=False), [\"pet\"]),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "preprocessor.set_output(transform=\"polars\")\n",
    "\n",
    "df_out = preprocessor.fit_transform(df)\n",
    "df_out\n",
    "\n",
    "# %%\n",
    "print(f\"Output type: {type(df_out)}\")\n",
    "\n",
    "# %%\n",
    "# Missing value support for Random Forest\n",
    "# ---------------------------------------\n",
    "# The classes :class:`ensemble.RandomForestClassifier` and\n",
    "# :class:`ensemble.RandomForestRegressor` now support missing values. When training\n",
    "# every individual tree, the splitter evaluates each potential threshold with the\n",
    "# missing values going to the left and right nodes. More details in the\n",
    "# :ref:`User Guide <tree_missing_value_support>`.\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0).fit(X, y)\n",
    "forest.predict(X)\n",
    "\n",
    "# %%\n",
    "# Add support for monotonic constraints in tree-based models\n",
    "# ----------------------------------------------------------\n",
    "# While we added support for monotonic constraints in histogram-based gradient boosting\n",
    "# in scikit-learn 0.23, we now support this feature for all other tree-based models as\n",
    "# trees, random forests, extra-trees, and exact gradient boosting. Here, we show this\n",
    "# feature for random forest on a regression problem.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "n_samples = 500\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, 2)\n",
    "noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n",
    "y = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise\n",
    "\n",
    "rf_no_cst = RandomForestRegressor().fit(X, y)\n",
    "rf_cst = RandomForestRegressor(monotonic_cst=[1, 0]).fit(X, y)\n",
    "\n",
    "disp = PartialDependenceDisplay.from_estimator(\n",
    "    rf_no_cst,\n",
    "    X,\n",
    "    features=[0],\n",
    "    feature_names=[\"feature 0\"],\n",
    "    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n",
    ")\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    rf_cst,\n",
    "    X,\n",
    "    features=[0],\n",
    "    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n",
    "    ax=disp.axes_,\n",
    ")\n",
    "disp.axes_[0, 0].plot(\n",
    "    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n",
    ")\n",
    "disp.axes_[0, 0].set_ylim(-3, 3)\n",
    "disp.axes_[0, 0].set_xlim(-1, 1)\n",
    "disp.axes_[0, 0].legend()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Enriched estimator displays\n",
    "# ---------------------------\n",
    "# Estimators displays have been enriched: if we look at `forest`, defined above:\n",
    "forest\n",
    "\n",
    "# %%\n",
    "# One can access the documentation of the estimator by clicking on the icon \"?\" on\n",
    "# the top right corner of the diagram.\n",
    "#\n",
    "# In addition, the display changes color, from orange to blue, when the estimator is\n",
    "# fitted. You can also get this information by hovering on the icon \"i\".\n",
    "from sklearn.base import clone\n",
    "\n",
    "clone(forest)  # the clone is not fitted\n",
    "\n",
    "# %%\n",
    "# Metadata Routing Support\n",
    "# ------------------------\n",
    "# Many meta-estimators and cross-validation routines now support metadata\n",
    "# routing, which are listed in the :ref:`user guide\n",
    "# <metadata_routing_models>`. For instance, this is how you can do a nested\n",
    "# cross-validation with sample weights and :class:`~model_selection.GroupKFold`:\n",
    "import sklearn\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, GroupKFold\n",
    "\n",
    "# For now by default metadata routing is disabled, and need to be explicitly\n",
    "# enabled.\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "\n",
    "n_samples = 100\n",
    "X, y = make_regression(n_samples=n_samples, n_features=5, noise=0.5)\n",
    "rng = np.random.RandomState(7)\n",
    "groups = rng.randint(0, 10, size=n_samples)\n",
    "sample_weights = rng.rand(n_samples)\n",
    "estimator = Lasso().set_fit_request(sample_weight=True)\n",
    "hyperparameter_grid = {\"alpha\": [0.1, 0.5, 1.0, 2.0]}\n",
    "scoring_inner_cv = get_scorer(\"neg_mean_squared_error\").set_score_request(\n",
    "    sample_weight=True\n",
    ")\n",
    "inner_cv = GroupKFold(n_splits=5)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=hyperparameter_grid,\n",
    "    cv=inner_cv,\n",
    "    scoring=scoring_inner_cv,\n",
    ")\n",
    "\n",
    "outer_cv = GroupKFold(n_splits=5)\n",
    "scorers = {\n",
    "    \"mse\": get_scorer(\"neg_mean_squared_error\").set_score_request(sample_weight=True)\n",
    "}\n",
    "results = cross_validate(\n",
    "    grid_search,\n",
    "    X,\n",
    "    y,\n",
    "    cv=outer_cv,\n",
    "    scoring=scorers,\n",
    "    return_estimator=True,\n",
    "    params={\"sample_weight\": sample_weights, \"groups\": groups},\n",
    ")\n",
    "print(\"cv error on test sets:\", results[\"test_mse\"])\n",
    "\n",
    "# Setting the flag to the default `False` to avoid interference with other\n",
    "# scripts.\n",
    "sklearn.set_config(enable_metadata_routing=False)\n",
    "\n",
    "# %%\n",
    "# Improved memory and runtime efficiency for PCA on sparse data\n",
    "# -------------------------------------------------------------\n",
    "# PCA is now able to handle sparse matrices natively for the `arpack`\n",
    "# solver by levaraging `scipy.sparse.linalg.LinearOperator` to avoid\n",
    "# materializing large sparse matrices when performing the\n",
    "# eigenvalue decomposition of the data set covariance matrix.\n",
    "#\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "\n",
    "X_sparse = sp.random(m=1000, n=1000, random_state=0)\n",
    "X_dense = X_sparse.toarray()\n",
    "\n",
    "t0 = time()\n",
    "PCA(n_components=10, svd_solver=\"arpack\").fit(X_sparse)\n",
    "time_sparse = time() - t0\n",
    "\n",
    "t0 = time()\n",
    "PCA(n_components=10, svd_solver=\"arpack\").fit(X_dense)\n",
    "time_dense = time() - t0\n",
    "\n",
    "print(f\"Speedup: {time_dense / time_sparse:.1f}x\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
