{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b14c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================\n",
    "Release Highlights for scikit-learn 0.22\n",
    "========================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 0.22, which comes\n",
    "with many bug fixes and new features! We detail below a few of the major\n",
    "features of this release. For an exhaustive list of all the changes, please\n",
    "refer to the :ref:`release notes <release_notes_0_22>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# New plotting API\n",
    "# ----------------\n",
    "#\n",
    "# A new plotting API is available for creating visualizations. This new API\n",
    "# allows for quickly adjusting the visuals of a plot without involving any\n",
    "# recomputation. It is also possible to add different plots to the same\n",
    "# figure. The following example illustrates `plot_roc_curve`,\n",
    "# but other plots utilities are supported like\n",
    "# `plot_partial_dependence`,\n",
    "# `plot_precision_recall_curve`, and\n",
    "# `plot_confusion_matrix`. Read more about this new API in the\n",
    "# :ref:`User Guide <visualizations>`.\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.fixes import parse_version\n",
    "\n",
    "X, y = make_classification(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# plot_roc_curve has been removed in version 1.2. From 1.2, use RocCurveDisplay instead.\n",
    "# svc_disp = plot_roc_curve(svc, X_test, y_test)\n",
    "# rfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\n",
    "svc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\n",
    "rfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=svc_disp.ax_)\n",
    "rfc_disp.figure_.suptitle(\"ROC curve comparison\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Stacking Classifier and Regressor\n",
    "# ---------------------------------\n",
    "# :class:`~ensemble.StackingClassifier` and\n",
    "# :class:`~ensemble.StackingRegressor`\n",
    "# allow you to have a stack of estimators with a final classifier or\n",
    "# a regressor.\n",
    "# Stacked generalization consists in stacking the output of individual\n",
    "# estimators and use a classifier to compute the final prediction. Stacking\n",
    "# allows to use the strength of each individual estimator by using their output\n",
    "# as input of a final estimator.\n",
    "# Base estimators are fitted on the full ``X`` while\n",
    "# the final estimator is trained using cross-validated predictions of the\n",
    "# base estimators using ``cross_val_predict``.\n",
    "#\n",
    "# Read more in the :ref:`User Guide <stacking>`.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "estimators = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    (\"svr\", make_pipeline(StandardScaler(), LinearSVC(dual=\"auto\", random_state=42))),\n",
    "]\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "# %%\n",
    "# Permutation-based feature importance\n",
    "# ------------------------------------\n",
    "#\n",
    "# The :func:`inspection.permutation_importance` can be used to get an\n",
    "# estimate of the importance of each feature, for any fitted estimator:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X, y = make_classification(random_state=0, n_features=5, n_informative=3)\n",
    "feature_names = np.array([f\"x_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0).fit(X, y)\n",
    "result = permutation_importance(rf, X, y, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\n",
    "# renamed to `tick_labels`. The following code handles this, but as a\n",
    "# scikit-learn user you probably can write simpler code by using `labels=...`\n",
    "# (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\n",
    "tick_labels_parameter_name = (\n",
    "    \"tick_labels\"\n",
    "    if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\n",
    "    else \"labels\"\n",
    ")\n",
    "tick_labels_dict = {tick_labels_parameter_name: feature_names[sorted_idx]}\n",
    "ax.boxplot(result.importances[sorted_idx].T, vert=False, **tick_labels_dict)\n",
    "ax.set_title(\"Permutation Importance of each feature\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Native support for missing values for gradient boosting\n",
    "# -------------------------------------------------------\n",
    "#\n",
    "# The :class:`ensemble.HistGradientBoostingClassifier`\n",
    "# and :class:`ensemble.HistGradientBoostingRegressor` now have native\n",
    "# support for missing values (NaNs). This means that there is no need for\n",
    "# imputing data when training or predicting.\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\n",
    "print(gbdt.predict(X))\n",
    "\n",
    "# %%\n",
    "# Precomputed sparse nearest neighbors graph\n",
    "# ------------------------------------------\n",
    "# Most estimators based on nearest neighbors graphs now accept precomputed\n",
    "# sparse graphs as input, to reuse the same graph for multiple estimator fits.\n",
    "# To use this feature in a pipeline, one can use the `memory` parameter, along\n",
    "# with one of the two new transformers,\n",
    "# :class:`neighbors.KNeighborsTransformer` and\n",
    "# :class:`neighbors.RadiusNeighborsTransformer`. The precomputation\n",
    "# can also be performed by custom estimators to use alternative\n",
    "# implementations, such as approximate nearest neighbors methods.\n",
    "# See more details in the :ref:`User Guide <neighbors_transformer>`.\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X, y = make_classification(random_state=0)\n",
    "\n",
    "with TemporaryDirectory(prefix=\"sklearn_cache_\") as tmpdir:\n",
    "    estimator = make_pipeline(\n",
    "        KNeighborsTransformer(n_neighbors=10, mode=\"distance\"),\n",
    "        Isomap(n_neighbors=10, metric=\"precomputed\"),\n",
    "        memory=tmpdir,\n",
    "    )\n",
    "    estimator.fit(X)\n",
    "\n",
    "    # We can decrease the number of neighbors and the graph will not be\n",
    "    # recomputed.\n",
    "    estimator.set_params(isomap__n_neighbors=5)\n",
    "    estimator.fit(X)\n",
    "\n",
    "# %%\n",
    "# KNN Based Imputation\n",
    "# ------------------------------------\n",
    "# We now support imputation for completing missing values using k-Nearest\n",
    "# Neighbors.\n",
    "#\n",
    "# Each sample's missing values are imputed using the mean value from\n",
    "# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n",
    "# close if the features that neither is missing are close.\n",
    "# By default, a euclidean distance metric\n",
    "# that supports missing values,\n",
    "# :func:`~sklearn.metrics.pairwise.nan_euclidean_distances`, is used to find the nearest\n",
    "# neighbors.\n",
    "#\n",
    "# Read more in the :ref:`User Guide <knnimpute>`.\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "X = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "print(imputer.fit_transform(X))\n",
    "\n",
    "# %%\n",
    "# Tree pruning\n",
    "# ------------\n",
    "#\n",
    "# It is now possible to prune most tree-based estimators once the trees are\n",
    "# built. The pruning is based on minimal cost-complexity. Read more in the\n",
    "# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n",
    "\n",
    "X, y = make_classification(random_state=0)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\n",
    "print(\n",
    "    \"Average number of nodes without pruning {:.1f}\".format(\n",
    "        np.mean([e.tree_.node_count for e in rf.estimators_])\n",
    "    )\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\n",
    "print(\n",
    "    \"Average number of nodes with pruning {:.1f}\".format(\n",
    "        np.mean([e.tree_.node_count for e in rf.estimators_])\n",
    "    )\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Retrieve dataframes from OpenML\n",
    "# -------------------------------\n",
    "# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n",
    "# properly handle datasets with heterogeneous data:\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "titanic = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"pandas\")\n",
    "print(titanic.data.head()[[\"pclass\", \"embarked\"]])\n",
    "\n",
    "# %%\n",
    "# Checking scikit-learn compatibility of an estimator\n",
    "# ---------------------------------------------------\n",
    "# Developers can check the compatibility of their scikit-learn compatible\n",
    "# estimators using :func:`~utils.estimator_checks.check_estimator`. For\n",
    "# instance, the ``check_estimator(LinearSVC())`` passes.\n",
    "#\n",
    "# We now provide a ``pytest`` specific decorator which allows ``pytest``\n",
    "# to run all checks independently and report the checks that are failing.\n",
    "#\n",
    "# ..note::\n",
    "#   This entry was slightly updated in version 0.24, where passing classes\n",
    "#   isn't supported anymore: pass instances instead.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils.estimator_checks import parametrize_with_checks\n",
    "\n",
    "\n",
    "@parametrize_with_checks([LogisticRegression(), DecisionTreeRegressor()])\n",
    "def test_sklearn_compatible_estimator(estimator, check):\n",
    "    check(estimator)\n",
    "\n",
    "\n",
    "# %%\n",
    "# ROC AUC now supports multiclass classification\n",
    "# ----------------------------------------------\n",
    "# The :func:`~sklearn.metrics.roc_auc_score` function can also be used in multi-class\n",
    "# classification. Two averaging strategies are currently supported: the\n",
    "# one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\n",
    "# the one-vs-rest algorithm computes the average of the ROC AUC scores for each\n",
    "# class against all other classes. In both cases, the multiclass ROC AUC scores\n",
    "# are computed from the probability estimates that a sample belongs to a\n",
    "# particular class according to the model. The OvO and OvR algorithms support\n",
    "# weighting uniformly (``average='macro'``) and weighting by the prevalence\n",
    "# (``average='weighted'``).\n",
    "#\n",
    "# Read more in the :ref:`User Guide <roc_metrics>`.\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_classification(n_classes=4, n_informative=16)\n",
    "clf = SVC(decision_function_shape=\"ovo\", probability=True).fit(X, y)\n",
    "print(roc_auc_score(y, clf.predict_proba(X), multi_class=\"ovo\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
