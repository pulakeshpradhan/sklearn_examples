{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "========================================\n",
    "Release Highlights for scikit-learn 0.23\n",
    "========================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 0.23! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_0_23>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################\n",
    "# Generalized Linear Models, and Poisson loss for gradient boosting\n",
    "# -----------------------------------------------------------------\n",
    "# Long-awaited Generalized Linear Models with non-normal loss functions are now\n",
    "# available. In particular, three new regressors were implemented:\n",
    "# :class:`~sklearn.linear_model.PoissonRegressor`,\n",
    "# :class:`~sklearn.linear_model.GammaRegressor`, and\n",
    "# :class:`~sklearn.linear_model.TweedieRegressor`. The Poisson regressor can be\n",
    "# used to model positive integer counts, or relative frequencies. Read more in\n",
    "# the :ref:`User Guide <Generalized_linear_regression>`. Additionally,\n",
    "# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` supports a new\n",
    "# 'poisson' loss as well.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# positive integer target correlated with X[:, 5] with many zeros:\n",
    "y = rng.poisson(lam=np.exp(X[:, 5]) / 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "glm = PoissonRegressor()\n",
    "gbdt = HistGradientBoostingRegressor(loss=\"poisson\", learning_rate=0.01)\n",
    "glm.fit(X_train, y_train)\n",
    "gbdt.fit(X_train, y_train)\n",
    "print(glm.score(X_test, y_test))\n",
    "print(gbdt.score(X_test, y_test))\n",
    "\n",
    "##############################################################################\n",
    "# Rich visual representation of estimators\n",
    "# -----------------------------------------\n",
    "# Estimators can now be visualized in notebooks by enabling the\n",
    "# `display='diagram'` option. This is particularly useful to summarise the\n",
    "# structure of pipelines and other composite estimators, with interactivity to\n",
    "# provide detail.  Click on the example image below to expand Pipeline\n",
    "# elements.  See :ref:`visualizing_composite_estimators` for how you can use\n",
    "# this feature.\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "num_proc = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "cat_proc = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (num_proc, (\"feat1\", \"feat3\")), (cat_proc, (\"feat0\", \"feat2\"))\n",
    ")\n",
    "\n",
    "clf = make_pipeline(preprocessor, LogisticRegression())\n",
    "clf\n",
    "\n",
    "##############################################################################\n",
    "# Scalability and stability improvements to KMeans\n",
    "# ------------------------------------------------\n",
    "# The :class:`~sklearn.cluster.KMeans` estimator was entirely re-worked, and it\n",
    "# is now significantly faster and more stable. In addition, the Elkan algorithm\n",
    "# is now compatible with sparse matrices. The estimator uses OpenMP based\n",
    "# parallelism instead of relying on joblib, so the `n_jobs` parameter has no\n",
    "# effect anymore. For more details on how to control the number of threads,\n",
    "# please refer to our :ref:`parallelism` notes.\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import completeness_score\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_blobs(random_state=rng)\n",
    "X = scipy.sparse.csr_matrix(X)\n",
    "X_train, X_test, _, y_test = train_test_split(X, y, random_state=rng)\n",
    "kmeans = KMeans(n_init=\"auto\").fit(X_train)\n",
    "print(completeness_score(kmeans.predict(X_test), y_test))\n",
    "\n",
    "##############################################################################\n",
    "# Improvements to the histogram-based Gradient Boosting estimators\n",
    "# ----------------------------------------------------------------\n",
    "# Various improvements were made to\n",
    "# :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and\n",
    "# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. On top of the\n",
    "# Poisson loss mentioned above, these estimators now support :ref:`sample\n",
    "# weights <sw_hgbdt>`. Also, an automatic early-stopping criterion was added:\n",
    "# early-stopping is enabled by default when the number of samples exceeds 10k.\n",
    "# Finally, users can now define :ref:`monotonic constraints\n",
    "# <monotonic_cst_gbdt>` to constrain the predictions based on the variations of\n",
    "# specific features. In the following example, we construct a target that is\n",
    "# generally positively correlated with the first feature, with some noise.\n",
    "# Applying monotoinc constraints allows the prediction to capture the global\n",
    "# effect of the first feature, instead of fitting the noise. For a usecase\n",
    "# example, see :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples = 500\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.randn(n_samples, 2)\n",
    "noise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n",
    "y = 5 * X[:, 0] + np.sin(10 * np.pi * X[:, 0]) - noise\n",
    "\n",
    "gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)\n",
    "gbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)\n",
    "\n",
    "# plot_partial_dependence has been removed in version 1.2. From 1.2, use\n",
    "# PartialDependenceDisplay instead.\n",
    "# disp = plot_partial_dependence(\n",
    "disp = PartialDependenceDisplay.from_estimator(\n",
    "    gbdt_no_cst,\n",
    "    X,\n",
    "    features=[0],\n",
    "    feature_names=[\"feature 0\"],\n",
    "    line_kw={\"linewidth\": 4, \"label\": \"unconstrained\", \"color\": \"tab:blue\"},\n",
    ")\n",
    "# plot_partial_dependence(\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    gbdt_cst,\n",
    "    X,\n",
    "    features=[0],\n",
    "    line_kw={\"linewidth\": 4, \"label\": \"constrained\", \"color\": \"tab:orange\"},\n",
    "    ax=disp.axes_,\n",
    ")\n",
    "disp.axes_[0, 0].plot(\n",
    "    X[:, 0], y, \"o\", alpha=0.5, zorder=-1, label=\"samples\", color=\"tab:green\"\n",
    ")\n",
    "disp.axes_[0, 0].set_ylim(-3, 3)\n",
    "disp.axes_[0, 0].set_xlim(-1, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "##############################################################################\n",
    "# Sample-weight support for Lasso and ElasticNet\n",
    "# ----------------------------------------------\n",
    "# The two linear regressors :class:`~sklearn.linear_model.Lasso` and\n",
    "# :class:`~sklearn.linear_model.ElasticNet` now support sample weights.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "n_samples, n_features = 1000, 20\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_regression(n_samples, n_features, random_state=rng)\n",
    "sample_weight = rng.rand(n_samples)\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(\n",
    "    X, y, sample_weight, random_state=rng\n",
    ")\n",
    "reg = Lasso()\n",
    "reg.fit(X_train, y_train, sample_weight=sw_train)\n",
    "print(reg.score(X_test, y_test, sw_test))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
