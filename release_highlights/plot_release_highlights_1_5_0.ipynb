{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "=======================================\n",
    "Release Highlights for scikit-learn 1.5\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 1.5! Many bug fixes\n",
    "and improvements were added, as well as some key new features. Below we\n",
    "detail the highlights of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_1_5>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# FixedThresholdClassifier: Setting the decision threshold of a binary classifier\n",
    "# -------------------------------------------------------------------------------\n",
    "# All binary classifiers of scikit-learn use a fixed decision threshold of 0.5\n",
    "# to convert probability estimates (i.e. output of `predict_proba`) into class\n",
    "# predictions. However, 0.5 is almost never the desired threshold for a given\n",
    "# problem. :class:`~model_selection.FixedThresholdClassifier` allows wrapping any\n",
    "# binary classifier and setting a custom decision threshold.\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=10_000, weights=[0.9, 0.1], random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "classifier_05 = LogisticRegression(C=1e6, random_state=0).fit(X_train, y_train)\n",
    "_ = ConfusionMatrixDisplay.from_estimator(classifier_05, X_test, y_test)\n",
    "\n",
    "# %%\n",
    "# Lowering the threshold, i.e. allowing more samples to be classified as the positive\n",
    "# class, increases the number of true positives at the cost of more false positives\n",
    "# (as is well known from the concavity of the ROC curve).\n",
    "from sklearn.model_selection import FixedThresholdClassifier\n",
    "\n",
    "classifier_01 = FixedThresholdClassifier(classifier_05, threshold=0.1)\n",
    "classifier_01.fit(X_train, y_train)\n",
    "_ = ConfusionMatrixDisplay.from_estimator(classifier_01, X_test, y_test)\n",
    "\n",
    "# %%\n",
    "# TunedThresholdClassifierCV: Tuning the decision threshold of a binary classifier\n",
    "# --------------------------------------------------------------------------------\n",
    "# The decision threshold of a binary classifier can be tuned to optimize a\n",
    "# given metric, using :class:`~model_selection.TunedThresholdClassifierCV`.\n",
    "#\n",
    "# It is particularly useful to find the best decision threshold when the model\n",
    "# is meant to be deployed in a specific application context where we can assign\n",
    "# different gains or costs for true positives, true negatives, false positives,\n",
    "# and false negatives.\n",
    "#\n",
    "# Let's illustrate this by considering an arbitrary case where:\n",
    "#\n",
    "# - each true positive gains 1 unit of profit, e.g. euro, year of life in good\n",
    "#   health, etc.;\n",
    "# - true negatives gain or cost nothing;\n",
    "# - each false negative costs 2;\n",
    "# - each false positive costs 0.1.\n",
    "#\n",
    "# Our metric quantifies the average profit per sample, which is defined by the\n",
    "# following Python function:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def custom_score(y_observed, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_observed, y_pred, normalize=\"all\").ravel()\n",
    "    return tp - 2 * fn - 0.1 * fp\n",
    "\n",
    "\n",
    "print(\"Untuned decision threshold: 0.5\")\n",
    "print(f\"Custom score: {custom_score(y_test, classifier_05.predict(X_test)):.2f}\")\n",
    "\n",
    "# %%\n",
    "# It is interesting to observe that the average gain per prediction is negative\n",
    "# which means that this decision system is making a loss on average.\n",
    "#\n",
    "# Tuning the threshold to optimize this custom metric gives a smaller threshold\n",
    "# that allows more samples to be classified as the positive class. As a result,\n",
    "# the average gain per prediction improves.\n",
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "custom_scorer = make_scorer(\n",
    "    custom_score, response_method=\"predict\", greater_is_better=True\n",
    ")\n",
    "tuned_classifier = TunedThresholdClassifierCV(\n",
    "    classifier_05, cv=5, scoring=custom_scorer\n",
    ").fit(X, y)\n",
    "\n",
    "print(f\"Tuned decision threshold: {tuned_classifier.best_threshold_:.3f}\")\n",
    "print(f\"Custom score: {custom_score(y_test, tuned_classifier.predict(X_test)):.2f}\")\n",
    "\n",
    "# %%\n",
    "# We observe that tuning the decision threshold can turn a machine\n",
    "# learning-based system that makes a loss on average into a beneficial one.\n",
    "#\n",
    "# In practice, defining a meaningful application-specific metric might involve\n",
    "# making those costs for bad predictions and gains for good predictions depend on\n",
    "# auxiliary metadata specific to each individual data point such as the amount\n",
    "# of a transaction in a fraud detection system.\n",
    "#\n",
    "# To achieve this, :class:`~model_selection.TunedThresholdClassifierCV`\n",
    "# leverages metadata routing support (:ref:`Metadata Routing User\n",
    "# Guide<metadata_routing>`) allowing to optimize complex business metrics as\n",
    "# detailed in :ref:`Post-tuning the decision threshold for cost-sensitive\n",
    "# learning\n",
    "# <sphx_glr_auto_examples_model_selection_plot_cost_sensitive_learning.py>`.\n",
    "\n",
    "# %%\n",
    "# Performance improvements in PCA\n",
    "# -------------------------------\n",
    "# :class:`~decomposition.PCA` has a new solver, `\"covariance_eigh\"`, which is\n",
    "# up to an order of magnitude faster and more memory efficient than the other\n",
    "# solvers for datasets with many data points and few features.\n",
    "from sklearn.datasets import make_low_rank_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = make_low_rank_matrix(\n",
    "    n_samples=10_000, n_features=100, tail_strength=0.1, random_state=0\n",
    ")\n",
    "\n",
    "pca = PCA(n_components=10, svd_solver=\"covariance_eigh\").fit(X)\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# The new solver also accepts sparse input data:\n",
    "from scipy.sparse import random\n",
    "\n",
    "X = random(10_000, 100, format=\"csr\", random_state=0)\n",
    "\n",
    "pca = PCA(n_components=10, svd_solver=\"covariance_eigh\").fit(X)\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2f}\")\n",
    "\n",
    "# %%\n",
    "# The `\"full\"` solver has also been improved to use less memory and allows\n",
    "# faster transformation. The default `svd_solver=\"auto\"`` option takes\n",
    "# advantage of the new solver and is now able to select an appropriate solver\n",
    "# for sparse datasets.\n",
    "#\n",
    "# Similarly to most other PCA solvers, the new `\"covariance_eigh\"` solver can leverage\n",
    "# GPU computation if the input data is passed as a PyTorch or CuPy array by\n",
    "# enabling the experimental support for :ref:`Array API <array_api>`.\n",
    "\n",
    "# %%\n",
    "# ColumnTransformer is subscriptable\n",
    "# ----------------------------------\n",
    "# The transformers of a :class:`~compose.ColumnTransformer` can now be directly\n",
    "# accessed using indexing by name.\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"std_scaler\", StandardScaler(), [0]), (\"one_hot\", OneHotEncoder(), [1, 2])]\n",
    ")\n",
    "\n",
    "column_transformer.fit(X)\n",
    "\n",
    "print(column_transformer[\"std_scaler\"])\n",
    "print(column_transformer[\"one_hot\"])\n",
    "\n",
    "# %%\n",
    "# Custom imputation strategies for the SimpleImputer\n",
    "# --------------------------------------------------\n",
    "# :class:`~impute.SimpleImputer` now supports custom strategies for imputation,\n",
    "# using a callable that computes a scalar value from the non missing values of\n",
    "# a column vector.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = np.array(\n",
    "    [\n",
    "        [-1.1, 1.1, 1.1],\n",
    "        [3.9, -1.2, np.nan],\n",
    "        [np.nan, 1.3, np.nan],\n",
    "        [-0.1, -1.4, -1.4],\n",
    "        [-4.9, 1.5, -1.5],\n",
    "        [np.nan, 1.6, 1.6],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def smallest_abs(arr):\n",
    "    \"\"\"Return the smallest absolute value of a 1D array.\"\"\"\n",
    "    return np.min(np.abs(arr))\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy=smallest_abs)\n",
    "\n",
    "imputer.fit_transform(X)\n",
    "\n",
    "# %%\n",
    "# Pairwise distances with non-numeric arrays\n",
    "# ------------------------------------------\n",
    "# :func:`~metrics.pairwise_distances` can now compute distances between\n",
    "# non-numeric arrays using a callable metric.\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "X = [\"cat\", \"dog\"]\n",
    "Y = [\"cat\", \"fox\"]\n",
    "\n",
    "\n",
    "def levenshtein_distance(x, y):\n",
    "    \"\"\"Return the Levenshtein distance between two strings.\"\"\"\n",
    "    if x == \"\" or y == \"\":\n",
    "        return max(len(x), len(y))\n",
    "    if x[0] == y[0]:\n",
    "        return levenshtein_distance(x[1:], y[1:])\n",
    "    return 1 + min(\n",
    "        levenshtein_distance(x[1:], y),\n",
    "        levenshtein_distance(x, y[1:]),\n",
    "        levenshtein_distance(x[1:], y[1:]),\n",
    "    )\n",
    "\n",
    "\n",
    "pairwise_distances(X, Y, metric=levenshtein_distance)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
