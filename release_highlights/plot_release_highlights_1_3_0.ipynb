{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa\n",
    "\"\"\"\n",
    "=======================================\n",
    "Release Highlights for scikit-learn 1.3\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "We are pleased to announce the release of scikit-learn 1.3! Many bug fixes\n",
    "and improvements were added, as well as some new key features. We detail\n",
    "below a few of the major features of this release. **For an exhaustive list of\n",
    "all the changes**, please refer to the :ref:`release notes <release_notes_1_3>`.\n",
    "\n",
    "To install the latest version (with pip)::\n",
    "\n",
    "    pip install --upgrade scikit-learn\n",
    "\n",
    "or with conda::\n",
    "\n",
    "    conda install -c conda-forge scikit-learn\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Metadata Routing\n",
    "# ----------------\n",
    "# We are in the process of introducing a new way to route metadata such as\n",
    "# ``sample_weight`` throughout the codebase, which would affect how\n",
    "# meta-estimators such as :class:`pipeline.Pipeline` and\n",
    "# :class:`model_selection.GridSearchCV` route metadata. While the\n",
    "# infrastructure for this feature is already included in this release, the work\n",
    "# is ongoing and not all meta-estimators support this new feature. You can read\n",
    "# more about this feature in the :ref:`Metadata Routing User Guide\n",
    "# <metadata_routing>`. Note that this feature is still under development and\n",
    "# not implemented for most meta-estimators.\n",
    "#\n",
    "# Third party developers can already start incorporating this into their\n",
    "# meta-estimators. For more details, see\n",
    "# :ref:`metadata routing developer guide\n",
    "# <sphx_glr_auto_examples_miscellaneous_plot_metadata_routing.py>`.\n",
    "\n",
    "# %%\n",
    "# HDBSCAN: hierarchical density-based clustering\n",
    "# ----------------------------------------------\n",
    "# Originally hosted in the scikit-learn-contrib repository, :class:`cluster.HDBSCAN`\n",
    "# has been adpoted into scikit-learn. It's missing a few features from the original\n",
    "# implementation which will be added in future releases.\n",
    "# By performing a modified version of :class:`cluster.DBSCAN` over multiple epsilon\n",
    "# values simultaneously, :class:`cluster.HDBSCAN` finds clusters of varying densities\n",
    "# making it more robust to parameter selection than :class:`cluster.DBSCAN`.\n",
    "# More details in the :ref:`User Guide <hdbscan>`.\n",
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "X, true_labels = load_digits(return_X_y=True)\n",
    "print(f\"number of digits: {len(np.unique(true_labels))}\")\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=15).fit(X)\n",
    "non_noisy_labels = hdbscan.labels_[hdbscan.labels_ != -1]\n",
    "print(f\"number of clusters found: {len(np.unique(non_noisy_labels))}\")\n",
    "\n",
    "print(v_measure_score(true_labels[hdbscan.labels_ != -1], non_noisy_labels))\n",
    "\n",
    "# %%\n",
    "# TargetEncoder: a new category encoding strategy\n",
    "# -----------------------------------------------\n",
    "# Well suited for categorical features with high cardinality,\n",
    "# :class:`preprocessing.TargetEncoder` encodes the categories based on a shrunk\n",
    "# estimate of the average target values for observations belonging to that category.\n",
    "# More details in the :ref:`User Guide <target_encoder>`.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "X = np.array([[\"cat\"] * 30 + [\"dog\"] * 20 + [\"snake\"] * 38], dtype=object).T\n",
    "y = [90.3] * 30 + [20.4] * 20 + [21.2] * 38\n",
    "\n",
    "enc = TargetEncoder(random_state=0)\n",
    "X_trans = enc.fit_transform(X, y)\n",
    "\n",
    "enc.encodings_\n",
    "\n",
    "# %%\n",
    "# Missing values support in decision trees\n",
    "# ----------------------------------------\n",
    "# The classes :class:`tree.DecisionTreeClassifier` and\n",
    "# :class:`tree.DecisionTreeRegressor` now support missing values. For each potential\n",
    "# threshold on the non-missing data, the splitter will evaluate the split with all the\n",
    "# missing values going to the left node or the right node.\n",
    "# See more details in the :ref:`User Guide <tree_missing_value_support>` or see\n",
    "# :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a usecase\n",
    "# example of this feature in :class:`~ensemble.HistGradientBoostingRegressor`.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0).fit(X, y)\n",
    "tree.predict(X)\n",
    "\n",
    "# %%\n",
    "# New display :class:`~model_selection.ValidationCurveDisplay`\n",
    "# ------------------------------------------------------------\n",
    "# :class:`model_selection.ValidationCurveDisplay` is now available to plot results\n",
    "# from :func:`model_selection.validation_curve`.\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ValidationCurveDisplay\n",
    "\n",
    "X, y = make_classification(1000, 10, random_state=0)\n",
    "\n",
    "_ = ValidationCurveDisplay.from_estimator(\n",
    "    LogisticRegression(),\n",
    "    X,\n",
    "    y,\n",
    "    param_name=\"C\",\n",
    "    param_range=np.geomspace(1e-5, 1e3, num=9),\n",
    "    score_type=\"both\",\n",
    "    score_name=\"Accuracy\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Gamma loss for gradient boosting\n",
    "# --------------------------------\n",
    "# The class :class:`ensemble.HistGradientBoostingRegressor` supports the\n",
    "# Gamma deviance loss function via `loss=\"gamma\"`. This loss function is useful for\n",
    "# modeling strictly positive targets with a right-skewed distribution.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_low_rank_matrix\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "n_samples, n_features = 500, 10\n",
    "rng = np.random.RandomState(0)\n",
    "X = make_low_rank_matrix(n_samples, n_features, random_state=rng)\n",
    "coef = rng.uniform(low=-10, high=20, size=n_features)\n",
    "y = rng.gamma(shape=2, scale=np.exp(X @ coef) / 2)\n",
    "gbdt = HistGradientBoostingRegressor(loss=\"gamma\")\n",
    "cross_val_score(gbdt, X, y).mean()\n",
    "\n",
    "# %%\n",
    "# Grouping infrequent categories in :class:`~preprocessing.OrdinalEncoder`\n",
    "# ------------------------------------------------------------------------\n",
    "# Similarly to :class:`preprocessing.OneHotEncoder`, the class\n",
    "# :class:`preprocessing.OrdinalEncoder` now supports aggregating infrequent categories\n",
    "# into a single output for each feature. The parameters to enable the gathering of\n",
    "# infrequent categories are `min_frequency` and `max_categories`.\n",
    "# See the :ref:`User Guide <encoder_infrequent_categories>` for more details.\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(\n",
    "    [[\"dog\"] * 5 + [\"cat\"] * 20 + [\"rabbit\"] * 10 + [\"snake\"] * 3], dtype=object\n",
    ").T\n",
    "enc = OrdinalEncoder(min_frequency=6).fit(X)\n",
    "enc.infrequent_categories_\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
