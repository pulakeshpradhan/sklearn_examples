{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4329b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==============================================================\n",
    "Restricted Boltzmann Machine features for digit classification\n",
    "==============================================================\n",
    "\n",
    "For greyscale image data where pixel values can be interpreted as degrees of\n",
    "blackness on a white background, like handwritten digit recognition, the\n",
    "Bernoulli Restricted Boltzmann machine model (:class:`BernoulliRBM\n",
    "<sklearn.neural_network.BernoulliRBM>`) can perform effective non-linear\n",
    "feature extraction.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Generate data\n",
    "# -------------\n",
    "#\n",
    "# In order to learn good latent representations from a small dataset, we\n",
    "# artificially generate more labeled data by perturbing the training data with\n",
    "# linear shifts of 1 pixel in each direction.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "\n",
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [1, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 1], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 1, 0]],\n",
    "    ]\n",
    "\n",
    "    def shift(x, w):\n",
    "        return convolve(x.reshape((8, 8)), mode=\"constant\", weights=w).ravel()\n",
    "\n",
    "    X = np.concatenate(\n",
    "        [X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors]\n",
    "    )\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "X = np.asarray(X, \"float32\")\n",
    "X, Y = nudge_dataset(X, y)\n",
    "X = minmax_scale(X, feature_range=(0, 1))  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Models definition\n",
    "# -----------------\n",
    "#\n",
    "# We build a classification pipeline with a BernoulliRBM feature extractor and\n",
    "# a :class:`LogisticRegression <sklearn.linear_model.LogisticRegression>`\n",
    "# classifier.\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "logistic = linear_model.LogisticRegression(solver=\"newton-cg\", tol=1)\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "\n",
    "rbm_features_classifier = Pipeline(steps=[(\"rbm\", rbm), (\"logistic\", logistic)])\n",
    "\n",
    "# %%\n",
    "# Training\n",
    "# --------\n",
    "#\n",
    "# The hyperparameters of the entire model (learning rate, hidden layer size,\n",
    "# regularization) were optimized by grid search, but the search is not\n",
    "# reproduced here because of runtime constraints.\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Hyper-parameters. These were set by cross-validation,\n",
    "# using a GridSearchCV. Here we are not performing cross-validation to\n",
    "# save time.\n",
    "rbm.learning_rate = 0.06\n",
    "rbm.n_iter = 10\n",
    "\n",
    "# More components tend to give better prediction performance, but larger\n",
    "# fitting time\n",
    "rbm.n_components = 100\n",
    "logistic.C = 6000\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "rbm_features_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Training the Logistic regression classifier directly on the pixel\n",
    "raw_pixel_classifier = clone(logistic)\n",
    "raw_pixel_classifier.C = 100.0\n",
    "raw_pixel_classifier.fit(X_train, Y_train)\n",
    "\n",
    "# %%\n",
    "# Evaluation\n",
    "# ----------\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "Y_pred = rbm_features_classifier.predict(X_test)\n",
    "print(\n",
    "    \"Logistic regression using RBM features:\\n%s\\n\"\n",
    "    % (metrics.classification_report(Y_test, Y_pred))\n",
    ")\n",
    "\n",
    "# %%\n",
    "Y_pred = raw_pixel_classifier.predict(X_test)\n",
    "print(\n",
    "    \"Logistic regression using raw pixel features:\\n%s\\n\"\n",
    "    % (metrics.classification_report(Y_test, Y_pred))\n",
    ")\n",
    "\n",
    "# %%\n",
    "# The features extracted by the BernoulliRBM help improve the classification\n",
    "# accuracy with respect to the logistic regression on raw pixels.\n",
    "\n",
    "# %%\n",
    "# Plotting\n",
    "# --------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4.2, 4))\n",
    "for i, comp in enumerate(rbm.components_):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "plt.suptitle(\"100 components extracted by RBM\", fontsize=16)\n",
    "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
