{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Scalable learning with polynomial kernel approximation\n",
    "======================================================\n",
    "\n",
    ".. currentmodule:: sklearn.kernel_approximation\n",
    "\n",
    "This example illustrates the use of :class:`PolynomialCountSketch` to\n",
    "efficiently generate polynomial kernel feature-space approximations.\n",
    "This is used to train linear classifiers that approximate the accuracy\n",
    "of kernelized ones.\n",
    "\n",
    "We use the Covtype dataset [2], trying to reproduce the experiments on the\n",
    "original paper of Tensor Sketch [1], i.e. the algorithm implemented by\n",
    ":class:`PolynomialCountSketch`.\n",
    "\n",
    "First, we compute the accuracy of a linear classifier on the original\n",
    "features. Then, we train linear classifiers on different numbers of\n",
    "features (`n_components`) generated by :class:`PolynomialCountSketch`,\n",
    "approximating the accuracy of a kernelized classifier in a scalable manner.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Preparing the data\n",
    "# ------------------\n",
    "#\n",
    "# Load the Covtype dataset, which contains 581,012 samples\n",
    "# with 54 features each, distributed among 6 classes. The goal of this dataset\n",
    "# is to predict forest cover type from cartographic variables only\n",
    "# (no remotely sensed data). After loading, we transform it into a binary\n",
    "# classification problem to match the version of the dataset in the\n",
    "# LIBSVM webpage [2], which was the one used in [1].\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "X, y = fetch_covtype(return_X_y=True)\n",
    "\n",
    "y[y != 2] = 0\n",
    "y[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.\n",
    "\n",
    "# %%\n",
    "# Partitioning the data\n",
    "# ---------------------\n",
    "#\n",
    "# Here we select 5,000 samples for training and 10,000 for testing.\n",
    "# To actually reproduce the results in the original Tensor Sketch paper,\n",
    "# select 100,000 for training.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=5_000, test_size=10_000, random_state=42\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Feature normalization\n",
    "# ---------------------\n",
    "#\n",
    "# Now scale features to the range [0, 1] to match the format of the dataset in\n",
    "# the LIBSVM webpage, and then normalize to unit length as done in the\n",
    "# original Tensor Sketch paper [1].\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "mm = make_pipeline(MinMaxScaler(), Normalizer())\n",
    "X_train = mm.fit_transform(X_train)\n",
    "X_test = mm.transform(X_test)\n",
    "\n",
    "# %%\n",
    "# Establishing a baseline model\n",
    "# -----------------------------\n",
    "#\n",
    "# As a baseline, train a linear SVM on the original features and print the\n",
    "# accuracy. We also measure and store accuracies and training times to\n",
    "# plot them later.\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "results = {}\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "start = time.time()\n",
    "lsvm.fit(X_train, y_train)\n",
    "lsvm_time = time.time() - start\n",
    "lsvm_score = 100 * lsvm.score(X_test, y_test)\n",
    "\n",
    "results[\"LSVM\"] = {\"time\": lsvm_time, \"score\": lsvm_score}\n",
    "print(f\"Linear SVM score on raw features: {lsvm_score:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Establishing the kernel approximation model\n",
    "# -------------------------------------------\n",
    "#\n",
    "# Then we train linear SVMs on the features generated by\n",
    "# :class:`PolynomialCountSketch` with different values for `n_components`,\n",
    "# showing that these kernel feature approximations improve the accuracy\n",
    "# of linear classification. In typical application scenarios, `n_components`\n",
    "# should be larger than the number of features in the input representation\n",
    "# in order to achieve an improvement with respect to linear classification.\n",
    "# As a rule of thumb, the optimum of evaluation score / run time cost is\n",
    "# typically achieved at around `n_components` = 10 * `n_features`, though this\n",
    "# might depend on the specific dataset being handled. Note that, since the\n",
    "# original samples have 54 features, the explicit feature map of the\n",
    "# polynomial kernel of degree four would have approximately 8.5 million\n",
    "# features (precisely, 54^4). Thanks to :class:`PolynomialCountSketch`, we can\n",
    "# condense most of the discriminative information of that feature space into a\n",
    "# much more compact representation. While we run the experiment only a single time\n",
    "# (`n_runs` = 1) in this example, in practice one should repeat the experiment several\n",
    "# times to compensate for the stochastic nature of :class:`PolynomialCountSketch`.\n",
    "\n",
    "from sklearn.kernel_approximation import PolynomialCountSketch\n",
    "\n",
    "n_runs = 1\n",
    "N_COMPONENTS = [250, 500, 1000, 2000]\n",
    "\n",
    "for n_components in N_COMPONENTS:\n",
    "    ps_lsvm_time = 0\n",
    "    ps_lsvm_score = 0\n",
    "    for _ in range(n_runs):\n",
    "        pipeline = make_pipeline(\n",
    "            PolynomialCountSketch(n_components=n_components, degree=4),\n",
    "            LinearSVC(),\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        ps_lsvm_time += time.time() - start\n",
    "        ps_lsvm_score += 100 * pipeline.score(X_test, y_test)\n",
    "\n",
    "    ps_lsvm_time /= n_runs\n",
    "    ps_lsvm_score /= n_runs\n",
    "\n",
    "    results[f\"LSVM + PS({n_components})\"] = {\n",
    "        \"time\": ps_lsvm_time,\n",
    "        \"score\": ps_lsvm_score,\n",
    "    }\n",
    "    print(\n",
    "        f\"Linear SVM score on {n_components} PolynomialCountSketch \"\n",
    "        + f\"features: {ps_lsvm_score:.2f}%\"\n",
    "    )\n",
    "\n",
    "# %%\n",
    "# Establishing the kernelized SVM model\n",
    "# -------------------------------------\n",
    "#\n",
    "# Train a kernelized SVM to see how well :class:`PolynomialCountSketch`\n",
    "# is approximating the performance of the kernel. This, of course, may take\n",
    "# some time, as the SVC class has a relatively poor scalability. This is the\n",
    "# reason why kernel approximators are so useful:\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ksvm = SVC(C=500.0, kernel=\"poly\", degree=4, coef0=0, gamma=1.0)\n",
    "\n",
    "start = time.time()\n",
    "ksvm.fit(X_train, y_train)\n",
    "ksvm_time = time.time() - start\n",
    "ksvm_score = 100 * ksvm.score(X_test, y_test)\n",
    "\n",
    "results[\"KSVM\"] = {\"time\": ksvm_time, \"score\": ksvm_score}\n",
    "print(f\"Kernel-SVM score on raw features: {ksvm_score:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Comparing the results\n",
    "# ---------------------\n",
    "#\n",
    "# Finally, plot the results of the different methods against their training\n",
    "# times. As we can see, the kernelized SVM achieves a higher accuracy,\n",
    "# but its training time is much larger and, most importantly, will grow\n",
    "# much faster if the number of training samples increases.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(\n",
    "    [\n",
    "        results[\"LSVM\"][\"time\"],\n",
    "    ],\n",
    "    [\n",
    "        results[\"LSVM\"][\"score\"],\n",
    "    ],\n",
    "    label=\"Linear SVM\",\n",
    "    c=\"green\",\n",
    "    marker=\"^\",\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    [\n",
    "        results[\"LSVM + PS(250)\"][\"time\"],\n",
    "    ],\n",
    "    [\n",
    "        results[\"LSVM + PS(250)\"][\"score\"],\n",
    "    ],\n",
    "    label=\"Linear SVM + PolynomialCountSketch\",\n",
    "    c=\"blue\",\n",
    ")\n",
    "\n",
    "for n_components in N_COMPONENTS:\n",
    "    ax.scatter(\n",
    "        [\n",
    "            results[f\"LSVM + PS({n_components})\"][\"time\"],\n",
    "        ],\n",
    "        [\n",
    "            results[f\"LSVM + PS({n_components})\"][\"score\"],\n",
    "        ],\n",
    "        c=\"blue\",\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"n_comp.={n_components}\",\n",
    "        (\n",
    "            results[f\"LSVM + PS({n_components})\"][\"time\"],\n",
    "            results[f\"LSVM + PS({n_components})\"][\"score\"],\n",
    "        ),\n",
    "        xytext=(-30, 10),\n",
    "        textcoords=\"offset pixels\",\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    [\n",
    "        results[\"KSVM\"][\"time\"],\n",
    "    ],\n",
    "    [\n",
    "        results[\"KSVM\"][\"score\"],\n",
    "    ],\n",
    "    label=\"Kernel SVM\",\n",
    "    c=\"red\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Training time (s)\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# References\n",
    "# ==========\n",
    "#\n",
    "# [1] Pham, Ninh and Rasmus Pagh. \"Fast and scalable polynomial kernels via\n",
    "# explicit feature maps.\" KDD '13 (2013).\n",
    "# https://doi.org/10.1145/2487575.2487591\n",
    "#\n",
    "# [2] LIBSVM binary datasets repository\n",
    "# https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
