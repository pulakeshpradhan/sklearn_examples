{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=====================================================\n",
    "Prediction Intervals for Gradient Boosting Regression\n",
    "=====================================================\n",
    "\n",
    "This example shows how quantile regression can be used to create prediction\n",
    "intervals. See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`\n",
    "for an example showcasing some other features of\n",
    ":class:`~ensemble.HistGradientBoostingRegressor`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Generate some data for a synthetic regression problem by applying the\n",
    "# function f to uniformly sampled random inputs.\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"The function to predict.\"\"\"\n",
    "    return x * np.sin(x)\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T\n",
    "expected_y = f(X).ravel()\n",
    "\n",
    "# %%\n",
    "# To make the problem interesting, we generate observations of the target y as\n",
    "# the sum of a deterministic term computed by the function f and a random noise\n",
    "# term that follows a centered `log-normal\n",
    "# <https://en.wikipedia.org/wiki/Log-normal_distribution>`_. To make this even\n",
    "# more interesting we consider the case where the amplitude of the noise\n",
    "# depends on the input variable x (heteroscedastic noise).\n",
    "#\n",
    "# The lognormal distribution is non-symmetric and long tailed: observing large\n",
    "# outliers is likely but it is impossible to observe small outliers.\n",
    "sigma = 0.5 + X.ravel() / 10\n",
    "noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)\n",
    "y = expected_y + noise\n",
    "\n",
    "# %%\n",
    "# Split into train, test datasets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Fitting non-linear quantile and least squares regressors\n",
    "# --------------------------------------------------------\n",
    "#\n",
    "# Fit gradient boosting models trained with the quantile loss and\n",
    "# alpha=0.05, 0.5, 0.95.\n",
    "#\n",
    "# The models obtained for alpha=0.05 and alpha=0.95 produce a 90% confidence\n",
    "# interval (95% - 5% = 90%).\n",
    "#\n",
    "# The model trained with alpha=0.5 produces a regression of the median: on\n",
    "# average, there should be the same number of target observations above and\n",
    "# below the predicted values.\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_pinball_loss, mean_squared_error\n",
    "\n",
    "all_models = {}\n",
    "common_params = dict(\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=9,\n",
    "    min_samples_split=9,\n",
    ")\n",
    "for alpha in [0.05, 0.5, 0.95]:\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, **common_params)\n",
    "    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Notice that :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is much\n",
    "# faster than :class:`~sklearn.ensemble.GradientBoostingRegressor` starting with\n",
    "# intermediate datasets (`n_samples >= 10_000`), which is not the case of the\n",
    "# present example.\n",
    "#\n",
    "# For the sake of comparison, we also fit a baseline model trained with the\n",
    "# usual (mean) squared error (MSE).\n",
    "gbr_ls = GradientBoostingRegressor(loss=\"squared_error\", **common_params)\n",
    "all_models[\"mse\"] = gbr_ls.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Create an evenly spaced evaluation set of input values spanning the [0, 10]\n",
    "# range.\n",
    "xx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n",
    "\n",
    "# %%\n",
    "# Plot the true conditional mean function f, the predictions of the conditional\n",
    "# mean (loss equals squared error), the conditional median and the conditional\n",
    "# 90% interval (from 5th to 95th conditional percentiles).\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = all_models[\"mse\"].predict(xx)\n",
    "y_lower = all_models[\"q 0.05\"].predict(xx)\n",
    "y_upper = all_models[\"q 0.95\"].predict(xx)\n",
    "y_med = all_models[\"q 0.50\"].predict(xx)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n",
    "plt.plot(xx, y_med, \"r-\", label=\"Predicted median\")\n",
    "plt.plot(xx, y_pred, \"r-\", label=\"Predicted mean\")\n",
    "plt.plot(xx, y_upper, \"k-\")\n",
    "plt.plot(xx, y_lower, \"k-\")\n",
    "plt.fill_between(\n",
    "    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.ylim(-10, 25)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Comparing the predicted median with the predicted mean, we note that the\n",
    "# median is on average below the mean as the noise is skewed towards high\n",
    "# values (large outliers). The median estimate also seems to be smoother\n",
    "# because of its natural robustness to outliers.\n",
    "#\n",
    "# Also observe that the inductive bias of gradient boosting trees is\n",
    "# unfortunately preventing our 0.05 quantile to fully capture the sinoisoidal\n",
    "# shape of the signal, in particular around x=8. Tuning hyper-parameters can\n",
    "# reduce this effect as shown in the last part of this notebook.\n",
    "#\n",
    "# Analysis of the error metrics\n",
    "# -----------------------------\n",
    "#\n",
    "# Measure the models with :func:`~sklearn.metrics.mean_squared_error` and\n",
    "# :func:`~sklearn.metrics.mean_pinball_loss` metrics on the training dataset.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def highlight_min(x):\n",
    "    x_min = x.min()\n",
    "    return [\"font-weight: bold\" if v == x_min else \"\" for v in x]\n",
    "\n",
    "\n",
    "results = []\n",
    "for name, gbr in sorted(all_models.items()):\n",
    "    metrics = {\"model\": name}\n",
    "    y_pred = gbr.predict(X_train)\n",
    "    for alpha in [0.05, 0.5, 0.95]:\n",
    "        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_train, y_pred, alpha=alpha)\n",
    "    metrics[\"MSE\"] = mean_squared_error(y_train, y_pred)\n",
    "    results.append(metrics)\n",
    "\n",
    "pd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)\n",
    "\n",
    "# %%\n",
    "# One column shows all models evaluated by the same metric. The minimum number\n",
    "# on a column should be obtained when the model is trained and measured with\n",
    "# the same metric. This should be always the case on the training set if the\n",
    "# training converged.\n",
    "#\n",
    "# Note that because the target distribution is asymmetric, the expected\n",
    "# conditional mean and conditional median are significantly different and\n",
    "# therefore one could not use the squared error model get a good estimation of\n",
    "# the conditional median nor the converse.\n",
    "#\n",
    "# If the target distribution were symmetric and had no outliers (e.g. with a\n",
    "# Gaussian noise), then median estimator and the least squares estimator would\n",
    "# have yielded similar predictions.\n",
    "#\n",
    "# We then do the same on the test set.\n",
    "results = []\n",
    "for name, gbr in sorted(all_models.items()):\n",
    "    metrics = {\"model\": name}\n",
    "    y_pred = gbr.predict(X_test)\n",
    "    for alpha in [0.05, 0.5, 0.95]:\n",
    "        metrics[\"pbl=%1.2f\" % alpha] = mean_pinball_loss(y_test, y_pred, alpha=alpha)\n",
    "    metrics[\"MSE\"] = mean_squared_error(y_test, y_pred)\n",
    "    results.append(metrics)\n",
    "\n",
    "pd.DataFrame(results).set_index(\"model\").style.apply(highlight_min)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Errors are higher meaning the models slightly overfitted the data. It still\n",
    "# shows that the best test metric is obtained when the model is trained by\n",
    "# minimizing this same metric.\n",
    "#\n",
    "# Note that the conditional median estimator is competitive with the squared\n",
    "# error estimator in terms of MSE on the test set: this can be explained by\n",
    "# the fact the squared error estimator is very sensitive to large outliers\n",
    "# which can cause significant overfitting. This can be seen on the right hand\n",
    "# side of the previous plot. The conditional median estimator is biased\n",
    "# (underestimation for this asymmetric noise) but is also naturally robust to\n",
    "# outliers and overfits less.\n",
    "#\n",
    "# .. _calibration-section:\n",
    "#\n",
    "# Calibration of the confidence interval\n",
    "# --------------------------------------\n",
    "#\n",
    "# We can also evaluate the ability of the two extreme quantile estimators at\n",
    "# producing a well-calibrated conditional 90%-confidence interval.\n",
    "#\n",
    "# To do this we can compute the fraction of observations that fall between the\n",
    "# predictions:\n",
    "def coverage_fraction(y, y_low, y_high):\n",
    "    return np.mean(np.logical_and(y >= y_low, y <= y_high))\n",
    "\n",
    "\n",
    "coverage_fraction(\n",
    "    y_train,\n",
    "    all_models[\"q 0.05\"].predict(X_train),\n",
    "    all_models[\"q 0.95\"].predict(X_train),\n",
    ")\n",
    "\n",
    "# %%\n",
    "# On the training set the calibration is very close to the expected coverage\n",
    "# value for a 90% confidence interval.\n",
    "coverage_fraction(\n",
    "    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "# On the test set, the estimated confidence interval is slightly too narrow.\n",
    "# Note, however, that we would need to wrap those metrics in a cross-validation\n",
    "# loop to assess their variability under data resampling.\n",
    "#\n",
    "# Tuning the hyper-parameters of the quantile regressors\n",
    "# ------------------------------------------------------\n",
    "#\n",
    "# In the plot above, we observed that the 5th percentile regressor seems to\n",
    "# underfit and could not adapt to sinusoidal shape of the signal.\n",
    "#\n",
    "# The hyper-parameters of the model were approximately hand-tuned for the\n",
    "# median regressor and there is no reason that the same hyper-parameters are\n",
    "# suitable for the 5th percentile regressor.\n",
    "#\n",
    "# To confirm this hypothesis, we tune the hyper-parameters of a new regressor\n",
    "# of the 5th percentile by selecting the best model parameters by\n",
    "# cross-validation on the pinball loss with alpha=0.05:\n",
    "\n",
    "# %%\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from pprint import pprint\n",
    "\n",
    "param_grid = dict(\n",
    "    learning_rate=[0.05, 0.1, 0.2],\n",
    "    max_depth=[2, 5, 10],\n",
    "    min_samples_leaf=[1, 5, 10, 20],\n",
    "    min_samples_split=[5, 10, 20, 30, 50],\n",
    ")\n",
    "alpha = 0.05\n",
    "neg_mean_pinball_loss_05p_scorer = make_scorer(\n",
    "    mean_pinball_loss,\n",
    "    alpha=alpha,\n",
    "    greater_is_better=False,  # maximize the negative loss\n",
    ")\n",
    "gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha, random_state=0)\n",
    "search_05p = HalvingRandomSearchCV(\n",
    "    gbr,\n",
    "    param_grid,\n",
    "    resource=\"n_estimators\",\n",
    "    max_resources=250,\n",
    "    min_resources=50,\n",
    "    scoring=neg_mean_pinball_loss_05p_scorer,\n",
    "    n_jobs=2,\n",
    "    random_state=0,\n",
    ").fit(X_train, y_train)\n",
    "pprint(search_05p.best_params_)\n",
    "\n",
    "# %%\n",
    "# We observe that the hyper-parameters that were hand-tuned for the median\n",
    "# regressor are in the same range as the hyper-parameters suitable for the 5th\n",
    "# percentile regressor.\n",
    "#\n",
    "# Let's now tune the hyper-parameters for the 95th percentile regressor. We\n",
    "# need to redefine the `scoring` metric used to select the best model, along\n",
    "# with adjusting the alpha parameter of the inner gradient boosting estimator\n",
    "# itself:\n",
    "from sklearn.base import clone\n",
    "\n",
    "alpha = 0.95\n",
    "neg_mean_pinball_loss_95p_scorer = make_scorer(\n",
    "    mean_pinball_loss,\n",
    "    alpha=alpha,\n",
    "    greater_is_better=False,  # maximize the negative loss\n",
    ")\n",
    "search_95p = clone(search_05p).set_params(\n",
    "    estimator__alpha=alpha,\n",
    "    scoring=neg_mean_pinball_loss_95p_scorer,\n",
    ")\n",
    "search_95p.fit(X_train, y_train)\n",
    "pprint(search_95p.best_params_)\n",
    "\n",
    "# %%\n",
    "# The result shows that the hyper-parameters for the 95th percentile regressor\n",
    "# identified by the search procedure are roughly in the same range as the hand-\n",
    "# tuned hyper-parameters for the median regressor and the hyper-parameters\n",
    "# identified by the search procedure for the 5th percentile regressor. However,\n",
    "# the hyper-parameter searches did lead to an improved 90% confidence interval\n",
    "# that is comprised by the predictions of those two tuned quantile regressors.\n",
    "# Note that the prediction of the upper 95th percentile has a much coarser shape\n",
    "# than the prediction of the lower 5th percentile because of the outliers:\n",
    "y_lower = search_05p.predict(xx)\n",
    "y_upper = search_95p.predict(xx)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(xx, f(xx), \"g:\", linewidth=3, label=r\"$f(x) = x\\,\\sin(x)$\")\n",
    "plt.plot(X_test, y_test, \"b.\", markersize=10, label=\"Test observations\")\n",
    "plt.plot(xx, y_upper, \"k-\")\n",
    "plt.plot(xx, y_lower, \"k-\")\n",
    "plt.fill_between(\n",
    "    xx.ravel(), y_lower, y_upper, alpha=0.4, label=\"Predicted 90% interval\"\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.ylim(-10, 25)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Prediction with tuned hyper-parameters\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# The plot looks qualitatively better than for the untuned models, especially\n",
    "# for the shape of the of lower quantile.\n",
    "#\n",
    "# We now quantitatively evaluate the joint-calibration of the pair of\n",
    "# estimators:\n",
    "coverage_fraction(y_train, search_05p.predict(X_train), search_95p.predict(X_train))\n",
    "# %%\n",
    "coverage_fraction(y_test, search_05p.predict(X_test), search_95p.predict(X_test))\n",
    "# %%\n",
    "# The calibration of the tuned pair is sadly not better on the test set: the\n",
    "# width of the estimated confidence interval is still too narrow.\n",
    "#\n",
    "# Again, we would need to wrap this study in a cross-validation loop to\n",
    "# better assess the variability of those estimates.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
