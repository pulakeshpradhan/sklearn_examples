{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================\n",
    "Gradient Boosting regression\n",
    "============================\n",
    "\n",
    "This example demonstrates Gradient Boosting to produce a predictive\n",
    "model from an ensemble of weak predictive models. Gradient boosting can be used\n",
    "for regression and classification problems. Here, we will train a model to\n",
    "tackle a diabetes regression task. We will obtain the results from\n",
    ":class:`~sklearn.ensemble.GradientBoostingRegressor` with least squares loss\n",
    "and 500 regression trees of depth 4.\n",
    "\n",
    "Note: For larger datasets (n_samples >= 10000), please refer to\n",
    ":class:`~sklearn.ensemble.HistGradientBoostingRegressor`. See\n",
    ":ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an example\n",
    "showcasing some other advantages of\n",
    ":class:`~ensemble.HistGradientBoostingRegressor`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.fixes import parse_version\n",
    "\n",
    "# %%\n",
    "# Load the data\n",
    "# -------------------------------------\n",
    "#\n",
    "# First we need to load the data.\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# %%\n",
    "# Data preprocessing\n",
    "# -------------------------------------\n",
    "#\n",
    "# Next, we will split our dataset to use 90% for training and leave the rest\n",
    "# for testing. We will also set the regression model parameters. You can play\n",
    "# with these parameters to see how the results change.\n",
    "#\n",
    "# `n_estimators` : the number of boosting stages that will be performed.\n",
    "# Later, we will plot deviance against boosting iterations.\n",
    "#\n",
    "# `max_depth` : limits the number of nodes in the tree.\n",
    "# The best value depends on the interaction of the input variables.\n",
    "#\n",
    "# `min_samples_split` : the minimum number of samples required to split an\n",
    "# internal node.\n",
    "#\n",
    "# `learning_rate` : how much the contribution of each tree will shrink.\n",
    "#\n",
    "# `loss` : loss function to optimize. The least squares function is  used in\n",
    "# this case however, there are many other options (see\n",
    "# :class:`~sklearn.ensemble.GradientBoostingRegressor` ).\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=13\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "\n",
    "# %%\n",
    "# Fit regression model\n",
    "# --------------------\n",
    "#\n",
    "# Now we will initiate the gradient boosting regressors and fit it with our\n",
    "# training data. Let's also look and the mean squared error on the test data.\n",
    "\n",
    "reg = ensemble.GradientBoostingRegressor(**params)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n",
    "\n",
    "# %%\n",
    "# Plot training deviance\n",
    "# ----------------------\n",
    "#\n",
    "# Finally, we will visualize the results. To do that we will first compute the\n",
    "# test set deviance and then plot it against boosting iterations.\n",
    "\n",
    "test_score = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "    test_score[i] = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title(\"Deviance\")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1,\n",
    "    reg.train_score_,\n",
    "    \"b-\",\n",
    "    label=\"Training Set Deviance\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(params[\"n_estimators\"]) + 1, test_score, \"r-\", label=\"Test Set Deviance\"\n",
    ")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Deviance\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot feature importance\n",
    "# -----------------------\n",
    "#\n",
    "# .. warning::\n",
    "#    Careful, impurity-based feature importances can be misleading for\n",
    "#    **high cardinality** features (many unique values). As an alternative,\n",
    "#    the permutation importances of ``reg`` can be computed on a\n",
    "#    held out test set. See :ref:`permutation_importance` for more details.\n",
    "#\n",
    "# For this example, the impurity-based and permutation methods identify the\n",
    "# same 2 strongly predictive features but not in the same order. The third most\n",
    "# predictive feature, \"bp\", is also the same for the 2 methods. The remaining\n",
    "# features are less predictive and the error bars of the permutation plot\n",
    "# show that they overlap with 0.\n",
    "\n",
    "feature_importance = reg.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\n",
    "# renamed to `tick_labels`. The following code handles this, but as a\n",
    "# scikit-learn user you probably can write simpler code by using `labels=...`\n",
    "# (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\n",
    "tick_labels_parameter_name = (\n",
    "    \"tick_labels\"\n",
    "    if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\n",
    "    else \"labels\"\n",
    ")\n",
    "tick_labels_dict = {\n",
    "    tick_labels_parameter_name: np.array(diabetes.feature_names)[sorted_idx]\n",
    "}\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, **tick_labels_dict)\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
