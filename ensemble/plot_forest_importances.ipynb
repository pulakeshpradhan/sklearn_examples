{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc4bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================================\n",
    "Feature importances with a forest of trees\n",
    "==========================================\n",
    "\n",
    "This example shows the use of a forest of trees to evaluate the importance of\n",
    "features on an artificial classification task. The blue bars are the feature\n",
    "importances of the forest, along with their inter-trees variability represented\n",
    "by the error bars.\n",
    "\n",
    "As expected, the plot suggests that 3 features are informative, while the\n",
    "remaining are not.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "# Data generation and model fitting\n",
    "# ---------------------------------\n",
    "# We generate a synthetic dataset with only 3 informative features. We will\n",
    "# explicitly not shuffle the dataset to ensure that the informative features\n",
    "# will correspond to the three first columns of X. In addition, we will split\n",
    "# our dataset into training and testing subsets.\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# %%\n",
    "# A random forest classifier will be fitted to compute the feature importances.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# Feature importance based on mean decrease in impurity\n",
    "# -----------------------------------------------------\n",
    "# Feature importances are provided by the fitted attribute\n",
    "# `feature_importances_` and they are computed as the mean and standard\n",
    "# deviation of accumulation of the impurity decrease within each tree.\n",
    "#\n",
    "# .. warning::\n",
    "#     Impurity-based feature importances can be misleading for **high\n",
    "#     cardinality** features (many unique values). See\n",
    "#     :ref:`permutation_importance` as an alternative below.\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "# %%\n",
    "# Let's plot the impurity-based importance.\n",
    "import pandas as pd\n",
    "\n",
    "forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# %%\n",
    "# We observe that, as expected, the three first features are found important.\n",
    "#\n",
    "# Feature importance based on feature permutation\n",
    "# -----------------------------------------------\n",
    "# Permutation feature importance overcomes limitations of the impurity-based\n",
    "# feature importance: they do not have a bias toward high-cardinality features\n",
    "# and can be computed on a left-out test set.\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# %%\n",
    "# The computation for full permutation importance is more costly. Features are\n",
    "# shuffled n times and the model refitted to estimate the importance of it.\n",
    "# Please see :ref:`permutation_importance` for more details. We can now plot\n",
    "# the importance ranking.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# The same features are detected as most important using both methods. Although\n",
    "# the relative importances vary. As seen on the plots, MDI is less likely than\n",
    "# permutation importance to fully omit a feature.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
