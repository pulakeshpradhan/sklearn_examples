{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa015d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=======================\n",
    "IsolationForest example\n",
    "=======================\n",
    "\n",
    "An example using :class:`~sklearn.ensemble.IsolationForest` for anomaly\n",
    "detection.\n",
    "\n",
    "The :ref:`isolation_forest` is an ensemble of \"Isolation Trees\" that \"isolate\"\n",
    "observations by recursive random partitioning, which can be represented by a\n",
    "tree structure. The number of splittings required to isolate a sample is lower\n",
    "for outliers and higher for inliers.\n",
    "\n",
    "In the present example we demo two ways to visualize the decision boundary of an\n",
    "Isolation Forest trained on a toy dataset.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Data generation\n",
    "# ---------------\n",
    "#\n",
    "# We generate two clusters (each one containing `n_samples`) by randomly\n",
    "# sampling the standard normal distribution as returned by\n",
    "# :func:`numpy.random.randn`. One of them is spherical and the other one is\n",
    "# slightly deformed.\n",
    "#\n",
    "# For consistency with the :class:`~sklearn.ensemble.IsolationForest` notation,\n",
    "# the inliers (i.e. the gaussian clusters) are assigned a ground truth label `1`\n",
    "# whereas the outliers (created with :func:`numpy.random.uniform`) are assigned\n",
    "# the label `-1`.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples, n_outliers = 120, 40\n",
    "rng = np.random.RandomState(0)\n",
    "covariance = np.array([[0.5, -0.1], [0.7, 0.4]])\n",
    "cluster_1 = 0.4 * rng.randn(n_samples, 2) @ covariance + np.array([2, 2])  # general\n",
    "cluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array([-2, -2])  # spherical\n",
    "outliers = rng.uniform(low=-4, high=4, size=(n_outliers, 2))\n",
    "\n",
    "X = np.concatenate([cluster_1, cluster_2, outliers])\n",
    "y = np.concatenate(\n",
    "    [np.ones((2 * n_samples), dtype=int), -np.ones((n_outliers), dtype=int)]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# %%\n",
    "# We can visualize the resulting clusters:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.title(\"Gaussian inliers with \\nuniformly distributed outliers\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Training of the model\n",
    "# ---------------------\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(max_samples=100, random_state=0)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# %%\n",
    "# Plot discrete decision boundary\n",
    "# -------------------------------\n",
    "#\n",
    "# We use the class :class:`~sklearn.inspection.DecisionBoundaryDisplay` to\n",
    "# visualize a discrete decision boundary. The background color represents\n",
    "# whether a sample in that given area is predicted to be an outlier\n",
    "# or not. The scatter plot displays the true labels.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    response_method=\"predict\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "disp.ax_.set_title(\"Binary decision boundary \\nof IsolationForest\")\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot path length decision boundary\n",
    "# ----------------------------------\n",
    "#\n",
    "# By setting the `response_method=\"decision_function\"`, the background of the\n",
    "# :class:`~sklearn.inspection.DecisionBoundaryDisplay` represents the measure of\n",
    "# normality of an observation. Such score is given by the path length averaged\n",
    "# over a forest of random trees, which itself is given by the depth of the leaf\n",
    "# (or equivalently the number of splits) required to isolate a given sample.\n",
    "#\n",
    "# When a forest of random trees collectively produce short path lengths for\n",
    "# isolating some particular samples, they are highly likely to be anomalies and\n",
    "# the measure of normality is close to `0`. Similarly, large paths correspond to\n",
    "# values close to `1` and are more likely to be inliers.\n",
    "\n",
    "disp = DecisionBoundaryDisplay.from_estimator(\n",
    "    clf,\n",
    "    X,\n",
    "    response_method=\"decision_function\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "disp.ax_.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "disp.ax_.set_title(\"Path length decision boundary \\nof IsolationForest\")\n",
    "plt.axis(\"square\")\n",
    "plt.legend(handles=handles, labels=[\"outliers\", \"inliers\"], title=\"true class\")\n",
    "plt.colorbar(disp.ax_.collections[1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
