{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================\n",
    "Categorical Feature Support in Gradient Boosting\n",
    "================================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "In this example, we will compare the training times and prediction\n",
    "performances of :class:`~ensemble.HistGradientBoostingRegressor` with\n",
    "different encoding strategies for categorical features. In\n",
    "particular, we will evaluate:\n",
    "\n",
    "- dropping the categorical features\n",
    "- using a :class:`~preprocessing.OneHotEncoder`\n",
    "- using an :class:`~preprocessing.OrdinalEncoder` and treat categories as\n",
    "  ordered, equidistant quantities\n",
    "- using an :class:`~preprocessing.OrdinalEncoder` and rely on the :ref:`native\n",
    "  category support <categorical_support_gbdt>` of the\n",
    "  :class:`~ensemble.HistGradientBoostingRegressor` estimator.\n",
    "\n",
    "We will work with the Ames Iowa Housing dataset which consists of numerical\n",
    "and categorical features, where the houses' sales prices is the target.\n",
    "\n",
    "See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an\n",
    "example showcasing some other features of\n",
    ":class:`~ensemble.HistGradientBoostingRegressor`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Load Ames Housing dataset\n",
    "# -------------------------\n",
    "# First, we load the Ames Housing data as a pandas dataframe. The features\n",
    "# are either categorical or numerical:\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(data_id=42165, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Select only a subset of features of X to make the example faster to run\n",
    "categorical_columns_subset = [\n",
    "    \"BldgType\",\n",
    "    \"GarageFinish\",\n",
    "    \"LotConfig\",\n",
    "    \"Functional\",\n",
    "    \"MasVnrType\",\n",
    "    \"HouseStyle\",\n",
    "    \"FireplaceQu\",\n",
    "    \"ExterCond\",\n",
    "    \"ExterQual\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "\n",
    "numerical_columns_subset = [\n",
    "    \"3SsnPorch\",\n",
    "    \"Fireplaces\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"HalfBath\",\n",
    "    \"GarageCars\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"GrLivArea\",\n",
    "    \"ScreenPorch\",\n",
    "]\n",
    "\n",
    "X = X[categorical_columns_subset + numerical_columns_subset]\n",
    "X[categorical_columns_subset] = X[categorical_columns_subset].astype(\"category\")\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=\"category\").columns\n",
    "n_categorical_features = len(categorical_columns)\n",
    "n_numerical_features = X.select_dtypes(include=\"number\").shape[1]\n",
    "\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of categorical features: {n_categorical_features}\")\n",
    "print(f\"Number of numerical features: {n_numerical_features}\")\n",
    "\n",
    "# %%\n",
    "# Gradient boosting estimator with dropped categorical features\n",
    "# -------------------------------------------------------------\n",
    "# As a baseline, we create an estimator where the categorical features are\n",
    "# dropped:\n",
    "\n",
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "dropper = make_column_transformer(\n",
    "    (\"drop\", make_column_selector(dtype_include=\"category\")), remainder=\"passthrough\"\n",
    ")\n",
    "hist_dropped = make_pipeline(dropper, HistGradientBoostingRegressor(random_state=42))\n",
    "\n",
    "# %%\n",
    "# Gradient boosting estimator with one-hot encoding\n",
    "# -------------------------------------------------\n",
    "# Next, we create a pipeline that will one-hot encode the categorical features\n",
    "# and let the rest of the numerical data to passthrough:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = make_column_transformer(\n",
    "    (\n",
    "        OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
    "        make_column_selector(dtype_include=\"category\"),\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "hist_one_hot = make_pipeline(\n",
    "    one_hot_encoder, HistGradientBoostingRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Gradient boosting estimator with ordinal encoding\n",
    "# -------------------------------------------------\n",
    "# Next, we create a pipeline that will treat categorical features as if they\n",
    "# were ordered quantities, i.e. the categories will be encoded as 0, 1, 2,\n",
    "# etc., and treated as continuous features.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = make_column_transformer(\n",
    "    (\n",
    "        OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=np.nan),\n",
    "        make_column_selector(dtype_include=\"category\"),\n",
    "    ),\n",
    "    remainder=\"passthrough\",\n",
    "    # Use short feature names to make it easier to specify the categorical\n",
    "    # variables in the HistGradientBoostingRegressor in the next step\n",
    "    # of the pipeline.\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "hist_ordinal = make_pipeline(\n",
    "    ordinal_encoder, HistGradientBoostingRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Gradient boosting estimator with native categorical support\n",
    "# -----------------------------------------------------------\n",
    "# We now create a :class:`~ensemble.HistGradientBoostingRegressor` estimator\n",
    "# that will natively handle categorical features. This estimator will not treat\n",
    "# categorical features as ordered quantities. We set\n",
    "# `categorical_features=\"from_dtype\"` such that features with categorical dtype\n",
    "# are considered categorical features.\n",
    "#\n",
    "# The main difference between this estimator and the previous one is that in\n",
    "# this one, we let the :class:`~ensemble.HistGradientBoostingRegressor` detect\n",
    "# which features are categorical from the DataFrame columns' dtypes.\n",
    "\n",
    "hist_native = HistGradientBoostingRegressor(\n",
    "    random_state=42, categorical_features=\"from_dtype\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Model comparison\n",
    "# ----------------\n",
    "# Finally, we evaluate the models using cross validation. Here we compare the\n",
    "# models performance in terms of\n",
    "# :func:`~metrics.mean_absolute_percentage_error` and fit times.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = \"neg_mean_absolute_percentage_error\"\n",
    "n_cv_folds = 3\n",
    "\n",
    "dropped_result = cross_validate(hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "one_hot_result = cross_validate(hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "ordinal_result = cross_validate(hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "native_result = cross_validate(hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "\n",
    "\n",
    "def plot_results(figure_title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "    plot_info = [\n",
    "        (\"fit_time\", \"Fit times (s)\", ax1, None),\n",
    "        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n",
    "    ]\n",
    "\n",
    "    x, width = np.arange(4), 0.9\n",
    "    for key, title, ax, y_limit in plot_info:\n",
    "        items = [\n",
    "            dropped_result[key],\n",
    "            one_hot_result[key],\n",
    "            ordinal_result[key],\n",
    "            native_result[key],\n",
    "        ]\n",
    "\n",
    "        mape_cv_mean = [np.mean(np.abs(item)) for item in items]\n",
    "        mape_cv_std = [np.std(item) for item in items]\n",
    "\n",
    "        ax.bar(\n",
    "            x=x,\n",
    "            height=mape_cv_mean,\n",
    "            width=width,\n",
    "            yerr=mape_cv_std,\n",
    "            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n",
    "        )\n",
    "        ax.set(\n",
    "            xlabel=\"Model\",\n",
    "            title=title,\n",
    "            xticks=x,\n",
    "            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n",
    "            ylim=y_limit,\n",
    "        )\n",
    "    fig.suptitle(figure_title)\n",
    "\n",
    "\n",
    "plot_results(\"Gradient Boosting on Ames Housing\")\n",
    "\n",
    "# %%\n",
    "# We see that the model with one-hot-encoded data is by far the slowest. This\n",
    "# is to be expected, since one-hot-encoding creates one additional feature per\n",
    "# category value (for each categorical feature), and thus more split points\n",
    "# need to be considered during fitting. In theory, we expect the native\n",
    "# handling of categorical features to be slightly slower than treating\n",
    "# categories as ordered quantities ('Ordinal'), since native handling requires\n",
    "# :ref:`sorting categories <categorical_support_gbdt>`. Fitting times should\n",
    "# however be close when the number of categories is small, and this may not\n",
    "# always be reflected in practice.\n",
    "#\n",
    "# In terms of prediction performance, dropping the categorical features leads\n",
    "# to poorer performance. The three models that use categorical features have\n",
    "# comparable error rates, with a slight edge for the native handling.\n",
    "\n",
    "# %%\n",
    "# Limiting the number of splits\n",
    "# -----------------------------\n",
    "# In general, one can expect poorer predictions from one-hot-encoded data,\n",
    "# especially when the tree depths or the number of nodes are limited: with\n",
    "# one-hot-encoded data, one needs more split points, i.e. more depth, in order\n",
    "# to recover an equivalent split that could be obtained in one single split\n",
    "# point with native handling.\n",
    "#\n",
    "# This is also true when categories are treated as ordinal quantities: if\n",
    "# categories are `A..F` and the best split is `ACF - BDE` the one-hot-encoder\n",
    "# model will need 3 split points (one per category in the left node), and the\n",
    "# ordinal non-native model will need 4 splits: 1 split to isolate `A`, 1 split\n",
    "# to isolate `F`, and 2 splits to isolate `C` from `BCDE`.\n",
    "#\n",
    "# How strongly the models' performances differ in practice will depend on the\n",
    "# dataset and on the flexibility of the trees.\n",
    "#\n",
    "# To see this, let us re-run the same analysis with under-fitting models where\n",
    "# we artificially limit the total number of splits by both limiting the number\n",
    "# of trees and the depth of each tree.\n",
    "\n",
    "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n",
    "    if pipe is hist_native:\n",
    "        # The native model does not use a pipeline so, we can set the parameters\n",
    "        # directly.\n",
    "        pipe.set_params(max_depth=3, max_iter=15)\n",
    "    else:\n",
    "        pipe.set_params(\n",
    "            histgradientboostingregressor__max_depth=3,\n",
    "            histgradientboostingregressor__max_iter=15,\n",
    "        )\n",
    "\n",
    "dropped_result = cross_validate(hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "one_hot_result = cross_validate(hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "ordinal_result = cross_validate(hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "native_result = cross_validate(hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n",
    "\n",
    "plot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# The results for these under-fitting models confirm our previous intuition:\n",
    "# the native category handling strategy performs the best when the splitting\n",
    "# budget is constrained. The two other strategies (one-hot encoding and\n",
    "# treating categories as ordinal values) lead to error values comparable\n",
    "# to the baseline model that just dropped the categorical features altogether.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
