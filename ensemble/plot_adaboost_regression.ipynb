{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5867ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================\n",
    "Decision Tree Regression with AdaBoost\n",
    "======================================\n",
    "\n",
    "A decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D\n",
    "sinusoidal dataset with a small amount of Gaussian noise.\n",
    "299 boosts (300 decision trees) is compared with a single decision tree\n",
    "regressor. As the number of boosts is increased the regressor can fit more\n",
    "detail.\n",
    "\n",
    "See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for an\n",
    "example showcasing the benefits of using more efficient regression models such\n",
    "as :class:`~ensemble.HistGradientBoostingRegressor`.\n",
    "\n",
    ".. [1] `H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
    "        <https://citeseerx.ist.psu.edu/doc_view/pid/8d49e2dedb817f2c3330e74b63c5fc86d2399ce3>`_\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "# Preparing the data\n",
    "# ------------------\n",
    "# First, we prepare dummy data with a sinusoidal relationship and some gaussian noise.\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# %%\n",
    "# Training and prediction with DecisionTree and AdaBoost Regressors\n",
    "# -----------------------------------------------------------------\n",
    "# Now, we define the classifiers and fit them to the data.\n",
    "# Then we predict on that same data to see how well they could fit it.\n",
    "# The first regressor is a `DecisionTreeRegressor` with `max_depth=4`.\n",
    "# The second regressor is an `AdaBoostRegressor` with a `DecisionTreeRegressor`\n",
    "# of `max_depth=4` as base learner and will be built with `n_estimators=300`\n",
    "# of those base learners.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng\n",
    ")\n",
    "\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "\n",
    "# %%\n",
    "# Plotting the results\n",
    "# --------------------\n",
    "# Finally, we plot how well our two regressors,\n",
    "# single decision tree regressor and AdaBoost regressor, could fit the data.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "colors = sns.color_palette(\"colorblind\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y, color=colors[0], label=\"training samples\")\n",
    "plt.plot(X, y_1, color=colors[1], label=\"n_estimators=1\", linewidth=2)\n",
    "plt.plot(X, y_2, color=colors[2], label=\"n_estimators=300\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
