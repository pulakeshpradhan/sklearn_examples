{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================\n",
    "Plot the decision boundaries of a VotingClassifier\n",
    "==================================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "Plot the decision boundaries of a :class:`~ensemble.VotingClassifier` for two\n",
    "features of the Iris dataset.\n",
    "\n",
    "Plot the class probabilities of the first sample in a toy dataset predicted by\n",
    "three different classifiers and averaged by the\n",
    ":class:`~ensemble.VotingClassifier`.\n",
    "\n",
    "First, three exemplary classifiers are initialized\n",
    "(:class:`~tree.DecisionTreeClassifier`,\n",
    ":class:`~neighbors.KNeighborsClassifier`, and :class:`~svm.SVC`) and used to\n",
    "initialize a soft-voting :class:`~ensemble.VotingClassifier` with weights `[2,\n",
    "1, 2]`, which means that the predicted probabilities of the\n",
    ":class:`~tree.DecisionTreeClassifier` and :class:`~svm.SVC` each count 2 times\n",
    "as much as the weights of the :class:`~neighbors.KNeighborsClassifier`\n",
    "classifier when the averaged probability is calculated.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(gamma=0.1, kernel=\"rbf\", probability=True)\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[(\"dt\", clf1), (\"knn\", clf2), (\"svc\", clf3)],\n",
    "    voting=\"soft\",\n",
    "    weights=[2, 1, 2],\n",
    ")\n",
    "\n",
    "clf1.fit(X, y)\n",
    "clf2.fit(X, y)\n",
    "clf3.fit(X, y)\n",
    "eclf.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "f, axarr = plt.subplots(2, 2, sharex=\"col\", sharey=\"row\", figsize=(10, 8))\n",
    "for idx, clf, tt in zip(\n",
    "    product([0, 1], [0, 1]),\n",
    "    [clf1, clf2, clf3, eclf],\n",
    "    [\"Decision Tree (depth=4)\", \"KNN (k=7)\", \"Kernel SVM\", \"Soft Voting\"],\n",
    "):\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        clf, X, alpha=0.4, ax=axarr[idx[0], idx[1]], response_method=\"predict\"\n",
    "    )\n",
    "    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor=\"k\")\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
