{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================\n",
    "Gradient Boosting regularization\n",
    "================================\n",
    "\n",
    "Illustration of the effect of different regularization strategies\n",
    "for Gradient Boosting. The example is taken from Hastie et al 2009 [1]_.\n",
    "\n",
    "The loss function used is binomial deviance. Regularization via\n",
    "shrinkage (``learning_rate < 1.0``) improves performance considerably.\n",
    "In combination with shrinkage, stochastic gradient boosting\n",
    "(``subsample < 1.0``) can produce more accurate models by reducing the\n",
    "variance via bagging.\n",
    "Subsampling without shrinkage usually does poorly.\n",
    "Another strategy to reduce the variance is by subsampling the features\n",
    "analogous to the random splits in Random Forests\n",
    "(via the ``max_features`` parameter).\n",
    "\n",
    ".. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n",
    "    Learning Ed. 2\", Springer, 2009.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_hastie_10_2(n_samples=4000, random_state=1)\n",
    "\n",
    "# map labels from {-1, 1} to {0, 1}\n",
    "labels, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)\n",
    "\n",
    "original_params = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"max_leaf_nodes\": 4,\n",
    "    \"max_depth\": None,\n",
    "    \"random_state\": 2,\n",
    "    \"min_samples_split\": 5,\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for label, color, setting in [\n",
    "    (\"No shrinkage\", \"orange\", {\"learning_rate\": 1.0, \"subsample\": 1.0}),\n",
    "    (\"learning_rate=0.2\", \"turquoise\", {\"learning_rate\": 0.2, \"subsample\": 1.0}),\n",
    "    (\"subsample=0.5\", \"blue\", {\"learning_rate\": 1.0, \"subsample\": 0.5}),\n",
    "    (\n",
    "        \"learning_rate=0.2, subsample=0.5\",\n",
    "        \"gray\",\n",
    "        {\"learning_rate\": 0.2, \"subsample\": 0.5},\n",
    "    ),\n",
    "    (\n",
    "        \"learning_rate=0.2, max_features=2\",\n",
    "        \"magenta\",\n",
    "        {\"learning_rate\": 0.2, \"max_features\": 2},\n",
    "    ),\n",
    "]:\n",
    "    params = dict(original_params)\n",
    "    params.update(setting)\n",
    "\n",
    "    clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # compute test set deviance\n",
    "    test_deviance = np.zeros((params[\"n_estimators\"],), dtype=np.float64)\n",
    "\n",
    "    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n",
    "        test_deviance[i] = 2 * log_loss(y_test, y_proba[:, 1])\n",
    "\n",
    "    plt.plot(\n",
    "        (np.arange(test_deviance.shape[0]) + 1)[::5],\n",
    "        test_deviance[::5],\n",
    "        \"-\",\n",
    "        color=color,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Boosting Iterations\")\n",
    "plt.ylabel(\"Test Set Deviance\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
