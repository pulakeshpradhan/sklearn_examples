{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================\n",
    "Gradient Boosting Out-of-Bag estimates\n",
    "======================================\n",
    "Out-of-bag (OOB) estimates can be a useful heuristic to estimate\n",
    "the \"optimal\" number of boosting iterations.\n",
    "OOB estimates are almost identical to cross-validation estimates but\n",
    "they can be computed on-the-fly without the need for repeated model\n",
    "fitting.\n",
    "OOB estimates are only available for Stochastic Gradient Boosting\n",
    "(i.e. ``subsample < 1.0``), the estimates are derived from the improvement\n",
    "in loss based on the examples not included in the bootstrap sample\n",
    "(the so-called out-of-bag examples).\n",
    "The OOB estimator is a pessimistic estimator of the true\n",
    "test loss, but remains a fairly good approximation for a small number of trees.\n",
    "The figure shows the cumulative sum of the negative OOB improvements\n",
    "as a function of the boosting iteration. As you can see, it tracks the test\n",
    "loss for the first hundred iterations but then diverges in a\n",
    "pessimistic way.\n",
    "The figure also shows the performance of 3-fold cross validation which\n",
    "usually gives a better estimate of the test loss\n",
    "but is computationally more demanding.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# Generate data (adapted from G. Ridgeway's gbm example)\n",
    "n_samples = 1000\n",
    "random_state = np.random.RandomState(13)\n",
    "x1 = random_state.uniform(size=n_samples)\n",
    "x2 = random_state.uniform(size=n_samples)\n",
    "x3 = random_state.randint(0, 4, size=n_samples)\n",
    "\n",
    "p = expit(np.sin(3 * x1) - 4 * x2 + x3)\n",
    "y = random_state.binomial(1, p, size=n_samples)\n",
    "\n",
    "X = np.c_[x1, x2, x3]\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=9)\n",
    "\n",
    "# Fit classifier with out-of-bag estimates\n",
    "params = {\n",
    "    \"n_estimators\": 1200,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"random_state\": 3,\n",
    "}\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "n_estimators = params[\"n_estimators\"]\n",
    "x = np.arange(n_estimators) + 1\n",
    "\n",
    "\n",
    "def heldout_score(clf, X_test, y_test):\n",
    "    \"\"\"compute deviance scores on ``X_test`` and ``y_test``.\"\"\"\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_proba in enumerate(clf.staged_predict_proba(X_test)):\n",
    "        score[i] = 2 * log_loss(y_test, y_proba[:, 1])\n",
    "    return score\n",
    "\n",
    "\n",
    "def cv_estimate(n_splits=None):\n",
    "    cv = KFold(n_splits=n_splits)\n",
    "    cv_clf = ensemble.GradientBoostingClassifier(**params)\n",
    "    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for train, test in cv.split(X_train, y_train):\n",
    "        cv_clf.fit(X_train[train], y_train[train])\n",
    "        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n",
    "    val_scores /= n_splits\n",
    "    return val_scores\n",
    "\n",
    "\n",
    "# Estimate best n_estimator using cross-validation\n",
    "cv_score = cv_estimate(3)\n",
    "\n",
    "# Compute best n_estimator for test data\n",
    "test_score = heldout_score(clf, X_test, y_test)\n",
    "\n",
    "# negative cumulative sum of oob improvements\n",
    "cumsum = -np.cumsum(clf.oob_improvement_)\n",
    "\n",
    "# min loss according to OOB\n",
    "oob_best_iter = x[np.argmin(cumsum)]\n",
    "\n",
    "# min loss according to test (normalize such that first loss is 0)\n",
    "test_score -= test_score[0]\n",
    "test_best_iter = x[np.argmin(test_score)]\n",
    "\n",
    "# min loss according to cv (normalize such that first loss is 0)\n",
    "cv_score -= cv_score[0]\n",
    "cv_best_iter = x[np.argmin(cv_score)]\n",
    "\n",
    "# color brew for the three curves\n",
    "oob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\n",
    "test_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\n",
    "cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n",
    "\n",
    "# line type for the three curves\n",
    "oob_line = \"dashed\"\n",
    "test_line = \"solid\"\n",
    "cv_line = \"dashdot\"\n",
    "\n",
    "# plot curves and vertical lines for best iterations\n",
    "plt.figure(figsize=(8, 4.8))\n",
    "plt.plot(x, cumsum, label=\"OOB loss\", color=oob_color, linestyle=oob_line)\n",
    "plt.plot(x, test_score, label=\"Test loss\", color=test_color, linestyle=test_line)\n",
    "plt.plot(x, cv_score, label=\"CV loss\", color=cv_color, linestyle=cv_line)\n",
    "plt.axvline(x=oob_best_iter, color=oob_color, linestyle=oob_line)\n",
    "plt.axvline(x=test_best_iter, color=test_color, linestyle=test_line)\n",
    "plt.axvline(x=cv_best_iter, color=cv_color, linestyle=cv_line)\n",
    "\n",
    "# add three vertical lines to xticks\n",
    "xticks = plt.xticks()\n",
    "xticks_pos = np.array(\n",
    "    xticks[0].tolist() + [oob_best_iter, cv_best_iter, test_best_iter]\n",
    ")\n",
    "xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) + [\"OOB\", \"CV\", \"Test\"])\n",
    "ind = np.argsort(xticks_pos)\n",
    "xticks_pos = xticks_pos[ind]\n",
    "xticks_label = xticks_label[ind]\n",
    "plt.xticks(xticks_pos, xticks_label, rotation=90)\n",
    "\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.ylabel(\"normalized loss\")\n",
    "plt.xlabel(\"number of iterations\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
