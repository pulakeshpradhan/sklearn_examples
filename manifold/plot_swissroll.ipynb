{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f02068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================================\n",
    "Swiss Roll And Swiss-Hole Reduction\n",
    "===================================\n",
    "This notebook seeks to compare two popular non-linear dimensionality\n",
    "techniques, T-distributed Stochastic Neighbor Embedding (t-SNE) and\n",
    "Locally Linear Embedding (LLE), on the classic Swiss Roll dataset.\n",
    "Then, we will explore how they both deal with the addition of a hole\n",
    "in the data.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Swiss Roll\n",
    "# ---------------------------------------------------\n",
    "#\n",
    "# We start by generating the Swiss Roll dataset.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, manifold\n",
    "\n",
    "sr_points, sr_color = datasets.make_swiss_roll(n_samples=1500, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Now, let's take a look at our data:\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(\n",
    "    sr_points[:, 0], sr_points[:, 1], sr_points[:, 2], c=sr_color, s=50, alpha=0.8\n",
    ")\n",
    "ax.set_title(\"Swiss Roll in Ambient Space\")\n",
    "ax.view_init(azim=-66, elev=12)\n",
    "_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n",
    "\n",
    "# %%\n",
    "# Computing the LLE and t-SNE embeddings, we find that LLE seems to unroll the\n",
    "# Swiss Roll pretty effectively. t-SNE on the other hand, is able\n",
    "# to preserve the general structure of the data, but, poorly represents the\n",
    "# continuous nature of our original data. Instead, it seems to unnecessarily\n",
    "# clump sections of points together.\n",
    "\n",
    "sr_lle, sr_err = manifold.locally_linear_embedding(\n",
    "    sr_points, n_neighbors=12, n_components=2\n",
    ")\n",
    "\n",
    "sr_tsne = manifold.TSNE(n_components=2, perplexity=40, random_state=0).fit_transform(\n",
    "    sr_points\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8), nrows=2)\n",
    "axs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\n",
    "axs[0].set_title(\"LLE Embedding of Swiss Roll\")\n",
    "axs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n",
    "_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n",
    "\n",
    "# %%\n",
    "# .. note::\n",
    "#\n",
    "#     LLE seems to be stretching the points from the center (purple)\n",
    "#     of the swiss roll. However, we observe that this is simply a byproduct\n",
    "#     of how the data was generated. There is a higher density of points near the\n",
    "#     center of the roll, which ultimately affects how LLE reconstructs the\n",
    "#     data in a lower dimension.\n",
    "\n",
    "# %%\n",
    "# Swiss-Hole\n",
    "# ---------------------------------------------------\n",
    "#\n",
    "# Now let's take a look at how both algorithms deal with us adding a hole to\n",
    "# the data. First, we generate the Swiss-Hole dataset and plot it:\n",
    "\n",
    "sh_points, sh_color = datasets.make_swiss_roll(\n",
    "    n_samples=1500, hole=True, random_state=0\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(\n",
    "    sh_points[:, 0], sh_points[:, 1], sh_points[:, 2], c=sh_color, s=50, alpha=0.8\n",
    ")\n",
    "ax.set_title(\"Swiss-Hole in Ambient Space\")\n",
    "ax.view_init(azim=-66, elev=12)\n",
    "_ = ax.text2D(0.8, 0.05, s=\"n_samples=1500\", transform=ax.transAxes)\n",
    "\n",
    "# %%\n",
    "# Computing the LLE and t-SNE embeddings, we obtain similar results to the\n",
    "# Swiss Roll. LLE very capably unrolls the data and even preserves\n",
    "# the hole. t-SNE, again seems to clump sections of points together, but, we\n",
    "# note that it preserves the general topology of the original data.\n",
    "\n",
    "\n",
    "sh_lle, sh_err = manifold.locally_linear_embedding(\n",
    "    sh_points, n_neighbors=12, n_components=2\n",
    ")\n",
    "\n",
    "sh_tsne = manifold.TSNE(\n",
    "    n_components=2, perplexity=40, init=\"random\", random_state=0\n",
    ").fit_transform(sh_points)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8), nrows=2)\n",
    "axs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\n",
    "axs[0].set_title(\"LLE Embedding of Swiss-Hole\")\n",
    "axs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n",
    "_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n",
    "\n",
    "# %%\n",
    "#\n",
    "# Concluding remarks\n",
    "# ------------------\n",
    "#\n",
    "# We note that t-SNE benefits from testing more combinations of parameters.\n",
    "# Better results could probably have been obtained by better tuning these\n",
    "# parameters.\n",
    "#\n",
    "# We observe that, as seen in the \"Manifold learning on\n",
    "# handwritten digits\" example, t-SNE generally performs better than LLE\n",
    "# on real world data.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
