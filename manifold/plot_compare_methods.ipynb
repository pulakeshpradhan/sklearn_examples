{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================\n",
    "Comparison of Manifold Learning methods\n",
    "=========================================\n",
    "\n",
    "An illustration of dimensionality reduction on the S-curve dataset\n",
    "with various manifold learning methods.\n",
    "\n",
    "For a discussion and comparison of these algorithms, see the\n",
    ":ref:`manifold module page <manifold>`\n",
    "\n",
    "For a similar example, where the methods are applied to a\n",
    "sphere dataset, see :ref:`sphx_glr_auto_examples_manifold_plot_manifold_sphere.py`\n",
    "\n",
    "Note that the purpose of the MDS is to find a low-dimensional\n",
    "representation of the data (here 2D) in which the distances respect well\n",
    "the distances in the original high-dimensional space, unlike other\n",
    "manifold-learning algorithms, it does not seeks an isotropic\n",
    "representation of the data in the low-dimensional space.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Dataset preparation\n",
    "# -------------------\n",
    "#\n",
    "# We start by generating the S-curve dataset.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "import mpl_toolkits.mplot3d  # noqa: F401\n",
    "from matplotlib import ticker\n",
    "\n",
    "from sklearn import datasets, manifold\n",
    "\n",
    "n_samples = 1500\n",
    "S_points, S_color = datasets.make_s_curve(n_samples, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Let's look at the original data. Also define some helping\n",
    "# functions, which we will use further on.\n",
    "\n",
    "\n",
    "def plot_3d(points, points_color, title):\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6),\n",
    "        facecolor=\"white\",\n",
    "        tight_layout=True,\n",
    "        subplot_kw={\"projection\": \"3d\"},\n",
    "    )\n",
    "    fig.suptitle(title, size=16)\n",
    "    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)\n",
    "    ax.view_init(azim=-60, elev=9)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    fig.colorbar(col, ax=ax, orientation=\"horizontal\", shrink=0.6, aspect=60, pad=0.01)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_2d(points, points_color, title):\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), facecolor=\"white\", constrained_layout=True)\n",
    "    fig.suptitle(title, size=16)\n",
    "    add_2d_scatter(ax, points, points_color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_2d_scatter(ax, points, points_color, title=None):\n",
    "    x, y = points.T\n",
    "    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(ticker.NullFormatter())\n",
    "\n",
    "\n",
    "plot_3d(S_points, S_color, \"Original S-curve samples\")\n",
    "\n",
    "# %%\n",
    "# Define algorithms for the manifold learning\n",
    "# -------------------------------------------\n",
    "#\n",
    "# Manifold learning is an approach to non-linear dimensionality reduction.\n",
    "# Algorithms for this task are based on the idea that the dimensionality of\n",
    "# many data sets is only artificially high.\n",
    "#\n",
    "# Read more in the :ref:`User Guide <manifold>`.\n",
    "\n",
    "n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\n",
    "n_components = 2  # number of coordinates for the manifold\n",
    "\n",
    "# %%\n",
    "# Locally Linear Embeddings\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# Locally linear embedding (LLE) can be thought of as a series of local\n",
    "# Principal Component Analyses which are globally compared to find the\n",
    "# best non-linear embedding.\n",
    "# Read more in the :ref:`User Guide <locally_linear_embedding>`.\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "    \"n_components\": n_components,\n",
    "    \"eigen_solver\": \"auto\",\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "lle_standard = manifold.LocallyLinearEmbedding(method=\"standard\", **params)\n",
    "S_standard = lle_standard.fit_transform(S_points)\n",
    "\n",
    "lle_ltsa = manifold.LocallyLinearEmbedding(method=\"ltsa\", **params)\n",
    "S_ltsa = lle_ltsa.fit_transform(S_points)\n",
    "\n",
    "lle_hessian = manifold.LocallyLinearEmbedding(method=\"hessian\", **params)\n",
    "S_hessian = lle_hessian.fit_transform(S_points)\n",
    "\n",
    "lle_mod = manifold.LocallyLinearEmbedding(method=\"modified\", **params)\n",
    "S_mod = lle_mod.fit_transform(S_points)\n",
    "\n",
    "# %%\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=2, ncols=2, figsize=(7, 7), facecolor=\"white\", constrained_layout=True\n",
    ")\n",
    "fig.suptitle(\"Locally Linear Embeddings\", size=16)\n",
    "\n",
    "lle_methods = [\n",
    "    (\"Standard locally linear embedding\", S_standard),\n",
    "    (\"Local tangent space alignment\", S_ltsa),\n",
    "    (\"Hessian eigenmap\", S_hessian),\n",
    "    (\"Modified locally linear embedding\", S_mod),\n",
    "]\n",
    "for ax, method in zip(axs.flat, lle_methods):\n",
    "    name, points = method\n",
    "    add_2d_scatter(ax, points, S_color, name)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Isomap Embedding\n",
    "# ^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# Non-linear dimensionality reduction through Isometric Mapping.\n",
    "# Isomap seeks a lower-dimensional embedding which maintains geodesic\n",
    "# distances between all points. Read more in the :ref:`User Guide <isomap>`.\n",
    "\n",
    "isomap = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components, p=1)\n",
    "S_isomap = isomap.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_isomap, S_color, \"Isomap Embedding\")\n",
    "\n",
    "# %%\n",
    "# Multidimensional scaling\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# Multidimensional scaling (MDS) seeks a low-dimensional representation\n",
    "# of the data in which the distances respect well the distances in the\n",
    "# original high-dimensional space.\n",
    "# Read more in the :ref:`User Guide <multidimensional_scaling>`.\n",
    "\n",
    "md_scaling = manifold.MDS(\n",
    "    n_components=n_components,\n",
    "    max_iter=50,\n",
    "    n_init=4,\n",
    "    random_state=0,\n",
    "    normalized_stress=False,\n",
    ")\n",
    "S_scaling = md_scaling.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_scaling, S_color, \"Multidimensional scaling\")\n",
    "\n",
    "# %%\n",
    "# Spectral embedding for non-linear dimensionality reduction\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# This implementation uses Laplacian Eigenmaps, which finds a low dimensional\n",
    "# representation of the data using a spectral decomposition of the graph Laplacian.\n",
    "# Read more in the :ref:`User Guide <spectral_embedding>`.\n",
    "\n",
    "spectral = manifold.SpectralEmbedding(\n",
    "    n_components=n_components, n_neighbors=n_neighbors, random_state=42\n",
    ")\n",
    "S_spectral = spectral.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_spectral, S_color, \"Spectral Embedding\")\n",
    "\n",
    "# %%\n",
    "# T-distributed Stochastic Neighbor Embedding\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "#\n",
    "# It converts similarities between data points to joint probabilities and\n",
    "# tries to minimize the Kullback-Leibler divergence between the joint probabilities\n",
    "# of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost\n",
    "# function that is not convex, i.e. with different initializations we can get\n",
    "# different results. Read more in the :ref:`User Guide <t_sne>`.\n",
    "\n",
    "t_sne = manifold.TSNE(\n",
    "    n_components=n_components,\n",
    "    perplexity=30,\n",
    "    init=\"random\",\n",
    "    max_iter=250,\n",
    "    random_state=0,\n",
    ")\n",
    "S_t_sne = t_sne.fit_transform(S_points)\n",
    "\n",
    "plot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n",
    "\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
