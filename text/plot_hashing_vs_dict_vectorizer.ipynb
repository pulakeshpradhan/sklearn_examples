{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fee625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===========================================\n",
    "FeatureHasher and DictVectorizer Comparison\n",
    "===========================================\n",
    "\n",
    "In this example we illustrate text vectorization, which is the process of\n",
    "representing non-numerical input data (such as dictionaries or text documents)\n",
    "as vectors of real numbers.\n",
    "\n",
    "We first compare :func:`~sklearn.feature_extraction.FeatureHasher` and\n",
    ":func:`~sklearn.feature_extraction.DictVectorizer` by using both methods to\n",
    "vectorize text documents that are preprocessed (tokenized) with the help of a\n",
    "custom Python function.\n",
    "\n",
    "Later we introduce and analyze the text-specific vectorizers\n",
    ":func:`~sklearn.feature_extraction.text.HashingVectorizer`,\n",
    ":func:`~sklearn.feature_extraction.text.CountVectorizer` and\n",
    ":func:`~sklearn.feature_extraction.text.TfidfVectorizer` that handle both the\n",
    "tokenization and the assembling of the feature matrix within a single class.\n",
    "\n",
    "The objective of the example is to demonstrate the usage of text vectorization\n",
    "API and to compare their processing time. See the example scripts\n",
    ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
    "and :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py` for actual\n",
    "learning on text documents.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Load Data\n",
    "# ---------\n",
    "#\n",
    "# We load data from :ref:`20newsgroups_dataset`, which comprises around\n",
    "# 18000 newsgroups posts on 20 topics split in two subsets: one for training and\n",
    "# one for testing. For the sake of simplicity and reducing the computational\n",
    "# cost, we select a subset of 7 topics and use the training set only.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.sys.ibm.pc.hardware\",\n",
    "    \"misc.forsale\",\n",
    "    \"rec.autos\",\n",
    "    \"sci.space\",\n",
    "    \"talk.religion.misc\",\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups training data\")\n",
    "raw_data, _ = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\n",
    "data_size_mb = sum(len(s.encode(\"utf-8\")) for s in raw_data) / 1e6\n",
    "print(f\"{len(raw_data)} documents - {data_size_mb:.3f}MB\")\n",
    "\n",
    "# %%\n",
    "# Define preprocessing functions\n",
    "# ------------------------------\n",
    "#\n",
    "# A token may be a word, part of a word or anything comprised between spaces or\n",
    "# symbols in a string. Here we define a function that extracts the tokens using\n",
    "# a simple regular expression (regex) that matches Unicode word characters. This\n",
    "# includes most characters that can be part of a word in any language, as well\n",
    "# as numbers and the underscore:\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    \"\"\"Extract tokens from doc.\n",
    "\n",
    "    This uses a simple regex that matches word characters to break strings\n",
    "    into tokens. For a more principled approach, see CountVectorizer or\n",
    "    TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))\n",
    "\n",
    "\n",
    "list(tokenize(\"This is a simple example, isn't it?\"))\n",
    "\n",
    "# %%\n",
    "# We define an additional function that counts the (frequency of) occurrence of\n",
    "# each token in a given document. It returns a frequency dictionary to be used\n",
    "# by the vectorizers.\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def token_freqs(doc):\n",
    "    \"\"\"Extract a dict mapping tokens from doc to their occurrences.\"\"\"\n",
    "\n",
    "    freq = defaultdict(int)\n",
    "    for tok in tokenize(doc):\n",
    "        freq[tok] += 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "token_freqs(\"That is one example, but this is another one\")\n",
    "\n",
    "# %%\n",
    "# Observe in particular that the repeated token `\"is\"` is counted twice for\n",
    "# instance.\n",
    "#\n",
    "# Breaking a text document into word tokens, potentially losing the order\n",
    "# information between the words in a sentence is often called a `Bag of Words\n",
    "# representation <https://en.wikipedia.org/wiki/Bag-of-words_model>`_.\n",
    "\n",
    "# %%\n",
    "# DictVectorizer\n",
    "# --------------\n",
    "#\n",
    "# First we benchmark the :func:`~sklearn.feature_extraction.DictVectorizer`,\n",
    "# then we compare it to :func:`~sklearn.feature_extraction.FeatureHasher` as\n",
    "# both of them receive dictionaries as input.\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dict_count_vectorizers = defaultdict(list)\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(\n",
    "    vectorizer.__class__.__name__ + \"\\non freq dicts\"\n",
    ")\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\n",
    "\n",
    "# %%\n",
    "# The actual mapping from text token to column index is explicitly stored in\n",
    "# the `.vocabulary_` attribute which is a potentially very large Python\n",
    "# dictionary:\n",
    "type(vectorizer.vocabulary_)\n",
    "\n",
    "# %%\n",
    "len(vectorizer.vocabulary_)\n",
    "\n",
    "# %%\n",
    "vectorizer.vocabulary_[\"example\"]\n",
    "\n",
    "# %%\n",
    "# FeatureHasher\n",
    "# -------------\n",
    "#\n",
    "# Dictionaries take up a large amount of storage space and grow in size as the\n",
    "# training set grows. Instead of growing the vectors along with a dictionary,\n",
    "# feature hashing builds a vector of pre-defined length by applying a hash\n",
    "# function `h` to the features (e.g., tokens), then using the hash values\n",
    "# directly as feature indices and updating the resulting vector at those\n",
    "# indices. When the feature space is not large enough, hashing functions tend to\n",
    "# map distinct values to the same hash code (hash collisions). As a result, it\n",
    "# is impossible to determine what object generated any particular hash code.\n",
    "#\n",
    "# Because of the above it is impossible to recover the original tokens from the\n",
    "# feature matrix and the best approach to estimate the number of unique terms in\n",
    "# the original dictionary is to count the number of active columns in the\n",
    "# encoded feature matrix. For such a purpose we define the following function:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def n_nonzero_columns(X):\n",
    "    \"\"\"Number of columns with at least one non-zero value in a CSR matrix.\n",
    "\n",
    "    This is useful to count the number of features columns that are effectively\n",
    "    active when using the FeatureHasher.\n",
    "    \"\"\"\n",
    "    return len(np.unique(X.nonzero()[1]))\n",
    "\n",
    "\n",
    "# %%\n",
    "# The default number of features for the\n",
    "# :func:`~sklearn.feature_extraction.FeatureHasher` is 2**20. Here we set\n",
    "# `n_features = 2**18` to illustrate hash collisions.\n",
    "#\n",
    "# **FeatureHasher on frequency dictionaries**\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "t0 = time()\n",
    "hasher = FeatureHasher(n_features=2**18)\n",
    "X = hasher.transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(\n",
    "    hasher.__class__.__name__ + \"\\non freq dicts\"\n",
    ")\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {n_nonzero_columns(X)} unique tokens\")\n",
    "\n",
    "# %%\n",
    "# The number of unique tokens when using the\n",
    "# :func:`~sklearn.feature_extraction.FeatureHasher` is lower than those obtained\n",
    "# using the :func:`~sklearn.feature_extraction.DictVectorizer`. This is due to\n",
    "# hash collisions.\n",
    "#\n",
    "# The number of collisions can be reduced by increasing the feature space.\n",
    "# Notice that the speed of the vectorizer does not change significantly when\n",
    "# setting a large number of features, though it causes larger coefficient\n",
    "# dimensions and then requires more memory usage to store them, even if a\n",
    "# majority of them is inactive.\n",
    "\n",
    "t0 = time()\n",
    "hasher = FeatureHasher(n_features=2**22)\n",
    "X = hasher.transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {n_nonzero_columns(X)} unique tokens\")\n",
    "\n",
    "# %%\n",
    "# We confirm that the number of unique tokens gets closer to the number of\n",
    "# unique terms found by the :func:`~sklearn.feature_extraction.DictVectorizer`.\n",
    "#\n",
    "# **FeatureHasher on raw tokens**\n",
    "#\n",
    "# Alternatively, one can set `input_type=\"string\"` in the\n",
    "# :func:`~sklearn.feature_extraction.FeatureHasher` to vectorize the strings\n",
    "# output directly from the customized `tokenize` function. This is equivalent to\n",
    "# passing a dictionary with an implied frequency of 1 for each feature name.\n",
    "\n",
    "t0 = time()\n",
    "hasher = FeatureHasher(n_features=2**18, input_type=\"string\")\n",
    "X = hasher.transform(tokenize(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(\n",
    "    hasher.__class__.__name__ + \"\\non raw tokens\"\n",
    ")\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {n_nonzero_columns(X)} unique tokens\")\n",
    "\n",
    "# %%\n",
    "# We now plot the speed of the above methods for vectorizing.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "y_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\n",
    "ax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\n",
    "ax.invert_yaxis()\n",
    "_ = ax.set_xlabel(\"speed (MB/s)\")\n",
    "\n",
    "# %%\n",
    "# In both cases :func:`~sklearn.feature_extraction.FeatureHasher` is\n",
    "# approximately twice as fast as\n",
    "# :func:`~sklearn.feature_extraction.DictVectorizer`. This is handy when dealing\n",
    "# with large amounts of data, with the downside of losing the invertibility of\n",
    "# the transformation, which in turn makes the interpretation of a model a more\n",
    "# complex task.\n",
    "#\n",
    "# The `FeatureHeasher` with `input_type=\"string\"` is slightly faster than the\n",
    "# variant that works on frequency dict because it does not count repeated\n",
    "# tokens: each token is implicitly counted once, even if it was repeated.\n",
    "# Depending on the downstream machine learning task, it can be a limitation or\n",
    "# not.\n",
    "#\n",
    "# Comparison with special purpose text vectorizers\n",
    "# ------------------------------------------------\n",
    "#\n",
    "# :func:`~sklearn.feature_extraction.text.CountVectorizer` accepts raw data as\n",
    "# it internally implements tokenization and occurrence counting. It is similar\n",
    "# to the :func:`~sklearn.feature_extraction.DictVectorizer` when used along with\n",
    "# the customized function `token_freqs` as done in the previous section. The\n",
    "# difference being that :func:`~sklearn.feature_extraction.text.CountVectorizer`\n",
    "# is more flexible. In particular it accepts various regex patterns through the\n",
    "# `token_pattern` parameter.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\n",
    "\n",
    "# %%\n",
    "# We see that using the :func:`~sklearn.feature_extraction.text.CountVectorizer`\n",
    "# implementation is approximately twice as fast as using the\n",
    "# :func:`~sklearn.feature_extraction.DictVectorizer` along with the simple\n",
    "# function we defined for mapping the tokens. The reason is that\n",
    "# :func:`~sklearn.feature_extraction.text.CountVectorizer` is optimized by\n",
    "# reusing a compiled regular expression for the full training set instead of\n",
    "# creating one per document as done in our naive tokenize function.\n",
    "#\n",
    "# Now we make a similar experiment with the\n",
    "# :func:`~sklearn.feature_extraction.text.HashingVectorizer`, which is\n",
    "# equivalent to combining the \"hashing trick\" implemented by the\n",
    "# :func:`~sklearn.feature_extraction.FeatureHasher` class and the text\n",
    "# preprocessing and tokenization of the\n",
    "# :func:`~sklearn.feature_extraction.text.CountVectorizer`.\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = HashingVectorizer(n_features=2**18)\n",
    "vectorizer.fit_transform(raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "\n",
    "# %%\n",
    "# We can observe that this is the fastest text tokenization strategy so far,\n",
    "# assuming that the downstream machine learning task can tolerate a few\n",
    "# collisions.\n",
    "#\n",
    "# TfidfVectorizer\n",
    "# ---------------\n",
    "#\n",
    "# In a large text corpus, some words appear with higher frequency (e.g. \"the\",\n",
    "# \"a\", \"is\" in English) and do not carry meaningful information about the actual\n",
    "# contents of a document. If we were to feed the word count data directly to a\n",
    "# classifier, those very common terms would shadow the frequencies of rarer yet\n",
    "# more informative terms. In order to re-weight the count features into floating\n",
    "# point values suitable for usage by a classifier it is very common to use the\n",
    "# tf-idf transform as implemented by the\n",
    "# :func:`~sklearn.feature_extraction.text.TfidfTransformer`. TF stands for\n",
    "# \"term-frequency\" while \"tf-idf\" means term-frequency times inverse\n",
    "# document-frequency.\n",
    "#\n",
    "# We now benchmark the :func:`~sklearn.feature_extraction.text.TfidfVectorizer`,\n",
    "# which is equivalent to combining the tokenization and occurrence counting of\n",
    "# the :func:`~sklearn.feature_extraction.text.CountVectorizer` along with the\n",
    "# normalizing and weighting from a\n",
    "# :func:`~sklearn.feature_extraction.text.TfidfTransformer`.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(raw_data)\n",
    "duration = time() - t0\n",
    "dict_count_vectorizers[\"vectorizer\"].append(vectorizer.__class__.__name__)\n",
    "dict_count_vectorizers[\"speed\"].append(data_size_mb / duration)\n",
    "print(f\"done in {duration:.3f} s at {data_size_mb / duration:.1f} MB/s\")\n",
    "print(f\"Found {len(vectorizer.get_feature_names_out())} unique terms\")\n",
    "\n",
    "# %%\n",
    "# Summary\n",
    "# -------\n",
    "# Let's conclude this notebook by summarizing all the recorded processing speeds\n",
    "# in a single plot:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "y_pos = np.arange(len(dict_count_vectorizers[\"vectorizer\"]))\n",
    "ax.barh(y_pos, dict_count_vectorizers[\"speed\"], align=\"center\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(dict_count_vectorizers[\"vectorizer\"])\n",
    "ax.invert_yaxis()\n",
    "_ = ax.set_xlabel(\"speed (MB/s)\")\n",
    "\n",
    "# %%\n",
    "# Notice from the plot that\n",
    "# :func:`~sklearn.feature_extraction.text.TfidfVectorizer` is slightly slower\n",
    "# than :func:`~sklearn.feature_extraction.text.CountVectorizer` because of the\n",
    "# extra operation induced by the\n",
    "# :func:`~sklearn.feature_extraction.text.TfidfTransformer`.\n",
    "#\n",
    "# Also notice that, by setting the number of features `n_features = 2**18`, the\n",
    "# :func:`~sklearn.feature_extraction.text.HashingVectorizer` performs better\n",
    "# than the :func:`~sklearn.feature_extraction.text.CountVectorizer` at the\n",
    "# expense of inversibility of the transformation due to hash collisions.\n",
    "#\n",
    "# We highlight that :func:`~sklearn.feature_extraction.text.CountVectorizer` and\n",
    "# :func:`~sklearn.feature_extraction.text.HashingVectorizer` perform better than\n",
    "# their equivalent :func:`~sklearn.feature_extraction.DictVectorizer` and\n",
    "# :func:`~sklearn.feature_extraction.FeatureHasher` on manually tokenized\n",
    "# documents since the internal tokenization step of the former vectorizers\n",
    "# compiles a regular expression once and then reuses it for all the documents.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
