{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca28b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=======================================\n",
    "Clustering text documents using k-means\n",
    "=======================================\n",
    "\n",
    "This is an example showing how the scikit-learn API can be used to cluster\n",
    "documents by topics using a `Bag of Words approach\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_.\n",
    "\n",
    "Two algorithms are demonstrated, namely :class:`~sklearn.cluster.KMeans` and its more\n",
    "scalable variant, :class:`~sklearn.cluster.MiniBatchKMeans`. Additionally,\n",
    "latent semantic analysis is used to reduce dimensionality and discover latent\n",
    "patterns in the data.\n",
    "\n",
    "This example uses two different text vectorizers: a\n",
    ":class:`~sklearn.feature_extraction.text.TfidfVectorizer` and a\n",
    ":class:`~sklearn.feature_extraction.text.HashingVectorizer`. See the example\n",
    "notebook :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
    "for more information on vectorizers and a comparison of their processing times.\n",
    "\n",
    "For document analysis via a supervised learning approach, see the example script\n",
    ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Loading text data\n",
    "# =================\n",
    "#\n",
    "# We load data from :ref:`20newsgroups_dataset`, which comprises around 18,000\n",
    "# newsgroups posts on 20 topics. For illustrative purposes and to reduce the\n",
    "# computational cost, we select a subset of 4 topics only accounting for around\n",
    "# 3,400 documents. See the example\n",
    "# :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
    "# to gain intuition on the overlap of such topics.\n",
    "#\n",
    "# Notice that, by default, the text samples contain some message metadata such\n",
    "# as `\"headers\"`, `\"footers\"` (signatures) and `\"quotes\"` to other posts. We use\n",
    "# the `remove` parameter from :func:`~sklearn.datasets.fetch_20newsgroups` to\n",
    "# strip those features and have a more sensible clustering problem.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} documents - {true_k} categories\")\n",
    "\n",
    "# %%\n",
    "# Quantifying the quality of clustering results\n",
    "# =============================================\n",
    "#\n",
    "# In this section we define a function to score different clustering pipelines\n",
    "# using several metrics.\n",
    "#\n",
    "# Clustering algorithms are fundamentally unsupervised learning methods.\n",
    "# However, since we happen to have class labels for this specific dataset, it is\n",
    "# possible to use evaluation metrics that leverage this \"supervised\" ground\n",
    "# truth information to quantify the quality of the resulting clusters. Examples\n",
    "# of such metrics are the following:\n",
    "#\n",
    "# - homogeneity, which quantifies how much clusters contain only members of a\n",
    "#   single class;\n",
    "#\n",
    "# - completeness, which quantifies how much members of a given class are\n",
    "#   assigned to the same clusters;\n",
    "#\n",
    "# - V-measure, the harmonic mean of completeness and homogeneity;\n",
    "#\n",
    "# - Rand-Index, which measures how frequently pairs of data points are grouped\n",
    "#   consistently according to the result of the clustering algorithm and the\n",
    "#   ground truth class assignment;\n",
    "#\n",
    "# - Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster\n",
    "#   assignment have an ARI of 0.0 in expectation.\n",
    "#\n",
    "# If the ground truth labels are not known, evaluation can only be performed\n",
    "# using the model results itself. In that case, the Silhouette Coefficient comes in\n",
    "# handy. See :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`\n",
    "# for an example on how to do it.\n",
    "#\n",
    "# For more reference, see :ref:`clustering_evaluation`.\n",
    "\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n",
    "\n",
    "\n",
    "# %%\n",
    "# K-means clustering on text features\n",
    "# ===================================\n",
    "#\n",
    "# Two feature extraction methods are used in this example:\n",
    "#\n",
    "# - :class:`~sklearn.feature_extraction.text.TfidfVectorizer` uses an in-memory\n",
    "#   vocabulary (a Python dict) to map the most frequent words to features\n",
    "#   indices and hence compute a word occurrence frequency (sparse) matrix. The\n",
    "#   word frequencies are then reweighted using the Inverse Document Frequency\n",
    "#   (IDF) vector collected feature-wise over the corpus.\n",
    "#\n",
    "# - :class:`~sklearn.feature_extraction.text.HashingVectorizer` hashes word\n",
    "#   occurrences to a fixed dimensional space, possibly with collisions. The word\n",
    "#   count vectors are then normalized to each have l2-norm equal to one\n",
    "#   (projected to the euclidean unit-sphere) which seems to be important for\n",
    "#   k-means to work in high dimensional space.\n",
    "#\n",
    "# Furthermore it is possible to post-process those extracted features using\n",
    "# dimensionality reduction. We will explore the impact of those choices on the\n",
    "# clustering quality in the following.\n",
    "#\n",
    "# Feature Extraction using TfidfVectorizer\n",
    "# ----------------------------------------\n",
    "#\n",
    "# We first benchmark the estimators using a dictionary vectorizer along with an\n",
    "# IDF normalization as provided by\n",
    "# :class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n",
    "\n",
    "# %%\n",
    "# After ignoring terms that appear in more than 50% of the documents (as set by\n",
    "# `max_df=0.5`) and terms that are not present in at least 5 documents (set by\n",
    "# `min_df=5`), the resulting number of unique terms `n_features` is around\n",
    "# 8,000. We can additionally quantify the sparsity of the `X_tfidf` matrix as\n",
    "# the fraction of non-zero entries divided by the total number of elements.\n",
    "\n",
    "print(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\")\n",
    "\n",
    "# %%\n",
    "# We find that around 0.7% of the entries of the `X_tfidf` matrix are non-zero.\n",
    "#\n",
    "# .. _kmeans_sparse_high_dim:\n",
    "#\n",
    "# Clustering sparse data with k-means\n",
    "# -----------------------------------\n",
    "#\n",
    "# As both :class:`~sklearn.cluster.KMeans` and\n",
    "# :class:`~sklearn.cluster.MiniBatchKMeans` optimize a non-convex objective\n",
    "# function, their clustering is not guaranteed to be optimal for a given random\n",
    "# init. Even further, on sparse high-dimensional data such as text vectorized\n",
    "# using the Bag of Words approach, k-means can initialize centroids on extremely\n",
    "# isolated data points. Those data points can stay their own centroids all\n",
    "# along.\n",
    "#\n",
    "# The following code illustrates how the previous phenomenon can sometimes lead\n",
    "# to highly imbalanced clusters, depending on the random initialization:\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for seed in range(5):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=true_k,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=seed,\n",
    "    ).fit(X_tfidf)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "print()\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# To avoid this problem, one possibility is to increase the number of runs with\n",
    "# independent random initiations `n_init`. In such case the clustering with the\n",
    "# best inertia (objective function of k-means) is chosen.\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\non tf-idf vectors\")\n",
    "\n",
    "# %%\n",
    "# All those clustering evaluation metrics have a maximum value of 1.0 (for a\n",
    "# perfect clustering result). Higher values are better. Values of the Adjusted\n",
    "# Rand-Index close to 0.0 correspond to a random labeling. Notice from the\n",
    "# scores above that the cluster assignment is indeed well above chance level,\n",
    "# but the overall quality can certainly improve.\n",
    "#\n",
    "# Keep in mind that the class labels may not reflect accurately the document\n",
    "# topics and therefore metrics that use labels are not necessarily the best to\n",
    "# evaluate the quality of our clustering pipeline.\n",
    "#\n",
    "# Performing dimensionality reduction using LSA\n",
    "# ---------------------------------------------\n",
    "#\n",
    "# A `n_init=1` can still be used as long as the dimension of the vectorized\n",
    "# space is reduced first to make k-means more stable. For such purpose we use\n",
    "# :class:`~sklearn.decomposition.TruncatedSVD`, which works on term count/tf-idf\n",
    "# matrices. Since SVD results are not normalized, we redo the normalization to\n",
    "# improve the :class:`~sklearn.cluster.KMeans` result. Using SVD to reduce the\n",
    "# dimensionality of TF-IDF document vectors is often known as `latent semantic\n",
    "# analysis <https://en.wikipedia.org/wiki/Latent_semantic_analysis>`_ (LSA) in\n",
    "# the information retrieval and text mining literature.\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "# %%\n",
    "# Using a single initialization means the processing time will be reduced for\n",
    "# both :class:`~sklearn.cluster.KMeans` and\n",
    "# :class:`~sklearn.cluster.MiniBatchKMeans`.\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\nwith LSA on tf-idf vectors\")\n",
    "\n",
    "# %%\n",
    "# We can observe that clustering on the LSA representation of the document is\n",
    "# significantly faster (both because of `n_init=1` and because the\n",
    "# dimensionality of the LSA feature space is much smaller). Furthermore, all the\n",
    "# clustering evaluation metrics have improved. We repeat the experiment with\n",
    "# :class:`~sklearn.cluster.MiniBatchKMeans`.\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=true_k,\n",
    "    n_init=1,\n",
    "    init_size=1000,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(\n",
    "    minibatch_kmeans,\n",
    "    X_lsa,\n",
    "    name=\"MiniBatchKMeans\\nwith LSA on tf-idf vectors\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Top terms per cluster\n",
    "# ---------------------\n",
    "#\n",
    "# Since :class:`~sklearn.feature_extraction.text.TfidfVectorizer` can be\n",
    "# inverted we can identify the cluster centers, which provide an intuition of\n",
    "# the most influential words **for each cluster**. See the example script\n",
    "# :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
    "# for a comparison with the most predictive words **for each target class**.\n",
    "\n",
    "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(f\"Cluster {i}: \", end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"{terms[ind]} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "# %%\n",
    "# HashingVectorizer\n",
    "# -----------------\n",
    "# An alternative vectorization can be done using a\n",
    "# :class:`~sklearn.feature_extraction.text.HashingVectorizer` instance, which\n",
    "# does not provide IDF weighting as this is a stateless model (the fit method\n",
    "# does nothing). When IDF weighting is needed it can be added by pipelining the\n",
    "# :class:`~sklearn.feature_extraction.text.HashingVectorizer` output to a\n",
    "# :class:`~sklearn.feature_extraction.text.TfidfTransformer` instance. In this\n",
    "# case we also add LSA to the pipeline to reduce the dimension and sparcity of\n",
    "# the hashed vector space.\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "lsa_vectorizer = make_pipeline(\n",
    "    HashingVectorizer(stop_words=\"english\", n_features=50_000),\n",
    "    TfidfTransformer(),\n",
    "    TruncatedSVD(n_components=100, random_state=0),\n",
    "    Normalizer(copy=False),\n",
    ")\n",
    "\n",
    "t0 = time()\n",
    "X_hashed_lsa = lsa_vectorizer.fit_transform(dataset.data)\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "\n",
    "# %%\n",
    "# One can observe that the LSA step takes a relatively long time to fit,\n",
    "# especially with hashed vectors. The reason is that a hashed space is typically\n",
    "# large (set to `n_features=50_000` in this example). One can try lowering the\n",
    "# number of features at the expense of having a larger fraction of features with\n",
    "# hash collisions as shown in the example notebook\n",
    "# :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
    "#\n",
    "# We now fit and evaluate the `kmeans` and `minibatch_kmeans` instances on this\n",
    "# hashed-lsa-reduced data:\n",
    "\n",
    "fit_and_evaluate(kmeans, X_hashed_lsa, name=\"KMeans\\nwith LSA on hashed vectors\")\n",
    "\n",
    "# %%\n",
    "fit_and_evaluate(\n",
    "    minibatch_kmeans,\n",
    "    X_hashed_lsa,\n",
    "    name=\"MiniBatchKMeans\\nwith LSA on hashed vectors\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Both methods lead to good results that are similar to running the same models\n",
    "# on the traditional LSA vectors (without hashing).\n",
    "#\n",
    "# Clustering evaluation summary\n",
    "# ==============================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
    "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
    "\n",
    "df.drop(\n",
    "    [\"train_time\"],\n",
    "    axis=\"columns\",\n",
    ").plot.barh(ax=ax0, xerr=df_std)\n",
    "ax0.set_xlabel(\"Clustering scores\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
    "ax1.set_xlabel(\"Clustering time (s)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# %%\n",
    "# :class:`~sklearn.cluster.KMeans` and :class:`~sklearn.cluster.MiniBatchKMeans`\n",
    "# suffer from the phenomenon called the `Curse of Dimensionality\n",
    "# <https://en.wikipedia.org/wiki/Curse_of_dimensionality>`_ for high dimensional\n",
    "# datasets such as text data. That is the reason why the overall scores improve\n",
    "# when using LSA. Using LSA reduced data also improves the stability and\n",
    "# requires lower clustering time, though keep in mind that the LSA step itself\n",
    "# takes a long time, especially with hashed vectors.\n",
    "#\n",
    "# The Silhouette Coefficient is defined between 0 and 1. In all cases we obtain\n",
    "# values close to 0 (even if they improve a bit after using LSA) because its\n",
    "# definition requires measuring distances, in contrast with other evaluation\n",
    "# metrics such as the V-measure and the Adjusted Rand Index which are only based\n",
    "# on cluster assignments rather than distances. Notice that strictly speaking,\n",
    "# one should not compare the Silhouette Coefficient between spaces of different\n",
    "# dimension, due to the different notions of distance they imply.\n",
    "#\n",
    "# The homogeneity, completeness and hence v-measure metrics do not yield a\n",
    "# baseline with regards to random labeling: this means that depending on the\n",
    "# number of samples, clusters and ground truth classes, a completely random\n",
    "# labeling will not always yield the same values. In particular random labeling\n",
    "# won't yield zero scores, especially when the number of clusters is large. This\n",
    "# problem can safely be ignored when the number of samples is more than a\n",
    "# thousand and the number of clusters is less than 10, which is the case of the\n",
    "# present example. For smaller sample sizes or larger number of clusters it is\n",
    "# safer to use an adjusted index such as the Adjusted Rand Index (ARI). See the\n",
    "# example\n",
    "# :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py` for\n",
    "# a demo on the effect of random labeling.\n",
    "#\n",
    "# The size of the error bars show that :class:`~sklearn.cluster.MiniBatchKMeans`\n",
    "# is less stable than :class:`~sklearn.cluster.KMeans` for this relatively small\n",
    "# dataset. It is more interesting to use when the number of samples is much\n",
    "# bigger, but it can come at the expense of a small degradation in clustering\n",
    "# quality compared to the traditional k-means algorithm.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
