{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================\n",
    "Classification of text documents using sparse features\n",
    "======================================================\n",
    "\n",
    "This is an example showing how scikit-learn can be used to classify documents by\n",
    "topics using a `Bag of Words approach\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_. This example uses a\n",
    "Tf-idf-weighted document-term sparse matrix to encode the features and\n",
    "demonstrates various classifiers that can efficiently handle sparse matrices.\n",
    "\n",
    "For document analysis via an unsupervised learning approach, see the example\n",
    "script :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "\n",
    "# %%\n",
    "# Loading and vectorizing the 20 newsgroups text dataset\n",
    "# ======================================================\n",
    "#\n",
    "# We define a function to load data from :ref:`20newsgroups_dataset`, which\n",
    "# comprises around 18,000 newsgroups posts on 20 topics split in two subsets:\n",
    "# one for training (or development) and the other one for testing (or for\n",
    "# performance evaluation). Note that, by default, the text samples contain some\n",
    "# message metadata such as `'headers'`, `'footers'` (signatures) and `'quotes'`\n",
    "# to other posts. The `fetch_20newsgroups` function therefore accepts a\n",
    "# parameter named `remove` to attempt stripping such information that can make\n",
    "# the classification problem \"too easy\". This is achieved using simple\n",
    "# heuristics that are neither perfect nor standard, hence disabled by default.\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode(\"utf-8\")) for s in docs) / 1e6\n",
    "\n",
    "\n",
    "def load_dataset(verbose=False, remove=()):\n",
    "    \"\"\"Load and vectorize the 20 newsgroups dataset.\"\"\"\n",
    "\n",
    "    data_train = fetch_20newsgroups(\n",
    "        subset=\"train\",\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=remove,\n",
    "    )\n",
    "\n",
    "    data_test = fetch_20newsgroups(\n",
    "        subset=\"test\",\n",
    "        categories=categories,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "        remove=remove,\n",
    "    )\n",
    "\n",
    "    # order of labels in `target_names` can be different from `categories`\n",
    "    target_names = data_train.target_names\n",
    "\n",
    "    # split target in a training set and a test set\n",
    "    y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\n",
    "    t0 = time()\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True, max_df=0.5, min_df=5, stop_words=\"english\"\n",
    "    )\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "    duration_train = time() - t0\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    t0 = time()\n",
    "    X_test = vectorizer.transform(data_test.data)\n",
    "    duration_test = time() - t0\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    if verbose:\n",
    "        # compute size of loaded data\n",
    "        data_train_size_mb = size_mb(data_train.data)\n",
    "        data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "        print(\n",
    "            f\"{len(data_train.data)} documents - \"\n",
    "            f\"{data_train_size_mb:.2f}MB (training set)\"\n",
    "        )\n",
    "        print(f\"{len(data_test.data)} documents - {data_test_size_mb:.2f}MB (test set)\")\n",
    "        print(f\"{len(target_names)} categories\")\n",
    "        print(\n",
    "            f\"vectorize training done in {duration_train:.3f}s \"\n",
    "            f\"at {data_train_size_mb / duration_train:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_train.shape[0]}, n_features: {X_train.shape[1]}\")\n",
    "        print(\n",
    "            f\"vectorize testing done in {duration_test:.3f}s \"\n",
    "            f\"at {data_test_size_mb / duration_test:.3f}MB/s\"\n",
    "        )\n",
    "        print(f\"n_samples: {X_test.shape[0]}, n_features: {X_test.shape[1]}\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, feature_names, target_names\n",
    "\n",
    "\n",
    "# %%\n",
    "# Analysis of a bag-of-words document classifier\n",
    "# ==============================================\n",
    "#\n",
    "# We will now train a classifier twice, once on the text samples including\n",
    "# metadata and once after stripping the metadata. For both cases we will analyze\n",
    "# the classification errors on a test set using a confusion matrix and inspect\n",
    "# the coefficients that define the classification function of the trained\n",
    "# models.\n",
    "#\n",
    "# Model without metadata stripping\n",
    "# --------------------------------\n",
    "#\n",
    "# We start by using the custom function `load_dataset` to load the data without\n",
    "# metadata stripping.\n",
    "\n",
    "X_train, X_test, y_train, y_test, feature_names, target_names = load_dataset(\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Our first model is an instance of the\n",
    "# :class:`~sklearn.linear_model.RidgeClassifier` class. This is a linear\n",
    "# classification model that uses the mean squared error on {-1, 1} encoded\n",
    "# targets, one for each possible class. Contrary to\n",
    "# :class:`~sklearn.linear_model.LogisticRegression`,\n",
    "# :class:`~sklearn.linear_model.RidgeClassifier` does not\n",
    "# provide probabilistic predictions (no `predict_proba` method),\n",
    "# but it is often faster to train.\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# %%\n",
    "# We plot the confusion matrix of this classifier to find if there is a pattern\n",
    "# in the classification errors.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "_ = ax.set_title(\n",
    "    f\"Confusion Matrix for {clf.__class__.__name__}\\non the original documents\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# The confusion matrix highlights that documents of the `alt.atheism` class are\n",
    "# often confused with documents with the class `talk.religion.misc` class and\n",
    "# vice-versa which is expected since the topics are semantically related.\n",
    "#\n",
    "# We also observe that some documents of the `sci.space` class can be misclassified as\n",
    "# `comp.graphics` while the converse is much rarer. A manual inspection of those\n",
    "# badly classified documents would be required to get some insights on this\n",
    "# asymmetry. It could be the case that the vocabulary of the space topic could\n",
    "# be more specific than the vocabulary for computer graphics.\n",
    "#\n",
    "# We can gain a deeper understanding of how this classifier makes its decisions\n",
    "# by looking at the words with the highest average feature effects:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_feature_effects():\n",
    "    # learned coefficients weighted by frequency of appearance\n",
    "    average_feature_effects = clf.coef_ * np.asarray(X_train.mean(axis=0)).ravel()\n",
    "\n",
    "    for i, label in enumerate(target_names):\n",
    "        top5 = np.argsort(average_feature_effects[i])[-5:][::-1]\n",
    "        if i == 0:\n",
    "            top = pd.DataFrame(feature_names[top5], columns=[label])\n",
    "            top_indices = top5\n",
    "        else:\n",
    "            top[label] = feature_names[top5]\n",
    "            top_indices = np.concatenate((top_indices, top5), axis=None)\n",
    "    top_indices = np.unique(top_indices)\n",
    "    predictive_words = feature_names[top_indices]\n",
    "\n",
    "    # plot feature effects\n",
    "    bar_size = 0.25\n",
    "    padding = 0.75\n",
    "    y_locs = np.arange(len(top_indices)) * (4 * bar_size + padding)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for i, label in enumerate(target_names):\n",
    "        ax.barh(\n",
    "            y_locs + (i - 2) * bar_size,\n",
    "            average_feature_effects[i, top_indices],\n",
    "            height=bar_size,\n",
    "            label=label,\n",
    "        )\n",
    "    ax.set(\n",
    "        yticks=y_locs,\n",
    "        yticklabels=predictive_words,\n",
    "        ylim=[\n",
    "            0 - 4 * bar_size,\n",
    "            len(top_indices) * (4 * bar_size + padding) - 4 * bar_size,\n",
    "        ],\n",
    "    )\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    print(\"top 5 keywords per class:\")\n",
    "    print(top)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "_ = plot_feature_effects().set_title(\"Average feature effect on the original data\")\n",
    "\n",
    "# %%\n",
    "# We can observe that the most predictive words are often strongly positively\n",
    "# associated with a single class and negatively associated with all the other\n",
    "# classes. Most of those positive associations are quite easy to interpret.\n",
    "# However, some words such as `\"god\"` and `\"people\"` are positively associated to\n",
    "# both `\"talk.misc.religion\"` and `\"alt.atheism\"` as those two classes expectedly\n",
    "# share some common vocabulary. Notice however that there are also words such as\n",
    "# `\"christian\"` and `\"morality\"` that are only positively associated with\n",
    "# `\"talk.misc.religion\"`. Furthermore, in this version of the dataset, the word\n",
    "# `\"caltech\"` is one of the top predictive features for atheism due to pollution\n",
    "# in the dataset coming from some sort of metadata such as the email addresses\n",
    "# of the sender of previous emails in the discussion as can be seen below:\n",
    "\n",
    "data_train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "for doc in data_train.data:\n",
    "    if \"caltech\" in doc:\n",
    "        print(doc)\n",
    "        break\n",
    "\n",
    "# %%\n",
    "# Such headers, signature footers (and quoted metadata from previous messages)\n",
    "# can be considered side information that artificially reveals the newsgroup by\n",
    "# identifying the registered members and one would rather want our text\n",
    "# classifier to only learn from the \"main content\" of each text document instead\n",
    "# of relying on the leaked identity of the writers.\n",
    "#\n",
    "# Model with metadata stripping\n",
    "# -----------------------------\n",
    "#\n",
    "# The `remove` option of the 20 newsgroups dataset loader in scikit-learn allows\n",
    "# to heuristically attempt to filter out some of this unwanted metadata that\n",
    "# makes the classification problem artificially easier. Be aware that such\n",
    "# filtering of the text contents is far from perfect.\n",
    "#\n",
    "# Let us try to leverage this option to train a text classifier that does not\n",
    "# rely too much on this kind of metadata to make its decisions:\n",
    "(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    feature_names,\n",
    "    target_names,\n",
    ") = load_dataset(remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "\n",
    "clf = RidgeClassifier(tol=1e-2, solver=\"sparse_cg\")\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, pred, ax=ax)\n",
    "ax.xaxis.set_ticklabels(target_names)\n",
    "ax.yaxis.set_ticklabels(target_names)\n",
    "_ = ax.set_title(\n",
    "    f\"Confusion Matrix for {clf.__class__.__name__}\\non filtered documents\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# By looking at the confusion matrix, it is more evident that the scores of the\n",
    "# model trained with metadata were over-optimistic. The classification problem\n",
    "# without access to the metadata is less accurate but more representative of the\n",
    "# intended text classification problem.\n",
    "\n",
    "_ = plot_feature_effects().set_title(\"Average feature effects on filtered documents\")\n",
    "\n",
    "# %%\n",
    "# In the next section we keep the dataset without metadata to compare several\n",
    "# classifiers.\n",
    "\n",
    "# %%\n",
    "# Benchmarking classifiers\n",
    "# ========================\n",
    "#\n",
    "# Scikit-learn provides many different kinds of classification algorithms. In\n",
    "# this section we will train a selection of those classifiers on the same text\n",
    "# classification problem and measure both their generalization performance\n",
    "# (accuracy on the test set) and their computation performance (speed), both at\n",
    "# training time and testing time. For such purpose we define the following\n",
    "# benchmarking utilities:\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "\n",
    "def benchmark(clf, custom_name=False):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(f\"train time: {train_time:.3}s\")\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(f\"test time:  {test_time:.3}s\")\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(f\"accuracy:   {score:.3}\")\n",
    "\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        print(f\"dimensionality: {clf.coef_.shape[1]}\")\n",
    "        print(f\"density: {density(clf.coef_)}\")\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    if custom_name:\n",
    "        clf_descr = str(custom_name)\n",
    "    else:\n",
    "        clf_descr = clf.__class__.__name__\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "# %%\n",
    "# We now train and test the datasets with 8 different classification models and\n",
    "# get performance results for each model. The goal of this study is to highlight\n",
    "# the computation/accuracy tradeoffs of different types of classifiers for\n",
    "# such a multi-class text classification problem.\n",
    "#\n",
    "# Notice that the most important hyperparameters values were tuned using a grid\n",
    "# search procedure not shown in this notebook for the sake of simplicity. See\n",
    "# the example script\n",
    "# :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`  # noqa: E501\n",
    "# for a demo on how such tuning can be done.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "    (LogisticRegression(C=5, max_iter=1000), \"Logistic Regression\"),\n",
    "    (RidgeClassifier(alpha=1.0, solver=\"sparse_cg\"), \"Ridge Classifier\"),\n",
    "    (KNeighborsClassifier(n_neighbors=100), \"kNN\"),\n",
    "    (RandomForestClassifier(), \"Random Forest\"),\n",
    "    # L2 penalty Linear SVC\n",
    "    (LinearSVC(C=0.1, dual=False, max_iter=1000), \"Linear SVC\"),\n",
    "    # L2 penalty Linear SGD\n",
    "    (\n",
    "        SGDClassifier(\n",
    "            loss=\"log_loss\", alpha=1e-4, n_iter_no_change=3, early_stopping=True\n",
    "        ),\n",
    "        \"log-loss SGD\",\n",
    "    ),\n",
    "    # NearestCentroid (aka Rocchio classifier)\n",
    "    (NearestCentroid(), \"NearestCentroid\"),\n",
    "    # Sparse naive Bayes classifier\n",
    "    (ComplementNB(alpha=0.1), \"Complement naive Bayes\"),\n",
    "):\n",
    "    print(\"=\" * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf, name))\n",
    "\n",
    "# %%\n",
    "# Plot accuracy, training and test time of each classifier\n",
    "# ========================================================\n",
    "#\n",
    "# The scatter plots show the trade-off between the test accuracy and the\n",
    "# training and testing time of each classifier.\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time)\n",
    "test_time = np.array(test_time)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "ax1.scatter(score, training_time, s=60)\n",
    "ax1.set(\n",
    "    title=\"Score-training time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"training time (s)\",\n",
    ")\n",
    "fig, ax2 = plt.subplots(figsize=(10, 8))\n",
    "ax2.scatter(score, test_time, s=60)\n",
    "ax2.set(\n",
    "    title=\"Score-test time trade-off\",\n",
    "    yscale=\"log\",\n",
    "    xlabel=\"test accuracy\",\n",
    "    ylabel=\"test time (s)\",\n",
    ")\n",
    "\n",
    "for i, txt in enumerate(clf_names):\n",
    "    ax1.annotate(txt, (score[i], training_time[i]))\n",
    "    ax2.annotate(txt, (score[i], test_time[i]))\n",
    "\n",
    "# %%\n",
    "# The naive Bayes model has the best trade-off between score and\n",
    "# training/testing time, while Random Forest is both slow to train, expensive to\n",
    "# predict and has a comparatively bad accuracy. This is expected: for\n",
    "# high-dimensional prediction problems, linear models are often better suited as\n",
    "# most problems become linearly separable when the feature space has 10,000\n",
    "# dimensions or more.\n",
    "#\n",
    "# The difference in training speed and accuracy of the linear models can be\n",
    "# explained by the choice of the loss function they optimize and the kind of\n",
    "# regularization they use. Be aware that some linear models with the same loss\n",
    "# but a different solver or regularization configuration may yield different\n",
    "# fitting times and test accuracy. We can observe on the second plot that once\n",
    "# trained, all linear models have approximately the same prediction speed which\n",
    "# is expected because they all implement the same prediction function.\n",
    "#\n",
    "# KNeighborsClassifier has a relatively low accuracy and has the highest testing\n",
    "# time. The long prediction time is also expected: for each prediction the model\n",
    "# has to compute the pairwise distances between the testing sample and each\n",
    "# document in the training set, which is computationally expensive. Furthermore,\n",
    "# the \"curse of dimensionality\" harms the ability of this model to yield\n",
    "# competitive accuracy in the high dimensional feature space of text\n",
    "# classification problems.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
