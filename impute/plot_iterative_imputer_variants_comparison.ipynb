{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=========================================================\n",
    "Imputing missing values with variants of IterativeImputer\n",
    "=========================================================\n",
    "\n",
    ".. currentmodule:: sklearn\n",
    "\n",
    "The :class:`~impute.IterativeImputer` class is very flexible - it can be\n",
    "used with a variety of estimators to do round-robin regression, treating every\n",
    "variable as an output in turn.\n",
    "\n",
    "In this example we compare some estimators for the purpose of missing feature\n",
    "imputation with :class:`~impute.IterativeImputer`:\n",
    "\n",
    "* :class:`~linear_model.BayesianRidge`: regularized linear regression\n",
    "* :class:`~ensemble.RandomForestRegressor`: Forests of randomized trees regression\n",
    "* :func:`~pipeline.make_pipeline` (:class:`~kernel_approximation.Nystroem`,\n",
    "  :class:`~linear_model.Ridge`): a pipeline with the expansion of a degree 2\n",
    "  polynomial kernel and regularized linear regression\n",
    "* :class:`~neighbors.KNeighborsRegressor`: comparable to other KNN\n",
    "  imputation approaches\n",
    "\n",
    "Of particular interest is the ability of\n",
    ":class:`~impute.IterativeImputer` to mimic the behavior of missForest, a\n",
    "popular imputation package for R.\n",
    "\n",
    "Note that :class:`~neighbors.KNeighborsRegressor` is different from KNN\n",
    "imputation, which learns from samples with missing values by using a distance\n",
    "metric that accounts for missing values, rather than imputing them.\n",
    "\n",
    "The goal is to compare different estimators to see which one is best for the\n",
    ":class:`~impute.IterativeImputer` when using a\n",
    ":class:`~linear_model.BayesianRidge` estimator on the California housing\n",
    "dataset with a single value randomly removed from each row.\n",
    "\n",
    "For this particular pattern of missing values we see that\n",
    ":class:`~linear_model.BayesianRidge` and\n",
    ":class:`~ensemble.RandomForestRegressor` give the best results.\n",
    "\n",
    "It should be noted that some estimators such as\n",
    ":class:`~ensemble.HistGradientBoostingRegressor` can natively deal with\n",
    "missing features and are often recommended over building pipelines with\n",
    "complex and costly missing values imputation strategies.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "X_full, y_full = fetch_california_housing(return_X_y=True)\n",
    "# ~2k samples is enough for the purpose of the example.\n",
    "# Remove the following two lines for a slower run with different error bars.\n",
    "X_full = X_full[::10]\n",
    "y_full = y_full[::10]\n",
    "n_samples, n_features = X_full.shape\n",
    "\n",
    "# Estimate the score on the entire dataset, with no missing values\n",
    "br_estimator = BayesianRidge()\n",
    "score_full_data = pd.DataFrame(\n",
    "    cross_val_score(\n",
    "        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n",
    "    ),\n",
    "    columns=[\"Full Data\"],\n",
    ")\n",
    "\n",
    "# Add a single missing value to each row\n",
    "X_missing = X_full.copy()\n",
    "y_missing = y_full\n",
    "missing_samples = np.arange(n_samples)\n",
    "missing_features = rng.choice(n_features, n_samples, replace=True)\n",
    "X_missing[missing_samples, missing_features] = np.nan\n",
    "\n",
    "# Estimate the score after imputation (mean and median strategies)\n",
    "score_simple_imputer = pd.DataFrame()\n",
    "for strategy in (\"mean\", \"median\"):\n",
    "    estimator = make_pipeline(\n",
    "        SimpleImputer(missing_values=np.nan, strategy=strategy), br_estimator\n",
    "    )\n",
    "    score_simple_imputer[strategy] = cross_val_score(\n",
    "        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n",
    "    )\n",
    "\n",
    "# Estimate the score after iterative imputation of the missing values\n",
    "# with different estimators\n",
    "estimators = [\n",
    "    BayesianRidge(),\n",
    "    RandomForestRegressor(\n",
    "        # We tuned the hyperparameters of the RandomForestRegressor to get a good\n",
    "        # enough predictive performance for a restricted execution time.\n",
    "        n_estimators=4,\n",
    "        max_depth=10,\n",
    "        bootstrap=True,\n",
    "        max_samples=0.5,\n",
    "        n_jobs=2,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    make_pipeline(\n",
    "        Nystroem(kernel=\"polynomial\", degree=2, random_state=0), Ridge(alpha=1e3)\n",
    "    ),\n",
    "    KNeighborsRegressor(n_neighbors=15),\n",
    "]\n",
    "score_iterative_imputer = pd.DataFrame()\n",
    "# iterative imputer is sensible to the tolerance and\n",
    "# dependent on the estimator used internally.\n",
    "# we tuned the tolerance to keep this example run with limited computational\n",
    "# resources while not changing the results too much compared to keeping the\n",
    "# stricter default value for the tolerance parameter.\n",
    "tolerances = (1e-3, 1e-1, 1e-1, 1e-2)\n",
    "for impute_estimator, tol in zip(estimators, tolerances):\n",
    "    estimator = make_pipeline(\n",
    "        IterativeImputer(\n",
    "            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n",
    "        ),\n",
    "        br_estimator,\n",
    "    )\n",
    "    score_iterative_imputer[impute_estimator.__class__.__name__] = cross_val_score(\n",
    "        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n",
    "    )\n",
    "\n",
    "scores = pd.concat(\n",
    "    [score_full_data, score_simple_imputer, score_iterative_imputer],\n",
    "    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# plot california housing results\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "means = -scores.mean()\n",
    "errors = scores.std()\n",
    "means.plot.barh(xerr=errors, ax=ax)\n",
    "ax.set_title(\"California Housing Regression with Different Imputation Methods\")\n",
    "ax.set_xlabel(\"MSE (smaller is better)\")\n",
    "ax.set_yticks(np.arange(means.shape[0]))\n",
    "ax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n",
    "plt.tight_layout(pad=1)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
