{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================================================\n",
    "Permutation Importance with Multicollinear or Correlated Features\n",
    "=================================================================\n",
    "\n",
    "In this example, we compute the\n",
    ":func:`~sklearn.inspection.permutation_importance` of the features to a trained\n",
    ":class:`~sklearn.ensemble.RandomForestClassifier` using the\n",
    ":ref:`breast_cancer_dataset`. The model can easily get about 97% accuracy on a\n",
    "test dataset. Because this dataset contains multicollinear features, the\n",
    "permutation importance shows that none of the features are important, in\n",
    "contradiction with the high test accuracy.\n",
    "\n",
    "We demo a possible approach to handling multicollinearity, which consists of\n",
    "hierarchical clustering on the features' Spearman rank-order correlations,\n",
    "picking a threshold, and keeping a single feature from each cluster.\n",
    "\n",
    ".. note::\n",
    "    See also\n",
    "    :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Random Forest Feature Importance on Breast Cancer Data\n",
    "# ------------------------------------------------------\n",
    "#\n",
    "# First, we define a function to ease the plotting:\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.fixes import parse_version\n",
    "\n",
    "\n",
    "def plot_permutation_importance(clf, X, y, ax):\n",
    "    result = permutation_importance(clf, X, y, n_repeats=10, random_state=42, n_jobs=2)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "    # `labels` argument in boxplot is deprecated in matplotlib 3.9 and has been\n",
    "    # renamed to `tick_labels`. The following code handles this, but as a\n",
    "    # scikit-learn user you probably can write simpler code by using `labels=...`\n",
    "    # (matplotlib < 3.9) or `tick_labels=...` (matplotlib >= 3.9).\n",
    "    tick_labels_parameter_name = (\n",
    "        \"tick_labels\"\n",
    "        if parse_version(matplotlib.__version__) >= parse_version(\"3.9\")\n",
    "        else \"labels\"\n",
    "    )\n",
    "    tick_labels_dict = {tick_labels_parameter_name: X.columns[perm_sorted_idx]}\n",
    "    ax.boxplot(result.importances[perm_sorted_idx].T, vert=False, **tick_labels_dict)\n",
    "    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "# %%\n",
    "# We then train a :class:`~sklearn.ensemble.RandomForestClassifier` on the\n",
    "# :ref:`breast_cancer_dataset` and evaluate its accuracy on a test set:\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Baseline accuracy on test data: {clf.score(X_test, y_test):.2}\")\n",
    "\n",
    "# %%\n",
    "# Next, we plot the tree based feature importance and the permutation\n",
    "# importance. The permutation importance is calculated on the training set to\n",
    "# show how much the model relies on each feature during training.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "mdi_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "mdi_importances.sort_values().plot.barh(ax=ax1)\n",
    "ax1.set_xlabel(\"Gini importance\")\n",
    "plot_permutation_importance(clf, X_train, y_train, ax2)\n",
    "ax2.set_xlabel(\"Decrease in accuracy score\")\n",
    "fig.suptitle(\n",
    "    \"Impurity-based vs. permutation importances on multicollinear features (train set)\"\n",
    ")\n",
    "_ = fig.tight_layout()\n",
    "\n",
    "# %%\n",
    "# The plot on the left shows the Gini importance of the model. As the\n",
    "# scikit-learn implementation of\n",
    "# :class:`~sklearn.ensemble.RandomForestClassifier` uses a random subsets of\n",
    "# :math:`\\sqrt{n_\\text{features}}` features at each split, it is able to dilute\n",
    "# the dominance of any single correlated feature. As a result, the individual\n",
    "# feature importance may be distributed more evenly among the correlated\n",
    "# features. Since the features have large cardinality and the classifier is\n",
    "# non-overfitted, we can relatively trust those values.\n",
    "#\n",
    "# The permutation importance on the right plot shows that permuting a feature\n",
    "# drops the accuracy by at most `0.012`, which would suggest that none of the\n",
    "# features are important. This is in contradiction with the high test accuracy\n",
    "# computed as baseline: some feature must be important.\n",
    "#\n",
    "# Similarly, the change in accuracy score computed on the test set appears to be\n",
    "# driven by chance:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plot_permutation_importance(clf, X_test, y_test, ax)\n",
    "ax.set_title(\"Permutation Importances on multicollinear features\\n(test set)\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "_ = ax.figure.tight_layout()\n",
    "\n",
    "# %%\n",
    "# Nevertheless, one can still compute a meaningful permutation importance in the\n",
    "# presence of correlated features, as demonstrated in the following section.\n",
    "#\n",
    "# Handling Multicollinear Features\n",
    "# --------------------------------\n",
    "# When features are collinear, permuting one feature has little effect on the\n",
    "# models performance because it can get the same information from a correlated\n",
    "# feature. Note that this is not the case for all predictive models and depends\n",
    "# on their underlying implementation.\n",
    "#\n",
    "# One way to handle multicollinear features is by performing hierarchical\n",
    "# clustering on the Spearman rank-order correlations, picking a threshold, and\n",
    "# keeping a single feature from each cluster. First, we plot a heatmap of the\n",
    "# correlated features:\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "corr = spearmanr(X).correlation\n",
    "\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=X.columns.to_list(), ax=ax1, leaf_rotation=90\n",
    ")\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "_ = fig.tight_layout()\n",
    "\n",
    "# %%\n",
    "# Next, we manually pick a threshold by visual inspection of the dendrogram to\n",
    "# group our features into clusters and choose a feature from each cluster to\n",
    "# keep, select those features from our dataset, and train a new random forest.\n",
    "# The test accuracy of the new random forest did not change much compared to the\n",
    "# random forest trained on the complete dataset.\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "selected_features_names = X.columns[selected_features]\n",
    "\n",
    "X_train_sel = X_train[selected_features_names]\n",
    "X_test_sel = X_test[selected_features_names]\n",
    "\n",
    "clf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_sel.fit(X_train_sel, y_train)\n",
    "print(\n",
    "    \"Baseline accuracy on test data with features removed:\"\n",
    "    f\" {clf_sel.score(X_test_sel, y_test):.2}\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# We can finally explore the permutation importance of the selected subset of\n",
    "# features:\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "plot_permutation_importance(clf_sel, X_test_sel, y_test, ax)\n",
    "ax.set_title(\"Permutation Importances on selected subset of features\\n(test set)\")\n",
    "ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "ax.figure.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
