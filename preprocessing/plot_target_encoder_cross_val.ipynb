{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a155952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=======================================\n",
    "Target Encoder's Internal Cross fitting\n",
    "=======================================\n",
    "\n",
    ".. currentmodule:: sklearn.preprocessing\n",
    "\n",
    "The :class:`TargetEncoder` replaces each category of a categorical feature with\n",
    "the shrunk mean of the target variable for that category. This method is useful\n",
    "in cases where there is a strong relationship between the categorical feature\n",
    "and the target. To prevent overfitting, :meth:`TargetEncoder.fit_transform` uses\n",
    "an internal :term:`cross fitting` scheme to encode the training data to be used\n",
    "by a downstream model. This scheme involves splitting the data into *k* folds\n",
    "and encoding each fold using the encodings learnt using the other *k-1* folds.\n",
    "In this example, we demonstrate the importance of the cross\n",
    "fitting procedure to prevent overfitting.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Create Synthetic Dataset\n",
    "# ========================\n",
    "# For this example, we build a dataset with three categorical features:\n",
    "#\n",
    "# * an informative feature with medium cardinality (\"informative\")\n",
    "# * an uninformative feature with medium cardinality (\"shuffled\")\n",
    "# * an uninformative feature with high cardinality (\"near_unique\")\n",
    "#\n",
    "# First, we generate the informative feature:\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "n_samples = 50_000\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "y = rng.randn(n_samples)\n",
    "noise = 0.5 * rng.randn(n_samples)\n",
    "n_categories = 100\n",
    "\n",
    "kbins = KBinsDiscretizer(\n",
    "    n_bins=n_categories,\n",
    "    encode=\"ordinal\",\n",
    "    strategy=\"uniform\",\n",
    "    random_state=rng,\n",
    "    subsample=None,\n",
    ")\n",
    "X_informative = kbins.fit_transform((y + noise).reshape(-1, 1))\n",
    "\n",
    "# Remove the linear relationship between y and the bin index by permuting the\n",
    "# values of X_informative:\n",
    "permuted_categories = rng.permutation(n_categories)\n",
    "X_informative = permuted_categories[X_informative.astype(np.int32)]\n",
    "\n",
    "# %%\n",
    "# The uninformative feature with medium cardinality is generated by permuting the\n",
    "# informative feature and removing the relationship with the target:\n",
    "X_shuffled = rng.permutation(X_informative)\n",
    "\n",
    "# %%\n",
    "# The uninformative feature with high cardinality is generated so that it is\n",
    "# independent of the target variable. We will show that target encoding without\n",
    "# :term:`cross fitting` will cause catastrophic overfitting for the downstream\n",
    "# regressor. These high cardinality features are basically unique identifiers\n",
    "# for samples which should generally be removed from machine learning datasets.\n",
    "# In this example, we generate them to show how :class:`TargetEncoder`'s default\n",
    "# :term:`cross fitting` behavior mitigates the overfitting issue automatically.\n",
    "X_near_unique_categories = rng.choice(\n",
    "    int(0.9 * n_samples), size=n_samples, replace=True\n",
    ").reshape(-1, 1)\n",
    "\n",
    "# %%\n",
    "# Finally, we assemble the dataset and perform a train test split:\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.DataFrame(\n",
    "    np.concatenate(\n",
    "        [X_informative, X_shuffled, X_near_unique_categories],\n",
    "        axis=1,\n",
    "    ),\n",
    "    columns=[\"informative\", \"shuffled\", \"near_unique\"],\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# %%\n",
    "# Training a Ridge Regressor\n",
    "# ==========================\n",
    "# In this section, we train a ridge regressor on the dataset with and without\n",
    "# encoding and explore the influence of target encoder with and without the\n",
    "# internal :term:`cross fitting`. First, we see the Ridge model trained on the\n",
    "# raw features will have low performance. This is because we permuted the order\n",
    "# of the informative feature meaning `X_informative` is not informative when\n",
    "# raw:\n",
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Configure transformers to always output DataFrames\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "\n",
    "ridge = Ridge(alpha=1e-6, solver=\"lsqr\", fit_intercept=False)\n",
    "\n",
    "raw_model = ridge.fit(X_train, y_train)\n",
    "print(\"Raw Model score on training set: \", raw_model.score(X_train, y_train))\n",
    "print(\"Raw Model score on test set: \", raw_model.score(X_test, y_test))\n",
    "\n",
    "# %%\n",
    "# Next, we create a pipeline with the target encoder and ridge model. The pipeline\n",
    "# uses :meth:`TargetEncoder.fit_transform` which uses :term:`cross fitting`. We\n",
    "# see that the model fits the data well and generalizes to the test set:\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "model_with_cf = make_pipeline(TargetEncoder(random_state=0), ridge)\n",
    "model_with_cf.fit(X_train, y_train)\n",
    "print(\"Model with CF on train set: \", model_with_cf.score(X_train, y_train))\n",
    "print(\"Model with CF on test set: \", model_with_cf.score(X_test, y_test))\n",
    "\n",
    "# %%\n",
    "# The coefficients of the linear model shows that most of the weight is on the\n",
    "# feature at column index 0, which is the informative feature\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
    "\n",
    "coefs_cf = pd.Series(\n",
    "    model_with_cf[-1].coef_, index=model_with_cf[-1].feature_names_in_\n",
    ").sort_values()\n",
    "ax = coefs_cf.plot(kind=\"barh\")\n",
    "_ = ax.set(\n",
    "    title=\"Target encoded with cross fitting\",\n",
    "    xlabel=\"Ridge coefficient\",\n",
    "    ylabel=\"Feature\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# While :meth:`TargetEncoder.fit_transform` uses an internal\n",
    "# :term:`cross fitting` scheme to learn encodings for the training set,\n",
    "# :meth:`TargetEncoder.transform` itself does not.\n",
    "# It uses the complete training set to learn encodings and to transform the\n",
    "# categorical features. Thus, we can use :meth:`TargetEncoder.fit` followed by\n",
    "# :meth:`TargetEncoder.transform` to disable the :term:`cross fitting`. This\n",
    "# encoding is then passed to the ridge model.\n",
    "target_encoder = TargetEncoder(random_state=0)\n",
    "target_encoder.fit(X_train, y_train)\n",
    "X_train_no_cf_encoding = target_encoder.transform(X_train)\n",
    "X_test_no_cf_encoding = target_encoder.transform(X_test)\n",
    "\n",
    "model_no_cf = ridge.fit(X_train_no_cf_encoding, y_train)\n",
    "\n",
    "# %%\n",
    "# We evaluate the model that did not use :term:`cross fitting` when encoding and\n",
    "# see that it overfits:\n",
    "print(\n",
    "    \"Model without CF on training set: \",\n",
    "    model_no_cf.score(X_train_no_cf_encoding, y_train),\n",
    ")\n",
    "print(\n",
    "    \"Model without CF on test set: \",\n",
    "    model_no_cf.score(\n",
    "        X_test_no_cf_encoding,\n",
    "        y_test,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# %%\n",
    "# The ridge model overfits because it assigns much more weight to the\n",
    "# uninformative extremely high cardinality (\"near_unique\") and medium\n",
    "# cardinality (\"shuffled\") features than when the model used\n",
    "# :term:`cross fitting` to encode the features.\n",
    "coefs_no_cf = pd.Series(\n",
    "    model_no_cf.coef_, index=model_no_cf.feature_names_in_\n",
    ").sort_values()\n",
    "ax = coefs_no_cf.plot(kind=\"barh\")\n",
    "_ = ax.set(\n",
    "    title=\"Target encoded without cross fitting\",\n",
    "    xlabel=\"Ridge coefficient\",\n",
    "    ylabel=\"Feature\",\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Conclusion\n",
    "# ==========\n",
    "# This example demonstrates the importance of :class:`TargetEncoder`'s internal\n",
    "# :term:`cross fitting`. It is important to use\n",
    "# :meth:`TargetEncoder.fit_transform` to encode training data before passing it\n",
    "# to a machine learning model. When a :class:`TargetEncoder` is a part of a\n",
    "# :class:`~sklearn.pipeline.Pipeline` and the pipeline is fitted, the pipeline\n",
    "# will correctly call :meth:`TargetEncoder.fit_transform` and use\n",
    "# :term:`cross fitting` when encoding the training data.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
