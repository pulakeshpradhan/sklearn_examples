{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb84df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================\n",
    "Using KBinsDiscretizer to discretize continuous features\n",
    "================================================================\n",
    "\n",
    "The example compares prediction result of linear regression (linear model)\n",
    "and decision tree (tree based model) with and without discretization of\n",
    "real-valued features.\n",
    "\n",
    "As is shown in the result before discretization, linear model is fast to\n",
    "build and relatively straightforward to interpret, but can only model\n",
    "linear relationships, while decision tree can build a much more complex model\n",
    "of the data. One way to make linear model more powerful on continuous data\n",
    "is to use discretization (also known as binning). In the example, we\n",
    "discretize the feature and one-hot encode the transformed data. Note that if\n",
    "the bins are not reasonably wide, there would appear to be a substantially\n",
    "increased risk of overfitting, so the discretizer parameters should usually\n",
    "be tuned under cross validation.\n",
    "\n",
    "After discretization, linear regression and decision tree make exactly the\n",
    "same prediction. As features are constant within each bin, any model must\n",
    "predict the same value for all points within a bin. Compared with the result\n",
    "before discretization, linear model become much more flexible while decision\n",
    "tree gets much less flexible. Note that binning features generally has no\n",
    "beneficial effect for tree-based models, as these models can learn to split\n",
    "up the data anywhere.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# construct the dataset\n",
    "rnd = np.random.RandomState(42)\n",
    "X = rnd.uniform(-3, 3, size=100)\n",
    "y = np.sin(X) + rnd.normal(size=len(X)) / 3\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "# transform the dataset with KBinsDiscretizer\n",
    "enc = KBinsDiscretizer(n_bins=10, encode=\"onehot\")\n",
    "X_binned = enc.fit_transform(X)\n",
    "\n",
    "# predict with original dataset\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\n",
    "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "ax1.plot(line, reg.predict(line), linewidth=2, color=\"green\", label=\"linear regression\")\n",
    "reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)\n",
    "ax1.plot(line, reg.predict(line), linewidth=2, color=\"red\", label=\"decision tree\")\n",
    "ax1.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set_ylabel(\"Regression output\")\n",
    "ax1.set_xlabel(\"Input feature\")\n",
    "ax1.set_title(\"Result before discretization\")\n",
    "\n",
    "# predict with transformed dataset\n",
    "line_binned = enc.transform(line)\n",
    "reg = LinearRegression().fit(X_binned, y)\n",
    "ax2.plot(\n",
    "    line,\n",
    "    reg.predict(line_binned),\n",
    "    linewidth=2,\n",
    "    color=\"green\",\n",
    "    linestyle=\"-\",\n",
    "    label=\"linear regression\",\n",
    ")\n",
    "reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X_binned, y)\n",
    "ax2.plot(\n",
    "    line,\n",
    "    reg.predict(line_binned),\n",
    "    linewidth=2,\n",
    "    color=\"red\",\n",
    "    linestyle=\":\",\n",
    "    label=\"decision tree\",\n",
    ")\n",
    "ax2.plot(X[:, 0], y, \"o\", c=\"k\")\n",
    "ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=0.2)\n",
    "ax2.legend(loc=\"best\")\n",
    "ax2.set_xlabel(\"Input feature\")\n",
    "ax2.set_title(\"Result after discretization\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
