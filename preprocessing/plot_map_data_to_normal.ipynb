{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=================================\n",
    "Map data to a normal distribution\n",
    "=================================\n",
    "\n",
    ".. currentmodule:: sklearn.preprocessing\n",
    "\n",
    "This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\n",
    "through :class:`~PowerTransformer` to map data from various\n",
    "distributions to a normal distribution.\n",
    "\n",
    "The power transform is useful as a transformation in modeling problems where\n",
    "homoscedasticity and normality are desired. Below are examples of Box-Cox and\n",
    "Yeo-Johnwon applied to six different probability distributions: Lognormal,\n",
    "Chi-squared, Weibull, Gaussian, Uniform, and Bimodal.\n",
    "\n",
    "Note that the transformations successfully map the data to a normal\n",
    "distribution when applied to certain datasets, but are ineffective with others.\n",
    "This highlights the importance of visualizing the data before and after\n",
    "transformation.\n",
    "\n",
    "Also note that even though Box-Cox seems to perform better than Yeo-Johnson for\n",
    "lognormal and chi-squared distributions, keep in mind that Box-Cox does not\n",
    "support inputs with negative values.\n",
    "\n",
    "For comparison, we also add the output from\n",
    ":class:`~QuantileTransformer`. It can force any arbitrary\n",
    "distribution into a gaussian, provided that there are enough training samples\n",
    "(thousands). Because it is a non-parametric method, it is harder to interpret\n",
    "than the parametric ones (Box-Cox and Yeo-Johnson).\n",
    "\n",
    "On \"small\" datasets (less than a few hundred points), the quantile transformer\n",
    "is prone to overfitting. The use of the power transform is then recommended.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "N_SAMPLES = 1000\n",
    "FONT_SIZE = 6\n",
    "BINS = 30\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(304)\n",
    "bc = PowerTransformer(method=\"box-cox\")\n",
    "yj = PowerTransformer(method=\"yeo-johnson\")\n",
    "# n_quantiles is set to the training set size rather than the default value\n",
    "# to avoid a warning being raised by this example\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=500, output_distribution=\"normal\", random_state=rng\n",
    ")\n",
    "size = (N_SAMPLES, 1)\n",
    "\n",
    "\n",
    "# lognormal distribution\n",
    "X_lognormal = rng.lognormal(size=size)\n",
    "\n",
    "# chi-squared distribution\n",
    "df = 3\n",
    "X_chisq = rng.chisquare(df=df, size=size)\n",
    "\n",
    "# weibull distribution\n",
    "a = 50\n",
    "X_weibull = rng.weibull(a=a, size=size)\n",
    "\n",
    "# gaussian distribution\n",
    "loc = 100\n",
    "X_gaussian = rng.normal(loc=loc, size=size)\n",
    "\n",
    "# uniform distribution\n",
    "X_uniform = rng.uniform(low=0, high=1, size=size)\n",
    "\n",
    "# bimodal distribution\n",
    "loc_a, loc_b = 100, 105\n",
    "X_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\n",
    "X_bimodal = np.concatenate([X_a, X_b], axis=0)\n",
    "\n",
    "\n",
    "# create plots\n",
    "distributions = [\n",
    "    (\"Lognormal\", X_lognormal),\n",
    "    (\"Chi-squared\", X_chisq),\n",
    "    (\"Weibull\", X_weibull),\n",
    "    (\"Gaussian\", X_gaussian),\n",
    "    (\"Uniform\", X_uniform),\n",
    "    (\"Bimodal\", X_bimodal),\n",
    "]\n",
    "\n",
    "colors = [\"#D81B60\", \"#0188FF\", \"#FFC107\", \"#B7A2FF\", \"#000000\", \"#2EC5AC\"]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))\n",
    "axes = axes.flatten()\n",
    "axes_idxs = [\n",
    "    (0, 3, 6, 9),\n",
    "    (1, 4, 7, 10),\n",
    "    (2, 5, 8, 11),\n",
    "    (12, 15, 18, 21),\n",
    "    (13, 16, 19, 22),\n",
    "    (14, 17, 20, 23),\n",
    "]\n",
    "axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]\n",
    "\n",
    "\n",
    "for distribution, color, axes in zip(distributions, colors, axes_list):\n",
    "    name, X = distribution\n",
    "    X_train, X_test = train_test_split(X, test_size=0.5)\n",
    "\n",
    "    # perform power transforms and quantile transform\n",
    "    X_trans_bc = bc.fit(X_train).transform(X_test)\n",
    "    lmbda_bc = round(bc.lambdas_[0], 2)\n",
    "    X_trans_yj = yj.fit(X_train).transform(X_test)\n",
    "    lmbda_yj = round(yj.lambdas_[0], 2)\n",
    "    X_trans_qt = qt.fit(X_train).transform(X_test)\n",
    "\n",
    "    ax_original, ax_bc, ax_yj, ax_qt = axes\n",
    "\n",
    "    ax_original.hist(X_train, color=color, bins=BINS)\n",
    "    ax_original.set_title(name, fontsize=FONT_SIZE)\n",
    "    ax_original.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "\n",
    "    for ax, X_trans, meth_name, lmbda in zip(\n",
    "        (ax_bc, ax_yj, ax_qt),\n",
    "        (X_trans_bc, X_trans_yj, X_trans_qt),\n",
    "        (\"Box-Cox\", \"Yeo-Johnson\", \"Quantile transform\"),\n",
    "        (lmbda_bc, lmbda_yj, None),\n",
    "    ):\n",
    "        ax.hist(X_trans, color=color, bins=BINS)\n",
    "        title = \"After {}\".format(meth_name)\n",
    "        if lmbda is not None:\n",
    "            title += \"\\n$\\\\lambda$ = {}\".format(lmbda)\n",
    "        ax.set_title(title, fontsize=FONT_SIZE)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=FONT_SIZE)\n",
    "        ax.set_xlim([-3.5, 3.5])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
