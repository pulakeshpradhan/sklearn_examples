{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd254be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "============================================\n",
    "Comparing Target Encoder with Other Encoders\n",
    "============================================\n",
    "\n",
    ".. currentmodule:: sklearn.preprocessing\n",
    "\n",
    "The :class:`TargetEncoder` uses the value of the target to encode each\n",
    "categorical feature. In this example, we will compare three different approaches\n",
    "for handling categorical features: :class:`TargetEncoder`,\n",
    ":class:`OrdinalEncoder`, :class:`OneHotEncoder` and dropping the category.\n",
    "\n",
    ".. note::\n",
    "    `fit(X, y).transform(X)` does not equal `fit_transform(X, y)` because a\n",
    "    cross fitting scheme is used in `fit_transform` for encoding. See the\n",
    "    :ref:`User Guide <target_encoder>`. for details.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Loading Data from OpenML\n",
    "# ========================\n",
    "# First, we load the wine reviews dataset, where the target is the points given\n",
    "# be a reviewer:\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "wine_reviews = fetch_openml(data_id=42074, as_frame=True)\n",
    "\n",
    "df = wine_reviews.frame\n",
    "df.head()\n",
    "\n",
    "# %%\n",
    "# For this example, we use the following subset of numerical and categorical\n",
    "# features in the data. The target are continuous values from 80 to 100:\n",
    "numerical_features = [\"price\"]\n",
    "categorical_features = [\n",
    "    \"country\",\n",
    "    \"province\",\n",
    "    \"region_1\",\n",
    "    \"region_2\",\n",
    "    \"variety\",\n",
    "    \"winery\",\n",
    "]\n",
    "target_name = \"points\"\n",
    "\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[target_name]\n",
    "\n",
    "_ = y.hist()\n",
    "\n",
    "# %%\n",
    "# Training and Evaluating Pipelines with Different Encoders\n",
    "# =========================================================\n",
    "# In this section, we will evaluate pipelines with\n",
    "# :class:`~sklearn.ensemble.HistGradientBoostingRegressor` with different encoding\n",
    "# strategies. First, we list out the encoders we will be using to preprocess\n",
    "# the categorical features:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, TargetEncoder\n",
    "\n",
    "categorical_preprocessors = [\n",
    "    (\"drop\", \"drop\"),\n",
    "    (\"ordinal\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    (\n",
    "        \"one_hot\",\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", max_categories=20, sparse_output=False),\n",
    "    ),\n",
    "    (\"target\", TargetEncoder(target_type=\"continuous\")),\n",
    "]\n",
    "\n",
    "# %%\n",
    "# Next, we evaluate the models using cross validation and record the results:\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_cv_folds = 3\n",
    "max_iter = 20\n",
    "results = []\n",
    "\n",
    "\n",
    "def evaluate_model_and_store(name, pipe):\n",
    "    result = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        cv=n_cv_folds,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    rmse_test_score = -result[\"test_score\"]\n",
    "    rmse_train_score = -result[\"train_score\"]\n",
    "    results.append(\n",
    "        {\n",
    "            \"preprocessor\": name,\n",
    "            \"rmse_test_mean\": rmse_test_score.mean(),\n",
    "            \"rmse_test_std\": rmse_train_score.std(),\n",
    "            \"rmse_train_mean\": rmse_train_score.mean(),\n",
    "            \"rmse_train_std\": rmse_train_score.std(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "for name, categorical_preprocessor in categorical_preprocessors:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numerical\", \"passthrough\", numerical_features),\n",
    "            (\"categorical\", categorical_preprocessor, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor, HistGradientBoostingRegressor(random_state=0, max_iter=max_iter)\n",
    "    )\n",
    "    evaluate_model_and_store(name, pipe)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Native Categorical Feature Support\n",
    "# ==================================\n",
    "# In this section, we build and evaluate a pipeline that uses native categorical\n",
    "# feature support in :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,\n",
    "# which only supports up to 255 unique categories. In our dataset, the most of\n",
    "# the categorical features have more than 255 unique categories:\n",
    "n_unique_categories = df[categorical_features].nunique().sort_values(ascending=False)\n",
    "n_unique_categories\n",
    "\n",
    "# %%\n",
    "# To workaround the limitation above, we group the categorical features into\n",
    "# low cardinality and high cardinality features. The high cardinality features\n",
    "# will be target encoded and the low cardinality features will use the native\n",
    "# categorical feature in gradient boosting.\n",
    "high_cardinality_features = n_unique_categories[n_unique_categories > 255].index\n",
    "low_cardinality_features = n_unique_categories[n_unique_categories <= 255].index\n",
    "mixed_encoded_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical\", \"passthrough\", numerical_features),\n",
    "        (\n",
    "            \"high_cardinality\",\n",
    "            TargetEncoder(target_type=\"continuous\"),\n",
    "            high_cardinality_features,\n",
    "        ),\n",
    "        (\n",
    "            \"low_cardinality\",\n",
    "            OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "            low_cardinality_features,\n",
    "        ),\n",
    "    ],\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# The output of the of the preprocessor must be set to pandas so the\n",
    "# gradient boosting model can detect the low cardinality features.\n",
    "mixed_encoded_preprocessor.set_output(transform=\"pandas\")\n",
    "mixed_pipe = make_pipeline(\n",
    "    mixed_encoded_preprocessor,\n",
    "    HistGradientBoostingRegressor(\n",
    "        random_state=0, max_iter=max_iter, categorical_features=low_cardinality_features\n",
    "    ),\n",
    ")\n",
    "mixed_pipe\n",
    "\n",
    "# %%\n",
    "# Finally, we evaluate the pipeline using cross validation and record the results:\n",
    "evaluate_model_and_store(\"mixed_target\", mixed_pipe)\n",
    "\n",
    "# %%\n",
    "# Plotting the Results\n",
    "# ====================\n",
    "# In this section, we display the results by plotting the test and train scores:\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "results_df = (\n",
    "    pd.DataFrame(results).set_index(\"preprocessor\").sort_values(\"rmse_test_mean\")\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1, 2, figsize=(12, 8), sharey=True, constrained_layout=True\n",
    ")\n",
    "xticks = range(len(results_df))\n",
    "name_to_color = dict(\n",
    "    zip((r[\"preprocessor\"] for r in results), [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\"])\n",
    ")\n",
    "\n",
    "for subset, ax in zip([\"test\", \"train\"], [ax1, ax2]):\n",
    "    mean, std = f\"rmse_{subset}_mean\", f\"rmse_{subset}_std\"\n",
    "    data = results_df[[mean, std]].sort_values(mean)\n",
    "    ax.bar(\n",
    "        x=xticks,\n",
    "        height=data[mean],\n",
    "        yerr=data[std],\n",
    "        width=0.9,\n",
    "        color=[name_to_color[name] for name in data.index],\n",
    "    )\n",
    "    ax.set(\n",
    "        title=f\"RMSE ({subset.title()})\",\n",
    "        xlabel=\"Encoding Scheme\",\n",
    "        xticks=xticks,\n",
    "        xticklabels=data.index,\n",
    "    )\n",
    "\n",
    "# %%\n",
    "# When evaluating the predictive performance on the test set, dropping the\n",
    "# categories perform the worst and the target encoders performs the best. This\n",
    "# can be explained as follows:\n",
    "#\n",
    "# - Dropping the categorical features makes the pipeline less expressive and\n",
    "#   underfitting as a result;\n",
    "# - Due to the high cardinality and to reduce the training time, the one-hot\n",
    "#   encoding scheme uses `max_categories=20` which prevents the features from\n",
    "#   expanding too much, which can result in underfitting.\n",
    "# - If we had not set `max_categories=20`, the one-hot encoding scheme would have\n",
    "#   likely made the pipeline overfitting as the number of features explodes with rare\n",
    "#   category occurrences that are correlated with the target by chance (on the training\n",
    "#   set only);\n",
    "# - The ordinal encoding imposes an arbitrary order to the features which are then\n",
    "#   treated as numerical values by the\n",
    "#   :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. Since this\n",
    "#   model groups numerical features in 256 bins per feature, many unrelated categories\n",
    "#   can be grouped together and as a result overall pipeline can underfit;\n",
    "# - When using the target encoder, the same binning happens, but since the encoded\n",
    "#   values are statistically ordered by marginal association with the target variable,\n",
    "#   the binning use by the :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n",
    "#   makes sense and leads to good results: the combination of smoothed target\n",
    "#   encoding and binning works as a good regularizing strategy against\n",
    "#   overfitting while not limiting the expressiveness of the pipeline too much.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
