{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================\n",
    "Model selection with Probabilistic PCA and Factor Analysis (FA)\n",
    "===============================================================\n",
    "\n",
    "Probabilistic PCA and Factor Analysis are probabilistic models.\n",
    "The consequence is that the likelihood of new data can be used\n",
    "for model selection and covariance estimation.\n",
    "Here we compare PCA and FA with cross-validation on low rank data corrupted\n",
    "with homoscedastic noise (noise variance\n",
    "is the same for each feature) or heteroscedastic noise (noise variance\n",
    "is the different for each feature). In a second step we compare the model\n",
    "likelihood to the likelihoods obtained from shrinkage covariance estimators.\n",
    "\n",
    "One can observe that with homoscedastic noise both FA and PCA succeed\n",
    "in recovering the size of the low rank subspace. The likelihood with PCA\n",
    "is higher than FA in this case. However PCA fails and overestimates\n",
    "the rank when heteroscedastic noise is present. Under appropriate\n",
    "circumstances (choice of the number of components), the held-out\n",
    "data is more likely for low rank models than for shrinkage models.\n",
    "\n",
    "The automatic estimation from\n",
    "Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\n",
    "by Thomas P. Minka is also compared.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "# %%\n",
    "# Create the data\n",
    "# ---------------\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "n_samples, n_features, rank = 500, 25, 5\n",
    "sigma = 1.0\n",
    "rng = np.random.RandomState(42)\n",
    "U, _, _ = linalg.svd(rng.randn(n_features, n_features))\n",
    "X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n",
    "\n",
    "# Adding homoscedastic noise\n",
    "X_homo = X + sigma * rng.randn(n_samples, n_features)\n",
    "\n",
    "# Adding heteroscedastic noise\n",
    "sigmas = sigma * rng.rand(n_features) + sigma / 2.0\n",
    "X_hetero = X + rng.randn(n_samples, n_features) * sigmas\n",
    "\n",
    "# %%\n",
    "# Fit the models\n",
    "# --------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.covariance import LedoitWolf, ShrunkCovariance\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "n_components = np.arange(0, n_features, 5)  # options for n_components\n",
    "\n",
    "\n",
    "def compute_scores(X):\n",
    "    pca = PCA(svd_solver=\"full\")\n",
    "    fa = FactorAnalysis()\n",
    "\n",
    "    pca_scores, fa_scores = [], []\n",
    "    for n in n_components:\n",
    "        pca.n_components = n\n",
    "        fa.n_components = n\n",
    "        pca_scores.append(np.mean(cross_val_score(pca, X)))\n",
    "        fa_scores.append(np.mean(cross_val_score(fa, X)))\n",
    "\n",
    "    return pca_scores, fa_scores\n",
    "\n",
    "\n",
    "def shrunk_cov_score(X):\n",
    "    shrinkages = np.logspace(-2, 0, 30)\n",
    "    cv = GridSearchCV(ShrunkCovariance(), {\"shrinkage\": shrinkages})\n",
    "    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n",
    "\n",
    "\n",
    "def lw_score(X):\n",
    "    return np.mean(cross_val_score(LedoitWolf(), X))\n",
    "\n",
    "\n",
    "for X, title in [(X_homo, \"Homoscedastic Noise\"), (X_hetero, \"Heteroscedastic Noise\")]:\n",
    "    pca_scores, fa_scores = compute_scores(X)\n",
    "    n_components_pca = n_components[np.argmax(pca_scores)]\n",
    "    n_components_fa = n_components[np.argmax(fa_scores)]\n",
    "\n",
    "    pca = PCA(svd_solver=\"full\", n_components=\"mle\")\n",
    "    pca.fit(X)\n",
    "    n_components_pca_mle = pca.n_components_\n",
    "\n",
    "    print(\"best n_components by PCA CV = %d\" % n_components_pca)\n",
    "    print(\"best n_components by FactorAnalysis CV = %d\" % n_components_fa)\n",
    "    print(\"best n_components by PCA MLE = %d\" % n_components_pca_mle)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(n_components, pca_scores, \"b\", label=\"PCA scores\")\n",
    "    plt.plot(n_components, fa_scores, \"r\", label=\"FA scores\")\n",
    "    plt.axvline(rank, color=\"g\", label=\"TRUTH: %d\" % rank, linestyle=\"-\")\n",
    "    plt.axvline(\n",
    "        n_components_pca,\n",
    "        color=\"b\",\n",
    "        label=\"PCA CV: %d\" % n_components_pca,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        n_components_fa,\n",
    "        color=\"r\",\n",
    "        label=\"FactorAnalysis CV: %d\" % n_components_fa,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        n_components_pca_mle,\n",
    "        color=\"k\",\n",
    "        label=\"PCA MLE: %d\" % n_components_pca_mle,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # compare with other covariance estimators\n",
    "    plt.axhline(\n",
    "        shrunk_cov_score(X),\n",
    "        color=\"violet\",\n",
    "        label=\"Shrunk Covariance MLE\",\n",
    "        linestyle=\"-.\",\n",
    "    )\n",
    "    plt.axhline(\n",
    "        lw_score(X),\n",
    "        color=\"orange\",\n",
    "        label=\"LedoitWolf MLE\" % n_components_pca_mle,\n",
    "        linestyle=\"-.\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"nb of components\")\n",
    "    plt.ylabel(\"CV scores\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title(title)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
